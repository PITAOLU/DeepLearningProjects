{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description:\n",
    ">这里是基于机器学习和深度学习的训练的基础上进行的又一次尝试，机器学习是用的多项式贝叶斯，深度学习用的bert，但是效果看起来还是有点不太理想，多项式贝叶斯的训练正确度35%左右，而Bert也只能到达70%左右，我初步怀疑是bert采用的数据集的数量不够，所以我下一步还想进一步增大数据集，用bert再运行。 这里尝试用另一种模型，word2vec与LSTM进行结合的多分类模型。 [参考博客](https://github.com/DLLXW/MultiClassify_LSTM_ForChinese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T06:17:27.914998Z",
     "start_time": "2020-01-09T06:17:24.148071Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"导入包\"\"\"\n",
    "# 基础包\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 处理语言句子的包\n",
    "import re\n",
    "import jieba\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "# 数据集处理\n",
    "import keras.utils\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import imp\n",
    "import yaml\n",
    "import sys\n",
    "\n",
    "# 创建LSTM模型\n",
    "from keras import utils as np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_yaml\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.models import load_model\n",
    "# 多进程处理\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备数据集\n",
    " 根据博客中的描述，这里的数据集需要在原来的基础上进行修改，老师给的只是一个.xlsx文件，但是在这里需要进行拆分，修改\n",
    "> * 首先，读入训练集，把多标签的样本拆成几个，都变成单标签\n",
    "> * 根据标签的类型进行分组，分完组之后把样本写入.txt文件，由于是六分类问题，文件名以六个类标签为名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T02:20:31.214724Z",
     "start_time": "2020-01-09T02:20:30.964181Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"读入原始数据集\"\"\"\n",
    "df = pd.read_excel('../Dataset/train.xlsx', index_col=False, names=['data','label'],encoding='utf-8')\n",
    "df['data'] = df['data'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T02:20:33.240513Z",
     "start_time": "2020-01-09T02:20:32.559968Z"
    }
   },
   "outputs": [],
   "source": [
    "for d, l in df.iterrows():\n",
    "    l.label = l.label.replace('[','').replace(']','').replace(\"'\",'').split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T02:20:34.591796Z",
     "start_time": "2020-01-09T02:20:34.587805Z"
    }
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame()\n",
    "df2 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T02:20:45.086887Z",
     "start_time": "2020-01-09T02:20:36.655038Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(df.shape[0]):\n",
    "    temp = df.iloc[i]\n",
    "    if (len(temp['label'])>1):\n",
    "        for item in temp['label']:\n",
    "            df2 = df2.append({\"data\":temp['data'], \"label\":item.strip()}, ignore_index=True)\n",
    "    else:\n",
    "        df1 = df1.append(temp, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T02:20:46.314580Z",
     "start_time": "2020-01-09T02:20:46.310590Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5364, 2)\n",
      "(1310, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df1.shape)\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T02:20:48.584070Z",
     "start_time": "2020-01-09T02:20:48.191749Z"
    }
   },
   "outputs": [],
   "source": [
    "for i,v in df1['label'].items():\n",
    "    if (type(v) == list):\n",
    "        df1['label'][i] = v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T02:20:50.517394Z",
     "start_time": "2020-01-09T02:20:50.512408Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df1, df2], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T02:20:52.283846Z",
     "start_time": "2020-01-09T02:20:52.279857Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfshape: (6674, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"dfshape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T02:20:54.557418Z",
     "start_time": "2020-01-09T02:20:54.545411Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>【 恶魔们 的 高跟鞋 】 gasoline glamour shoes 集锦 ~ ~ ！ ！ ！</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>经过 长途跋涉 ， 终于 到 了 威尼斯 。 晩上 从 机场 搭乘 boat taxl ， ...</td>\n",
       "      <td>Happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hello kitty 」 停 不 了 的 爱 。</td>\n",
       "      <td>Happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>唱 错词 了 哈哈 好 可爱 坑 死 日本人 - 评论 视频 ： 1991 年 【 beyo...</td>\n",
       "      <td>Happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>我 是 能量 满 分 选手 ， 我们 又 聊到 这么 晚 才 回家 ， 但是 如果 还 有 ...</td>\n",
       "      <td>Happiness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data      label\n",
       "0  【 恶魔们 的 高跟鞋 】 gasoline glamour shoes 集锦 ~ ~ ！ ！ ！       None\n",
       "1  经过 长途跋涉 ， 终于 到 了 威尼斯 。 晩上 从 机场 搭乘 boat taxl ， ...  Happiness\n",
       "2                          hello kitty 」 停 不 了 的 爱 。  Happiness\n",
       "3  唱 错词 了 哈哈 好 可爱 坑 死 日本人 - 评论 视频 ： 1991 年 【 beyo...  Happiness\n",
       "4  我 是 能量 满 分 选手 ， 我们 又 聊到 这么 晚 才 回家 ， 但是 如果 还 有 ...  Happiness"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T02:20:56.690619Z",
     "start_time": "2020-01-09T02:20:56.683637Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None         1896\n",
       "Happiness    1823\n",
       "Sadness      1086\n",
       "Surprise      651\n",
       "Fear          648\n",
       "Anger         570\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"统计标签个数，并准备写入.txt\"\"\"\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T02:20:58.643948Z",
     "start_time": "2020-01-09T02:20:58.637893Z"
    }
   },
   "outputs": [],
   "source": [
    "none = pd.DataFrame()\n",
    "Happiness = pd.DataFrame()\n",
    "Sadness = pd.DataFrame()\n",
    "Surprise = pd.DataFrame()\n",
    "Fear = pd.DataFrame()\n",
    "Anger = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T02:21:00.564690Z",
     "start_time": "2020-01-09T02:21:00.560684Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['None', 'Happiness', 'Happiness', ..., 'Sadness', 'Happiness',\n",
       "       'Fear'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T02:21:10.687135Z",
     "start_time": "2020-01-09T02:21:02.384043Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"遍历df，开始筛选，划分\"\"\"\n",
    "for i in range(df.shape[0]):\n",
    "    temp = df.iloc[i]\n",
    "    if temp['label'] == 'None':\n",
    "        none = none.append(temp, ignore_index=True)\n",
    "    elif temp['label'] == 'Happiness':\n",
    "        Happiness = Happiness.append(temp, ignore_index=True)\n",
    "    elif temp['label'] == 'Sadness':\n",
    "        Sadness = Sadness.append(temp, ignore_index=True)\n",
    "    elif temp['label'] == 'Surprise':\n",
    "        Surprise = Surprise.append(temp, ignore_index=True)\n",
    "    elif temp['label'] == 'Fear':\n",
    "        Fear = Fear.append(temp, ignore_index=True)\n",
    "    else:\n",
    "        Anger = Anger.append(temp, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T02:21:12.634778Z",
     "start_time": "2020-01-09T02:21:12.583689Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"写入.txt文件\"\"\"\n",
    "none['data'].to_csv('newdata/None.txt', index=False)\n",
    "Happiness['data'].to_csv('newdata/Happiness.txt', index=False)\n",
    "Sadness['data'].to_csv('newdata/Sadness.txt', index=False)\n",
    "Surprise['data'].to_csv('newdata/Surprise.txt', index=False)\n",
    "Fear['data'].to_csv('newdata/Fear.txt', index=False)\n",
    "Anger['data'].to_csv('newdata/Anger.txt', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入数据集并进行清洗\n",
    "> 上一步已经把需要处理的数据处理好了，下面导入上面的数据集，并且进行清理，去掉特殊符号（标点符号，数字，空格等）只保留汉字\n",
    "><br>\n",
    "> 这里的数据集在去掉特殊符号的基础上，利用jieba分词进行了处理，这样就做好了word2vec的输入，方便下面基于word2vec进行词向量的构建。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T02:21:16.907342Z",
     "start_time": "2020-01-09T02:21:16.902355Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"清理数据，把无关的字符全部去掉\"\"\"\n",
    "def clean_data(rpath,wpath):\n",
    "    # coding=utf-8\n",
    "    pchinese = re.compile('([\\u4e00-\\u9fa5]+)+?')\n",
    "    f = open(rpath,encoding=\"UTF-8\")\n",
    "    fw = open(wpath, \"w\",encoding=\"UTF-8\")\n",
    "    for line in f.readlines():\n",
    "        m = pchinese.findall(str(line))\n",
    "        if m:\n",
    "            str1 = ''.join(m)\n",
    "            str2 = str(str1)\n",
    "            fw.write(str2)\n",
    "            fw.write(\"\\n\")\n",
    "    f.close()\n",
    "    fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T02:21:18.987938Z",
     "start_time": "2020-01-09T02:21:18.975948Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"导入数据集\"\"\"\n",
    "def loadfile():\n",
    "    none = []\n",
    "    happiness = []\n",
    "    sadness = []\n",
    "    surprise = []\n",
    "    fear = []\n",
    "    anger = []\n",
    " \n",
    "    with open('newdata_clean/None.txt',encoding='UTF-8') as f:\n",
    "        for line in f.readlines():\n",
    "            none.append(list(jieba.cut(line, cut_all=False, HMM=True))[:-1])\n",
    "    with open('newdata_clean/Happiness.txt',encoding='UTF-8') as f:\n",
    "        for line in f.readlines():\n",
    "            happiness.append(list(jieba.cut(line, cut_all=False, HMM=True))[:-1])\n",
    "        f.close()\n",
    "    with open('newdata_clean/Sadness.txt',encoding='UTF-8') as f:\n",
    "        for line in f.readlines():\n",
    "            sadness.append(list(jieba.cut(line, cut_all=False, HMM=True))[:-1])\n",
    "        f.close()\n",
    "    with open('newdata_clean/Surprise.txt',encoding='UTF-8') as f:\n",
    "        for line in f.readlines():\n",
    "            surprise.append(list(jieba.cut(line, cut_all=False, HMM=True))[:-1])\n",
    "        f.close()\n",
    "    with open('newdata_clean/Fear.txt',encoding='UTF-8') as f:\n",
    "        for line in f.readlines():\n",
    "            fear.append(list(jieba.cut(line, cut_all=False, HMM=True))[:-1])\n",
    "        f.close()\n",
    "    with open('newdata_clean/Anger.txt',encoding='UTF-8') as f:\n",
    "        for line in f.readlines():\n",
    "            anger.append(list(jieba.cut(line, cut_all=False, HMM=True))[:-1])\n",
    "        f.close()\n",
    "        \n",
    "        X_Vec = np.concatenate((none, happiness, sadness,surprise, fear, anger))\n",
    "\n",
    "    y = np.concatenate((np.zeros(len(none), dtype=int),\n",
    "                        np.ones(len(happiness), dtype=int),\n",
    "                        2*np.ones(len(sadness), dtype=int),\n",
    "                        3*np.ones(len(surprise), dtype=int),\n",
    "                        4*np.ones(len(fear),  dtype=int),\n",
    "                        5*np.ones(len(anger), dtype=int)))\n",
    "\n",
    "    return X_Vec, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T02:21:22.005449Z",
     "start_time": "2020-01-09T02:21:21.900354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始清洗数据................\n",
      "清洗数据完成................\n"
     ]
    }
   ],
   "source": [
    "print(\"开始清洗数据................\")\n",
    "clean_data('newdata/None.txt','newdata_clean/None.txt')\n",
    "clean_data('newdata/Happiness.txt','newdata_clean/Happiness.txt')\n",
    "clean_data('newdata/Sadness.txt','newdata_clean/Sadness.txt')\n",
    "clean_data('newdata/Surprise.txt','newdata_clean/Surprise.txt')\n",
    "clean_data('newdata/Fear.txt','newdata_clean/Fear.txt')\n",
    "clean_data('newdata/Anger.txt','newdata_clean/Anger.txt')\n",
    "print(\"清洗数据完成................\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T02:21:27.742784Z",
     "start_time": "2020-01-09T02:21:25.410979Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ZHONGQ~1\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始导入数据................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.584 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "导入数据完成................\n"
     ]
    }
   ],
   "source": [
    "print(\"开始导入数据................\")\n",
    "X_Vec, y=loadfile()\n",
    "print(\"导入数据完成................\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T02:21:29.282590Z",
     "start_time": "2020-01-09T02:21:29.278568Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6664,)\n",
      "(6664,)\n"
     ]
    }
   ],
   "source": [
    "print(X_Vec.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于word2vec构建词向量\n",
    "> 计算机只能处理数字，为了将词语送给计算机处理，自然而然的要想办法将词语进行编码(对图像而言，编码无非就是一个一/三维的像素矩阵)，但是对自然语言的编码就困难了，想要把意思相近的词语编码成相似的向量可不是一件容易的事情，于是有人想到word2vec,其实它就干了一件事儿，把词语嵌入(编码)到一个高维空间(向量)，这些向量之间隐含了词语之间的关系。这个向量怎么来呢？答：训练得来。\n",
    ">\n",
    "而之前的机器学习只是单独的统计了词频作为了特征，并没有关心词与词之间的关系，而这里的word2vec向量，是把词语之间的关系包含在了一个高维空间中。\n",
    "这里利用python 的gensim库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T02:23:05.026111Z",
     "start_time": "2020-01-09T02:23:05.019079Z"
    }
   },
   "outputs": [],
   "source": [
    "voc_dim = 150 #word的向量维度\n",
    "min_out = 4  #单词出现频率数\n",
    "window_size = 7 #\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "\n",
    "def word2vec_train(X_Vec):\n",
    "    model_word = Word2Vec(size=voc_dim,\n",
    "                     min_count=min_out,\n",
    "                     window=window_size,\n",
    "                     workers=cpu_count,\n",
    "                     iter=100)\n",
    "    model_word.build_vocab(X_Vec)\n",
    "    model_word.train(X_Vec, total_examples=model_word.corpus_count, epochs=model_word.iter)\n",
    "    model_word.save('model/Word2Vec_java.pkl')\n",
    "\n",
    "    print(len(model_word.wv.vocab.keys()))\n",
    "    input_dim = len(model_word.wv.vocab.keys()) + 1 #频数小于阈值的词语统统放一起，编码为0\n",
    "    embedding_weights = np.zeros((input_dim, voc_dim))\n",
    "    w2dic={}\n",
    "    for i in range(len(model_word.wv.vocab.keys())):\n",
    "        embedding_weights[i+1, :] = model_word[list(model_word.wv.vocab.keys())[i]]\n",
    "        w2dic[list(model_word.wv.vocab.keys())[i]]=i+1\n",
    "    return input_dim,embedding_weights,w2dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T02:23:16.390596Z",
     "start_time": "2020-01-09T02:23:07.040744Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始构建词向量................\n",
      "5863\n",
      "构建词向量完成................\n"
     ]
    }
   ],
   "source": [
    "print(\"开始构建词向量................\")\n",
    "input_dim,embedding_weights,w2dic = word2vec_train(X_Vec)\n",
    "print(\"构建词向量完成................\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T02:23:19.203346Z",
     "start_time": "2020-01-09T02:23:19.199358Z"
    }
   },
   "outputs": [],
   "source": [
    "def data2inx(w2indx,X_Vec):\n",
    "    data = []\n",
    "    for sentence in X_Vec:\n",
    "        new_txt = []\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                new_txt.append(w2indx[word])\n",
    "            except:\n",
    "                new_txt.append(0)\n",
    "\n",
    "        data.append(new_txt)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T02:23:21.375004Z",
     "start_time": "2020-01-09T02:23:21.280219Z"
    }
   },
   "outputs": [],
   "source": [
    "index = data2inx(w2dic,X_Vec)\n",
    "index2 = sequence.pad_sequences(index, maxlen=voc_dim )\n",
    "x_train, x_test, y_train, y_test = train_test_split(index2, y, test_size=0.2)\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes=6)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建模型并进行训练\n",
    ">训练好了词向量，下一步就是把词向量合成句子的向量，这一会利用RNN来把高维的句向量编重新编码，注二分类用的sigmoid，现在多分类需要改为softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T02:23:23.170235Z",
     "start_time": "2020-01-09T02:23:23.166246Z"
    }
   },
   "outputs": [],
   "source": [
    "imp.reload(sys)\n",
    "np.random.seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T02:23:24.669583Z",
     "start_time": "2020-01-09T02:23:24.663600Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"构建LSTM模型\"\"\"\n",
    "lstm_input = 150#lstm输入维度\n",
    "voc_dim = 150 #word的向量维度\n",
    "def lstm(input_dim, embedding_weights):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(output_dim=voc_dim,\n",
    "                        input_dim=input_dim,\n",
    "                        mask_zero=True,\n",
    "                        weights=[embedding_weights],\n",
    "                        input_length=lstm_input))\n",
    "    model.add(LSTM(256, activation='softsign'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(6))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T02:23:26.481493Z",
     "start_time": "2020-01-09T02:23:26.475468Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"训练LSTM模型\"\"\"\n",
    "\n",
    "voc_dim = 150 #word的向量维度\n",
    "lstm_input = 150#lstm输入维度\n",
    "epoch_time = 10#epoch\n",
    "batch_size = 32 #batch\n",
    "\n",
    "def train_lstm(model, x_train, y_train, x_test, y_test):\n",
    "    print('Compiling the Model...')\n",
    "    model.compile(loss='binary_crossentropy',#hinge\n",
    "                  optimizer='adam', metrics=['mae', 'acc'])\n",
    "\n",
    "    print(\"Train...\" )\n",
    "    model.fit(x_train, y_train, batch_size=batch_size, epochs=epoch_time, verbose=1)\n",
    "\n",
    "    print(\"Evaluate...\")\n",
    "    print(model.predict(x_test))\n",
    "    score = model.evaluate(x_test, y_test,\n",
    "                           batch_size=batch_size)\n",
    "\n",
    "    yaml_string = model.to_yaml()\n",
    "    with open('model/lstm_java.yml', 'w') as outfile:\n",
    "        outfile.write(yaml.dump(yaml_string, default_flow_style=True))\n",
    "    model.save('model/lstm_java_total.h5')\n",
    "    print('Test score:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T02:32:40.328101Z",
     "start_time": "2020-01-09T02:23:28.499856Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling the Model...\n",
      "Train...\n",
      "Epoch 1/10\n",
      "5331/5331 [==============================] - 54s 10ms/step - loss: 0.4070 - mean_absolute_error: 0.2508 - acc: 0.8368\n",
      "Epoch 2/10\n",
      "5331/5331 [==============================] - 53s 10ms/step - loss: 0.3661 - mean_absolute_error: 0.2285 - acc: 0.8478\n",
      "Epoch 3/10\n",
      "5331/5331 [==============================] - 54s 10ms/step - loss: 0.3311 - mean_absolute_error: 0.2075 - acc: 0.8610\n",
      "Epoch 4/10\n",
      "5331/5331 [==============================] - 53s 10ms/step - loss: 0.2907 - mean_absolute_error: 0.1834 - acc: 0.8804\n",
      "Epoch 5/10\n",
      "5331/5331 [==============================] - 56s 11ms/step - loss: 0.2486 - mean_absolute_error: 0.1578 - acc: 0.8980\n",
      "Epoch 6/10\n",
      "5331/5331 [==============================] - 56s 10ms/step - loss: 0.2051 - mean_absolute_error: 0.1309 - acc: 0.9175\n",
      "Epoch 7/10\n",
      "5331/5331 [==============================] - 53s 10ms/step - loss: 0.1702 - mean_absolute_error: 0.1088 - acc: 0.9312\n",
      "Epoch 8/10\n",
      "5331/5331 [==============================] - 58s 11ms/step - loss: 0.1469 - mean_absolute_error: 0.0932 - acc: 0.9418\n",
      "Epoch 9/10\n",
      "5331/5331 [==============================] - 52s 10ms/step - loss: 0.1249 - mean_absolute_error: 0.0791 - acc: 0.9498\n",
      "Epoch 10/10\n",
      "5331/5331 [==============================] - 52s 10ms/step - loss: 0.1075 - mean_absolute_error: 0.0677 - acc: 0.9554\n",
      "Evaluate...\n",
      "[[7.90300721e-04 2.57938862e-01 1.06189974e-01 1.15828682e-02\n",
      "  2.07524206e-02 6.02745652e-01]\n",
      " [1.70506850e-01 1.68644071e-01 6.37524903e-01 4.76863794e-03\n",
      "  1.64785068e-02 2.07703863e-03]\n",
      " [7.26029873e-01 1.51252188e-02 2.29372650e-01 1.31181786e-02\n",
      "  9.84987151e-03 6.50430424e-03]\n",
      " ...\n",
      " [1.62442288e-04 9.93346095e-01 2.54053320e-03 2.38077925e-03\n",
      "  1.54293608e-03 2.72556026e-05]\n",
      " [4.01248079e-04 3.81279830e-03 8.82076025e-01 1.40505666e-02\n",
      "  9.73844305e-02 2.27491464e-03]\n",
      " [5.99940913e-03 4.88846630e-01 2.98689250e-02 4.38060999e-01\n",
      "  1.15980655e-02 2.56259218e-02]]\n",
      "1333/1333 [==============================] - 4s 3ms/step\n",
      "Test score: [0.6580483824886599, 0.2214420978077712, 0.7985746448234965]\n"
     ]
    }
   ],
   "source": [
    "model = lstm(input_dim, embedding_weights)\n",
    "train_lstm(model, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试\n",
    "> 上面的模型训练好了之后，下面是用自己的实例进行推理。输入句子--->保留汉字--->分词--->word2Vec编码--->送入模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T06:17:14.393417Z",
     "start_time": "2020-01-09T06:17:14.277678Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Word2Vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-bad2c09bc7c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mvoc_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m150\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m######\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel_word\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model/Word2Vec_java.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'Word2Vec' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"导入模型\"\"\"\n",
    "voc_dim = 150\n",
    "######\n",
    "model_word=Word2Vec.load('model/Word2Vec_java.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T02:33:21.344674Z",
     "start_time": "2020-01-09T02:33:18.419455Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04378584027290344, 0.06542085111141205, 0.07086724787950516, 0.3303574323654175, 0.08224285393953323, 0.40732577443122864]\n",
      "输入：\n",
      "   真是气死我了\n",
      "        \n",
      "输出:\n",
      "   Anger\n"
     ]
    }
   ],
   "source": [
    "input_dim = len(model_word.wv.vocab.keys()) + 1\n",
    "embedding_weights = np.zeros((input_dim, voc_dim))\n",
    "w2dic={}\n",
    "for i in range(len(model_word.wv.vocab.keys())):\n",
    "    embedding_weights[i+1, :] = model_word [list(model_word.wv.vocab.keys())[i]]\n",
    "    w2dic[list(model_word.wv.vocab.keys())[i]]=i+1\n",
    "\n",
    "model = load_model('model/lstm_java_total.h5')\n",
    "\n",
    "pchinese = re.compile('([\\u4e00-\\u9fa5]+)+?')\n",
    "\n",
    "label={0:\"None\", 1:\"Happiness\", 2:\"Sadness\", 3:\"Surprise\", 4:\"Fear\", 5:\"Anger\"}\n",
    "#in_stc=[\"明天\",\"就要\",\"考试\",\"我\",\"特别\",\"紧张\",\"一点\",\"都\",\"没有\",\"复习\"]\n",
    "in_str = \"真是气死我了\"\n",
    "in_stc=''.join(pchinese.findall(in_str))\n",
    "\n",
    "in_stc=list(jieba.cut(in_stc,cut_all=True, HMM=False))\n",
    "\n",
    "new_txt=[]\n",
    "\n",
    "data=[]\n",
    "for word in in_stc:\n",
    "    try:\n",
    "        new_txt.append(w2dic[word])\n",
    "    except:\n",
    "        new_txt.append(0)\n",
    "data.append(new_txt)\n",
    "\n",
    "data=sequence.pad_sequences(data, maxlen=voc_dim )\n",
    "pre=model.predict(data)[0].tolist()\n",
    "print(pre)\n",
    "print(\"输入：\")\n",
    "print(\"  \",in_str)\n",
    "print(\"        \")\n",
    "print(\"输出:\")\n",
    "print(\"  \",label[pre.index(max(pre))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 产生提交的结果\n",
    "> 这一块就是根据老师的测试集进行测试了， 测试集也需要先把多余的符号给去掉，然后借鉴上面的这个测试把测试集的标签打上，然后写入到文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T02:33:25.051355Z",
     "start_time": "2020-01-09T02:33:25.011040Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"读入测试集\"\"\"\n",
    "test = pd.read_excel('../DataSet/test.xlsx')\n",
    "test.to_csv('newdata/test.txt', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T02:33:27.552355Z",
     "start_time": "2020-01-09T02:33:27.545404Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>我 收藏 了 ： why me-李宇 春</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>嗯 、 够钟 翻归 了 。 明天 顶住 个 翻工 。 —— at last 《 你 瞒 我 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>《 鹿鼎记 ： 龙脉 传说 》 : 12月 23日 觉醒 公测 ， 追寻 龙脉 遗迹 ， 梦...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>我 正在 收听 sarah connor 的 单曲 《 just one last danc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>呢 杯野 ！ 好＂勒 ＂ 啊 ！ blue mountain ！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>马天宇 是 不 是 个 gay ？ 难道 没有 解答 ？</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                我 收藏 了 ： why me-李宇 春\n",
       "0  嗯 、 够钟 翻归 了 。 明天 顶住 个 翻工 。 —— at last 《 你 瞒 我 ...\n",
       "1  《 鹿鼎记 ： 龙脉 传说 》 : 12月 23日 觉醒 公测 ， 追寻 龙脉 遗迹 ， 梦...\n",
       "2  我 正在 收听 sarah connor 的 单曲 《 just one last danc...\n",
       "3                   呢 杯野 ！ 好＂勒 ＂ 啊 ！ blue mountain ！\n",
       "4                       马天宇 是 不 是 个 gay ？ 难道 没有 解答 ？"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T02:33:31.029930Z",
     "start_time": "2020-01-09T02:33:31.007990Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"处理测试集，把多余的符号给去掉，生成新的测试集,然后读入\"\"\"\n",
    "\n",
    "clean_data('newdata/test.txt','newdata_clean/test.txt')\n",
    "newtest = pd.read_csv(\"newdata_clean/test.txt\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T02:33:32.742353Z",
     "start_time": "2020-01-09T02:33:32.739402Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1199, 1)\n"
     ]
    }
   ],
   "source": [
    "print(newtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T01:54:25.746655Z",
     "start_time": "2020-01-09T01:54:25.740710Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>我收藏了李宇春</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>嗯够钟翻归了明天顶住个翻工你瞒我瞒无言的亲亲亲侵袭我心仍宁愿亲口讲你累得很如除我以外在你心还...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>鹿鼎记龙脉传说月日觉醒公测追寻龙脉遗迹梦回鹿鼎江湖金顶门的很强力啊不过还是我技高一筹啊哈哈</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>我正在收听的单曲来自豆瓣私人兆赫</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>呢杯野好勒啊</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0                                            我收藏了李宇春\n",
       "1  嗯够钟翻归了明天顶住个翻工你瞒我瞒无言的亲亲亲侵袭我心仍宁愿亲口讲你累得很如除我以外在你心还...\n",
       "2      鹿鼎记龙脉传说月日觉醒公测追寻龙脉遗迹梦回鹿鼎江湖金顶门的很强力啊不过还是我技高一筹啊哈哈\n",
       "3                                   我正在收听的单曲来自豆瓣私人兆赫\n",
       "4                                             呢杯野好勒啊"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newtest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T01:58:04.207273Z",
     "start_time": "2020-01-09T01:58:04.203283Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['我收藏了李宇春'],\n",
       "       ['嗯够钟翻归了明天顶住个翻工你瞒我瞒无言的亲亲亲侵袭我心仍宁愿亲口讲你累得很如除我以外在你心还多出一个人你瞒住我我亦瞒住我太合衬无人车我翻归我行出大堂门口挥一挥衣袖打车是一件他妈奢侈的玩意'],\n",
       "       ['鹿鼎记龙脉传说月日觉醒公测追寻龙脉遗迹梦回鹿鼎江湖金顶门的很强力啊不过还是我技高一筹啊哈哈'],\n",
       "       ...,\n",
       "       ['发表了博文可爱死了拉'],\n",
       "       ['过去只有自己照顾自己的时候掌声很少的时候没有人给他那么多建议的时候大多数时间只能自己沉默的时候自己一个人背着骑着机车在夜幕中穿越台北街头的时候可能无形中早就练就了他一个人做决定的习惯他成熟的很早很快很坦然很坚决他一直是这样我也将一直支持萧敬腾摘自贴吧'],\n",
       "       ['哎呦你能笑得再贱一点儿么']], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newtest.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T02:34:18.765190Z",
     "start_time": "2020-01-09T02:34:18.754181Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"把测试的写成一个函数\"\"\"\n",
    "def compute_sample_label(word):\n",
    "\n",
    "     # 建立映射字典\n",
    "    label_mapping = {0:\"None\", 1:\"Happiness\", 2:\"Sadness\", 3:\"Surprise\", 4:\"Fear\", 5:\"Anger\"}\n",
    "    \n",
    "    input_dim = len(model_word.wv.vocab.keys()) + 1\n",
    "    embedding_weights = np.zeros((input_dim, voc_dim))\n",
    "    w2dic={}\n",
    "    for i in range(len(model_word.wv.vocab.keys())):\n",
    "        embedding_weights[i+1, :] = model_word [list(model_word.wv.vocab.keys())[i]]\n",
    "        w2dic[list(model_word.wv.vocab.keys())[i]]=i+1\n",
    "\n",
    "    model = load_model('model/lstm_java_total.h5')\n",
    "\n",
    "    pchinese = re.compile('([\\u4e00-\\u9fa5]+)+?')\n",
    "\n",
    "    label={0:\"None\", 1:\"Happiness\", 2:\"Sadness\", 3:\"Surprise\", 4:\"Fear\", 5:\"Anger\"}\n",
    "    #in_stc=[\"明天\",\"就要\",\"考试\",\"我\",\"特别\",\"紧张\",\"一点\",\"都\",\"没有\",\"复习\"]\n",
    "    in_str = word\n",
    "    in_stc=''.join(pchinese.findall(in_str))\n",
    "\n",
    "    in_stc=list(jieba.cut(in_stc,cut_all=True, HMM=False))\n",
    "\n",
    "    new_txt=[]\n",
    "\n",
    "    data=[]\n",
    "    for word in in_stc:\n",
    "        try:\n",
    "            new_txt.append(w2dic[word])\n",
    "        except:\n",
    "            new_txt.append(0)\n",
    "    data.append(new_txt)\n",
    "\n",
    "    data=sequence.pad_sequences(data, maxlen=voc_dim )\n",
    "    pre=model.predict(data)[0].tolist()\n",
    "    \n",
    "    sample_label = []\n",
    "    for i in range(len(pre)):\n",
    "        if (pre[i]>0.3):\n",
    "            sample_label.append(label_mapping[i])\n",
    "    \n",
    "    return sample_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T02:46:28.139566Z",
     "start_time": "2020-01-09T02:37:19.739399Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[1;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[0;32m    340\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m       \u001b[0mxla_compile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_XlaCompile\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m       xla_separate_compiled_gradients = op.get_attr(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1666\u001b[0m       raise ValueError(\"No attr named '\" + name + \"' in \" +\n\u001b[1;32m-> 1667\u001b[1;33m                        str(self._node_def))\n\u001b[0m\u001b[0;32m   1668\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_node_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No attr named '_XlaCompile' in name: \"lstm_1_69/while/clip_by_value/Minimum\"\nop: \"Minimum\"\ninput: \"lstm_1_69/while/add_1\"\ninput: \"lstm_1_69/while/Const_1\"\nattr {\n  key: \"T\"\n  value {\n    type: DT_FLOAT\n  }\n}\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-8ec56b5c479b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnewtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0msamplelabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_sample_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamplelabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-37-7e9afc761c4d>\u001b[0m in \u001b[0;36mcompute_sample_label\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mw2dic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_word\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model/lstm_java_total.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mpchinese\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'([\\u4e00-\\u9fa5]+)+?'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    275\u001b[0m             \u001b[1;31m# Build train function (to get weight updates).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m                 \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    278\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    988\u001b[0m                     training_updates = self.optimizer.get_updates(\n\u001b[0;32m    989\u001b[0m                         \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_collected_trainable_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 990\u001b[1;33m                         loss=self.total_loss)\n\u001b[0m\u001b[0;32m    991\u001b[0m                 \u001b[0mupdates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdates\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtraining_updates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m                 \u001b[1;31m# Gets loss and metrics. Updates weights at each call.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\keras\\optimizers.py\u001b[0m in \u001b[0;36mget_updates\u001b[1;34m(self, loss, params)\u001b[0m\n\u001b[0;32m    413\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_updates_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_updates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 415\u001b[1;33m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    416\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\keras\\optimizers.py\u001b[0m in \u001b[0;36mget_gradients\u001b[1;34m(self, loss, params)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'clipnorm'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclipnorm\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[0mnorm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mgradients\u001b[1;34m(loss, variables)\u001b[0m\n\u001b[0;32m   2392\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mgradients\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2393\u001b[0m     \"\"\"\n\u001b[1;32m-> 2394\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2396\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\u001b[0m in \u001b[0;36mgradients\u001b[1;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method)\u001b[0m\n\u001b[0;32m    538\u001b[0m                 \u001b[1;31m# functions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m                 in_grads = _MaybeCompile(\n\u001b[1;32m--> 540\u001b[1;33m                     grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[0;32m    541\u001b[0m               \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m                 \u001b[1;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[1;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[0;32m    344\u001b[0m       \u001b[0mxla_scope\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_XlaScope\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Exit early\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mxla_compile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    538\u001b[0m                 \u001b[1;31m# functions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m                 in_grads = _MaybeCompile(\n\u001b[1;32m--> 540\u001b[1;33m                     grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[0;32m    541\u001b[0m               \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m                 \u001b[1;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36m_MinimumGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m    769\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_MinimumGrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m   \u001b[1;34m\"\"\"Returns grad*(x < y, x >= y) with type of grad.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0m_MaximumMinimumGrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mless_equal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36m_MaximumMinimumGrad\u001b[1;34m(op, grad, selector_op)\u001b[0m\n\u001b[0;32m    751\u001b[0m   \u001b[0mgradshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m   \u001b[0mzeros\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 753\u001b[1;33m   \u001b[0mxmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselector_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    754\u001b[0m   \u001b[0mrx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mry\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_broadcast_gradient_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    755\u001b[0m   \u001b[0mxgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzeros\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mless_equal\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1054\u001b[0m   \"\"\"\n\u001b[1;32m-> 1055\u001b[1;33m   \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op_def_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LessEqual\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1056\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    765\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    766\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    768\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[0;32m   2504\u001b[0m     ret = Operation(node_def, self, inputs=inputs, output_types=dtypes,\n\u001b[0;32m   2505\u001b[0m                     \u001b[0mcontrol_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcontrol_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2506\u001b[1;33m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[0m\u001b[0;32m   2507\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2508\u001b[0m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   1271\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_control_flow_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_control_flow_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1272\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_control_flow_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1273\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_control_flow_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAddOp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1274\u001b[0m     \u001b[1;31m# NOTE(keveman): Control flow context's AddOp could be creating new ops and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1275\u001b[0m     \u001b[1;31m# setting op.inputs[index] = new_op. Thus the new ops' id could be larger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mAddOp\u001b[1;34m(self, op)\u001b[0m\n\u001b[0;32m   2145\u001b[0m             \u001b[0mop_input_ctxt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_AddOpInternal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2146\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2147\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_AddOpInternal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2149\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_AddOpInternal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36m_AddOpInternal\u001b[1;34m(self, op)\u001b[0m\n\u001b[0;32m   2168\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2169\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2170\u001b[1;33m         \u001b[0mreal_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAddValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2171\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreal_x\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2172\u001b[0m           \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mAddValue\u001b[1;34m(self, val)\u001b[0m\n\u001b[0;32m   2101\u001b[0m               \u001b[0mforward_ctxt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_ctxt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGetWhileContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2102\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mforward_ctxt\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mgrad_ctxt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_context\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2103\u001b[1;33m             \u001b[0mreal_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrad_ctxt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGetRealValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2104\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_external_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreal_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2105\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mreal_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mGetRealValue\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    988\u001b[0m           \u001b[1;31m# TODO(yuanbyu): Avoid recording constants.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grad_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 990\u001b[1;33m           \u001b[0mhistory_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcur_grad_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAddForwardAccumulator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    991\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grad_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEnter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m           \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mAddForwardAccumulator\u001b[1;34m(self, value, dead_branch)\u001b[0m\n\u001b[0;32m    862\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m         \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 864\u001b[1;33m         \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_data_flow_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"f_acc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    865\u001b[0m         \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mcurr_ctxt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcurr_ctxt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_data_flow_ops.py\u001b[0m in \u001b[0;36m_stack\u001b[1;34m(elem_type, stack_name, name)\u001b[0m\n\u001b[0;32m   1474\u001b[0m   \"\"\"\n\u001b[0;32m   1475\u001b[0m   result = _op_def_lib.apply_op(\"Stack\", elem_type=elem_type,\n\u001b[1;32m-> 1476\u001b[1;33m                                 stack_name=stack_name, name=name)\n\u001b[0m\u001b[0;32m   1477\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1478\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    765\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    766\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    768\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[0;32m   2506\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[0;32m   2507\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2508\u001b[1;33m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2509\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2510\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   1871\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1872\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1873\u001b[1;33m   \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1874\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1875\u001b[0m     raise RuntimeError(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   1821\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1822\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1823\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[0;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[0;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[0;32m    611\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m       \u001b[1;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[0;32m    669\u001b[0m       output = pywrap_tensorflow.RunCppShapeInference(\n\u001b[0;32m    670\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[0;32m    672\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No shape inference function exists for op\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"产生结果\"\"\"\n",
    "labels = []\n",
    "\n",
    "for word in newtest.values:\n",
    "    samplelabel = compute_sample_label(word[0])\n",
    "    labels.append(samplelabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总结与参考\n",
    "> 参考的一个博客:[LSTM对中文文本的多分类](https://zhuanlan.zhihu.com/p/97745819)  在这个博客上的基础上修改而来的，自己唯一做的工作就是把老师的数据集，通过处理，换成这个模型能够处理的数据集，然后去进行分类。<br>\n",
    "> 如果想要用测试集进行测试，还是需要把测试集进行数据预处理一下，然后再进行测试。这一个的效果或许会好一点，甚至比bert模型的那个效果还好，可能是因为词比单个字的特征把控起来容易。<br>\n",
    ">\n",
    "记录一下知识点：\n",
    "> * 关于jieba\n",
    "> * 关于gensim\n",
    "> * Word2Vec\n",
    "> * 代码的具体细节"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
