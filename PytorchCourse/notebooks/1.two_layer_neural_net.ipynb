{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 第一课\n",
    "\n",
    "褚则伟 zeweichu@gmail.com\n",
    "\n",
    "[参考资料 reference](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "什么是PyTorch?\n",
    "================\n",
    "\n",
    "PyTorch是一个基于Python的科学计算库，它有以下特点:\n",
    "\n",
    "- 类似于NumPy，但是它可以使用GPU\n",
    "- 可以用它定义深度学习模型，可以灵活地进行深度学习模型的训练和使用\n",
    "\n",
    "Tensors\n",
    "---------------\n",
    "\n",
    "\n",
    "Tensor类似与NumPy的ndarray，唯一的区别是Tensor可以在GPU上加速运算。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T02:37:25.670552Z",
     "start_time": "2020-02-25T02:37:23.207878Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造一个未初始化的5x3矩阵:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T02:38:25.921954Z",
     "start_time": "2020-02-25T02:38:25.887047Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.0273e-01, 6.8103e-43, 4.0274e-01],\n",
       "        [6.8103e-43, 4.0274e-01, 6.8103e-43],\n",
       "        [4.0274e-01, 6.8103e-43, 4.0277e-01],\n",
       "        [6.8103e-43, 4.0277e-01, 6.8103e-43],\n",
       "        [4.0278e-01, 6.8103e-43, 4.0278e-01]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.empty(5, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建一个随机初始化的矩阵:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T02:38:48.500345Z",
     "start_time": "2020-02-25T02:38:48.494361Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.5785, 0.3924],\n",
       "        [0.5834, 0.3587, 0.7411],\n",
       "        [0.2405, 0.2649, 0.7328],\n",
       "        [0.2475, 0.9244, 0.8744],\n",
       "        [0.4679, 0.5440, 0.9812]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建一个全部为0，类型为long的矩阵:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T02:39:49.804699Z",
     "start_time": "2020-02-25T02:39:49.798716Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T02:39:57.107470Z",
     "start_time": "2020-02-25T02:39:57.103482Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T02:40:38.068540Z",
     "start_time": "2020-02-25T02:40:38.063554Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(5, 3).long()\n",
    "x.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从数据直接直接构建tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T02:42:10.580346Z",
     "start_time": "2020-02-25T02:42:10.574364Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.5000, 3.0000])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([5.5, 3])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以从一个已有的tensor构建一个tensor。这些方法会重用原来tensor的特征，例如，数据类型，除非提供新的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T02:42:39.163866Z",
     "start_time": "2020-02-25T02:42:39.158878Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.new_ones(5, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T02:43:30.637807Z",
     "start_time": "2020-02-25T02:43:30.631823Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0504, -0.8949, -0.8185],\n",
       "        [-0.5997,  0.5454, -0.0252],\n",
       "        [ 0.2941,  0.3912, -0.6967],\n",
       "        [-0.3260, -1.2392,  1.9875],\n",
       "        [ 0.9372, -2.2931,  1.1308]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn_like(x, dtype=torch.float)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到tensor的形状:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T02:44:10.068295Z",
     "start_time": "2020-02-25T02:44:10.063309Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T02:44:23.507586Z",
     "start_time": "2020-02-25T02:44:23.503552Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>注意</h4><p>``torch.Size`` 返回的是一个tuple</p></div>\n",
    "\n",
    "Operations\n",
    "\n",
    "\n",
    "有很多种tensor运算。我们先介绍加法运算。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T02:44:54.965054Z",
     "start_time": "2020-02-25T02:44:54.960067Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5474, 0.5270, 0.7894],\n",
       "        [0.0124, 0.3016, 0.8688],\n",
       "        [0.2528, 0.7705, 0.1715],\n",
       "        [0.8004, 0.8628, 0.8776],\n",
       "        [0.4489, 0.7268, 0.8369]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(5, 3)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T02:45:07.499481Z",
     "start_time": "2020-02-25T02:45:07.495451Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5979, -0.3678, -0.0291],\n",
       "        [-0.5873,  0.8470,  0.8435],\n",
       "        [ 0.5469,  1.1618, -0.5252],\n",
       "        [ 0.4744, -0.3764,  2.8651],\n",
       "        [ 1.3861, -1.5663,  1.9676]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另一种着加法的写法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T02:45:23.003852Z",
     "start_time": "2020-02-25T02:45:22.997827Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5979, -0.3678, -0.0291],\n",
       "        [-0.5873,  0.8470,  0.8435],\n",
       "        [ 0.5469,  1.1618, -0.5252],\n",
       "        [ 0.4744, -0.3764,  2.8651],\n",
       "        [ 1.3861, -1.5663,  1.9676]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加法：把输出作为一个变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T02:45:55.421847Z",
     "start_time": "2020-02-25T02:45:55.416839Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5979, -0.3678, -0.0291],\n",
       "        [-0.5873,  0.8470,  0.8435],\n",
       "        [ 0.5469,  1.1618, -0.5252],\n",
       "        [ 0.4744, -0.3764,  2.8651],\n",
       "        [ 1.3861, -1.5663,  1.9676]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "# 类似result = x+y  所以上面这样写没多大意义\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in-place加法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T02:46:50.235623Z",
     "start_time": "2020-02-25T02:46:50.229639Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5474, 0.5270, 0.7894],\n",
       "        [0.0124, 0.3016, 0.8688],\n",
       "        [0.2528, 0.7705, 0.1715],\n",
       "        [0.8004, 0.8628, 0.8776],\n",
       "        [0.4489, 0.7268, 0.8369]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T02:47:05.419712Z",
     "start_time": "2020-02-25T02:47:05.414723Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5979, -0.3678, -0.0291],\n",
       "        [-0.5873,  0.8470,  0.8435],\n",
       "        [ 0.5469,  1.1618, -0.5252],\n",
       "        [ 0.4744, -0.3764,  2.8651],\n",
       "        [ 1.3861, -1.5663,  1.9676]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.add_(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>注意</h4><p>任何in-place的运算都会以``_``结尾。\n",
    "    举例来说：``x.copy_(y)``, ``x.t_()``, 会改变 ``x``。</p></div>\n",
    "\n",
    "各种类似NumPy的indexing都可以在PyTorch tensor上面使用。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T02:47:45.381331Z",
     "start_time": "2020-02-25T02:47:45.375349Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8949, -0.8185],\n",
       "        [ 0.5454, -0.0252],\n",
       "        [ 0.3912, -0.6967],\n",
       "        [-1.2392,  1.9875],\n",
       "        [-2.2931,  1.1308]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resizing: 如果你希望resize/reshape一个tensor，可以使用``torch.view``："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T02:48:06.676302Z",
     "start_time": "2020-02-25T02:48:06.670318Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3336, 0.5088, 0.8541, 0.0020],\n",
       "        [0.6179, 0.8664, 0.7470, 0.0266],\n",
       "        [0.2415, 0.3528, 0.0577, 0.4929],\n",
       "        [0.3647, 0.8183, 0.8700, 0.7377]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(4,4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T02:48:20.803820Z",
     "start_time": "2020-02-25T02:48:20.798835Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3336, 0.5088, 0.8541, 0.0020, 0.6179, 0.8664, 0.7470, 0.0266, 0.2415,\n",
       "        0.3528, 0.0577, 0.4929, 0.3647, 0.8183, 0.8700, 0.7377])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x.view(16)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T02:48:36.043930Z",
     "start_time": "2020-02-25T02:48:36.038903Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3336, 0.5088, 0.8541, 0.0020, 0.6179, 0.8664, 0.7470, 0.0266],\n",
       "        [0.2415, 0.3528, 0.0577, 0.4929, 0.3647, 0.8183, 0.8700, 0.7377]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = x.view(2, 8)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T02:48:51.324656Z",
     "start_time": "2020-02-25T02:48:51.320626Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3336, 0.5088, 0.8541, 0.0020, 0.6179, 0.8664, 0.7470, 0.0266],\n",
       "        [0.2415, 0.3528, 0.0577, 0.4929, 0.3647, 0.8183, 0.8700, 0.7377]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = x.view(-1, 8)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你有一个只有一个元素的tensor，使用``.item()``方法可以把里面的value变成Python数值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T02:49:20.550657Z",
     "start_time": "2020-02-25T02:49:20.544600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2592])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T02:50:11.931671Z",
     "start_time": "2020-02-25T02:50:11.926683Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.259178876876831"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.item()   # 把数字拿出来了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T02:50:51.260710Z",
     "start_time": "2020-02-25T02:50:51.255732Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3336, 0.2415],\n",
       "        [0.5088, 0.3528],\n",
       "        [0.8541, 0.0577],\n",
       "        [0.0020, 0.4929],\n",
       "        [0.6179, 0.3647],\n",
       "        [0.8664, 0.8183],\n",
       "        [0.7470, 0.8700],\n",
       "        [0.0266, 0.7377]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.transpose(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T02:49:45.375954Z",
     "start_time": "2020-02-25T02:49:45.368924Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T',\n",
       " '__abs__',\n",
       " '__add__',\n",
       " '__and__',\n",
       " '__array__',\n",
       " '__array_priority__',\n",
       " '__array_wrap__',\n",
       " '__bool__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__div__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__float__',\n",
       " '__floordiv__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__iadd__',\n",
       " '__iand__',\n",
       " '__idiv__',\n",
       " '__ilshift__',\n",
       " '__imul__',\n",
       " '__index__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__int__',\n",
       " '__invert__',\n",
       " '__ior__',\n",
       " '__ipow__',\n",
       " '__irshift__',\n",
       " '__isub__',\n",
       " '__iter__',\n",
       " '__itruediv__',\n",
       " '__ixor__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__long__',\n",
       " '__lshift__',\n",
       " '__lt__',\n",
       " '__matmul__',\n",
       " '__mod__',\n",
       " '__module__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__neg__',\n",
       " '__new__',\n",
       " '__nonzero__',\n",
       " '__or__',\n",
       " '__pow__',\n",
       " '__radd__',\n",
       " '__rdiv__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__reversed__',\n",
       " '__rfloordiv__',\n",
       " '__rmul__',\n",
       " '__rpow__',\n",
       " '__rshift__',\n",
       " '__rsub__',\n",
       " '__rtruediv__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__sub__',\n",
       " '__subclasshook__',\n",
       " '__truediv__',\n",
       " '__weakref__',\n",
       " '__xor__',\n",
       " '_backward_hooks',\n",
       " '_base',\n",
       " '_cdata',\n",
       " '_coalesced_',\n",
       " '_dimI',\n",
       " '_dimV',\n",
       " '_grad',\n",
       " '_grad_fn',\n",
       " '_indices',\n",
       " '_is_view',\n",
       " '_make_subclass',\n",
       " '_nnz',\n",
       " '_values',\n",
       " '_version',\n",
       " 'abs',\n",
       " 'abs_',\n",
       " 'acos',\n",
       " 'acos_',\n",
       " 'add',\n",
       " 'add_',\n",
       " 'addbmm',\n",
       " 'addbmm_',\n",
       " 'addcdiv',\n",
       " 'addcdiv_',\n",
       " 'addcmul',\n",
       " 'addcmul_',\n",
       " 'addmm',\n",
       " 'addmm_',\n",
       " 'addmv',\n",
       " 'addmv_',\n",
       " 'addr',\n",
       " 'addr_',\n",
       " 'all',\n",
       " 'allclose',\n",
       " 'any',\n",
       " 'apply_',\n",
       " 'argmax',\n",
       " 'argmin',\n",
       " 'argsort',\n",
       " 'as_strided',\n",
       " 'as_strided_',\n",
       " 'asin',\n",
       " 'asin_',\n",
       " 'atan',\n",
       " 'atan2',\n",
       " 'atan2_',\n",
       " 'atan_',\n",
       " 'backward',\n",
       " 'baddbmm',\n",
       " 'baddbmm_',\n",
       " 'bernoulli',\n",
       " 'bernoulli_',\n",
       " 'bfloat16',\n",
       " 'bincount',\n",
       " 'bitwise_not',\n",
       " 'bitwise_not_',\n",
       " 'bmm',\n",
       " 'bool',\n",
       " 'byte',\n",
       " 'cauchy_',\n",
       " 'ceil',\n",
       " 'ceil_',\n",
       " 'char',\n",
       " 'cholesky',\n",
       " 'cholesky_inverse',\n",
       " 'cholesky_solve',\n",
       " 'chunk',\n",
       " 'clamp',\n",
       " 'clamp_',\n",
       " 'clamp_max',\n",
       " 'clamp_max_',\n",
       " 'clamp_min',\n",
       " 'clamp_min_',\n",
       " 'clone',\n",
       " 'coalesce',\n",
       " 'contiguous',\n",
       " 'copy_',\n",
       " 'cos',\n",
       " 'cos_',\n",
       " 'cosh',\n",
       " 'cosh_',\n",
       " 'cpu',\n",
       " 'cross',\n",
       " 'cuda',\n",
       " 'cumprod',\n",
       " 'cumsum',\n",
       " 'data',\n",
       " 'data_ptr',\n",
       " 'dense_dim',\n",
       " 'dequantize',\n",
       " 'det',\n",
       " 'detach',\n",
       " 'detach_',\n",
       " 'device',\n",
       " 'diag',\n",
       " 'diag_embed',\n",
       " 'diagflat',\n",
       " 'diagonal',\n",
       " 'digamma',\n",
       " 'digamma_',\n",
       " 'dim',\n",
       " 'dist',\n",
       " 'div',\n",
       " 'div_',\n",
       " 'dot',\n",
       " 'double',\n",
       " 'dtype',\n",
       " 'eig',\n",
       " 'element_size',\n",
       " 'eq',\n",
       " 'eq_',\n",
       " 'equal',\n",
       " 'erf',\n",
       " 'erf_',\n",
       " 'erfc',\n",
       " 'erfc_',\n",
       " 'erfinv',\n",
       " 'erfinv_',\n",
       " 'exp',\n",
       " 'exp_',\n",
       " 'expand',\n",
       " 'expand_as',\n",
       " 'expm1',\n",
       " 'expm1_',\n",
       " 'exponential_',\n",
       " 'fft',\n",
       " 'fill_',\n",
       " 'fill_diagonal_',\n",
       " 'flatten',\n",
       " 'flip',\n",
       " 'float',\n",
       " 'floor',\n",
       " 'floor_',\n",
       " 'fmod',\n",
       " 'fmod_',\n",
       " 'frac',\n",
       " 'frac_',\n",
       " 'gather',\n",
       " 'ge',\n",
       " 'ge_',\n",
       " 'gels',\n",
       " 'geometric_',\n",
       " 'geqrf',\n",
       " 'ger',\n",
       " 'get_device',\n",
       " 'grad',\n",
       " 'grad_fn',\n",
       " 'gt',\n",
       " 'gt_',\n",
       " 'half',\n",
       " 'hardshrink',\n",
       " 'histc',\n",
       " 'ifft',\n",
       " 'index_add',\n",
       " 'index_add_',\n",
       " 'index_copy',\n",
       " 'index_copy_',\n",
       " 'index_fill',\n",
       " 'index_fill_',\n",
       " 'index_put',\n",
       " 'index_put_',\n",
       " 'index_select',\n",
       " 'indices',\n",
       " 'int',\n",
       " 'int_repr',\n",
       " 'inverse',\n",
       " 'irfft',\n",
       " 'is_coalesced',\n",
       " 'is_complex',\n",
       " 'is_contiguous',\n",
       " 'is_cuda',\n",
       " 'is_distributed',\n",
       " 'is_floating_point',\n",
       " 'is_leaf',\n",
       " 'is_mkldnn',\n",
       " 'is_nonzero',\n",
       " 'is_pinned',\n",
       " 'is_quantized',\n",
       " 'is_same_size',\n",
       " 'is_set_to',\n",
       " 'is_shared',\n",
       " 'is_signed',\n",
       " 'is_sparse',\n",
       " 'isclose',\n",
       " 'item',\n",
       " 'kthvalue',\n",
       " 'layout',\n",
       " 'le',\n",
       " 'le_',\n",
       " 'lerp',\n",
       " 'lerp_',\n",
       " 'lgamma',\n",
       " 'lgamma_',\n",
       " 'log',\n",
       " 'log10',\n",
       " 'log10_',\n",
       " 'log1p',\n",
       " 'log1p_',\n",
       " 'log2',\n",
       " 'log2_',\n",
       " 'log_',\n",
       " 'log_normal_',\n",
       " 'log_softmax',\n",
       " 'logdet',\n",
       " 'logsumexp',\n",
       " 'long',\n",
       " 'lstsq',\n",
       " 'lt',\n",
       " 'lt_',\n",
       " 'lu',\n",
       " 'lu_solve',\n",
       " 'map2_',\n",
       " 'map_',\n",
       " 'masked_fill',\n",
       " 'masked_fill_',\n",
       " 'masked_scatter',\n",
       " 'masked_scatter_',\n",
       " 'masked_select',\n",
       " 'matmul',\n",
       " 'matrix_power',\n",
       " 'max',\n",
       " 'mean',\n",
       " 'median',\n",
       " 'min',\n",
       " 'mm',\n",
       " 'mode',\n",
       " 'mul',\n",
       " 'mul_',\n",
       " 'multinomial',\n",
       " 'mv',\n",
       " 'mvlgamma',\n",
       " 'mvlgamma_',\n",
       " 'name',\n",
       " 'narrow',\n",
       " 'narrow_copy',\n",
       " 'ndim',\n",
       " 'ndimension',\n",
       " 'ne',\n",
       " 'ne_',\n",
       " 'neg',\n",
       " 'neg_',\n",
       " 'nelement',\n",
       " 'new',\n",
       " 'new_empty',\n",
       " 'new_full',\n",
       " 'new_ones',\n",
       " 'new_tensor',\n",
       " 'new_zeros',\n",
       " 'nonzero',\n",
       " 'norm',\n",
       " 'normal_',\n",
       " 'numel',\n",
       " 'numpy',\n",
       " 'orgqr',\n",
       " 'ormqr',\n",
       " 'output_nr',\n",
       " 'permute',\n",
       " 'pin_memory',\n",
       " 'pinverse',\n",
       " 'polygamma',\n",
       " 'polygamma_',\n",
       " 'pow',\n",
       " 'pow_',\n",
       " 'prelu',\n",
       " 'prod',\n",
       " 'put_',\n",
       " 'q_scale',\n",
       " 'q_zero_point',\n",
       " 'qr',\n",
       " 'qscheme',\n",
       " 'random_',\n",
       " 'reciprocal',\n",
       " 'reciprocal_',\n",
       " 'record_stream',\n",
       " 'register_hook',\n",
       " 'reinforce',\n",
       " 'relu',\n",
       " 'relu_',\n",
       " 'remainder',\n",
       " 'remainder_',\n",
       " 'renorm',\n",
       " 'renorm_',\n",
       " 'repeat',\n",
       " 'repeat_interleave',\n",
       " 'requires_grad',\n",
       " 'requires_grad_',\n",
       " 'reshape',\n",
       " 'reshape_as',\n",
       " 'resize',\n",
       " 'resize_',\n",
       " 'resize_as',\n",
       " 'resize_as_',\n",
       " 'retain_grad',\n",
       " 'rfft',\n",
       " 'roll',\n",
       " 'rot90',\n",
       " 'round',\n",
       " 'round_',\n",
       " 'rsqrt',\n",
       " 'rsqrt_',\n",
       " 'scatter',\n",
       " 'scatter_',\n",
       " 'scatter_add',\n",
       " 'scatter_add_',\n",
       " 'select',\n",
       " 'set_',\n",
       " 'shape',\n",
       " 'share_memory_',\n",
       " 'short',\n",
       " 'sigmoid',\n",
       " 'sigmoid_',\n",
       " 'sign',\n",
       " 'sign_',\n",
       " 'sin',\n",
       " 'sin_',\n",
       " 'sinh',\n",
       " 'sinh_',\n",
       " 'size',\n",
       " 'slogdet',\n",
       " 'smm',\n",
       " 'softmax',\n",
       " 'solve',\n",
       " 'sort',\n",
       " 'sparse_dim',\n",
       " 'sparse_mask',\n",
       " 'sparse_resize_',\n",
       " 'sparse_resize_and_clear_',\n",
       " 'split',\n",
       " 'split_with_sizes',\n",
       " 'sqrt',\n",
       " 'sqrt_',\n",
       " 'squeeze',\n",
       " 'squeeze_',\n",
       " 'sspaddmm',\n",
       " 'std',\n",
       " 'stft',\n",
       " 'storage',\n",
       " 'storage_offset',\n",
       " 'storage_type',\n",
       " 'stride',\n",
       " 'sub',\n",
       " 'sub_',\n",
       " 'sum',\n",
       " 'sum_to_size',\n",
       " 'svd',\n",
       " 'symeig',\n",
       " 't',\n",
       " 't_',\n",
       " 'take',\n",
       " 'tan',\n",
       " 'tan_',\n",
       " 'tanh',\n",
       " 'tanh_',\n",
       " 'to',\n",
       " 'to_dense',\n",
       " 'to_mkldnn',\n",
       " 'to_sparse',\n",
       " 'tolist',\n",
       " 'topk',\n",
       " 'trace',\n",
       " 'transpose',\n",
       " 'transpose_',\n",
       " 'triangular_solve',\n",
       " 'tril',\n",
       " 'tril_',\n",
       " 'triu',\n",
       " 'triu_',\n",
       " 'trunc',\n",
       " 'trunc_',\n",
       " 'type',\n",
       " 'type_as',\n",
       " 'unbind',\n",
       " 'unfold',\n",
       " 'uniform_',\n",
       " 'unique',\n",
       " 'unique_consecutive',\n",
       " 'unsqueeze',\n",
       " 'unsqueeze_',\n",
       " 'values',\n",
       " 'var',\n",
       " 'view',\n",
       " 'view_as',\n",
       " 'where',\n",
       " 'zero_']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dir(x)    # data grad  grad_fn有用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**更多阅读**\n",
    "\n",
    "\n",
    "  各种Tensor operations, 包括transposing, indexing, slicing,\n",
    "  mathematical operations, linear algebra, random numbers在\n",
    "  `<https://pytorch.org/docs/torch>`.\n",
    "\n",
    "Numpy和Tensor之间的转化\n",
    "------------\n",
    "\n",
    "在Torch Tensor和NumPy array之间相互转化非常容易。\n",
    "\n",
    "Torch Tensor和NumPy array会共享内存，所以改变其中一项也会改变另一项。\n",
    "\n",
    "把Torch Tensor转变成NumPy Array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T02:51:20.547842Z",
     "start_time": "2020-02-25T02:51:20.542854Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T02:51:30.972952Z",
     "start_time": "2020-02-25T02:51:30.967966Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改变numpy array里面的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T02:51:52.379614Z",
     "start_time": "2020-02-25T02:51:52.374628Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 1., 1., 1.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[1] = 2\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把NumPy ndarray转成Torch Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T02:52:14.567571Z",
     "start_time": "2020-02-25T02:52:14.559592Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T02:54:01.227332Z",
     "start_time": "2020-02-25T02:54:01.222346Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.add(a, 1, out=a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T02:54:09.147807Z",
     "start_time": "2020-02-25T02:54:09.142820Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T02:54:23.980606Z",
     "start_time": "2020-02-25T02:54:23.976616Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 3., 3., 3., 3.])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = a + 1 # 这个情况和np.add(a, 1, out=a)不一样，后者还是从原来内存上操作，而这种前者是新开辟了块内存空间\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T02:54:30.450092Z",
     "start_time": "2020-02-25T02:54:30.446103Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T02:55:17.700980Z",
     "start_time": "2020-02-25T02:55:17.695994Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 3., 3., 3., 3.], dtype=torch.float64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(b, 1, out=b)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T02:55:22.659039Z",
     "start_time": "2020-02-25T02:55:22.654053Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 3., 3., 3., 3.])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有CPU上的Tensor都支持转成numpy或者从numpy转成Tensor。\n",
    "\n",
    "CUDA Tensors\n",
    "------------\n",
    "\n",
    "使用``.to``方法，Tensor可以被移动到别的device上。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T02:56:45.926344Z",
     "start_time": "2020-02-25T02:56:45.887415Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T02:58:17.431104Z",
     "start_time": "2020-02-25T02:58:17.422127Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.2592], device='cuda:0')\n",
      "tensor([2.2592], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    y = torch.ones_like(x, device=device)\n",
    "    x = x.to(device)\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T02:59:41.803764Z",
     "start_time": "2020-02-25T02:59:41.798742Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"一个Tensor在GPU上，是无法直接转成numpy的， 需要先转到CPU上\"\"\"\n",
    "# y.data.numpy()\n",
    "y.to(\"cpu\").data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()  # 把模型搬到GPU上     如果想用GPU，需要把东西搬到GPU上去"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "热身: 用numpy实现两层神经网络\n",
    "--------------\n",
    "\n",
    "一个全连接ReLU神经网络，一个隐藏层，没有bias。用来从x预测y，使用L2 Loss。<br>\n",
    "* $hidden = W_1X$\n",
    "* $a = max(0, h)$\n",
    "* $y_{hat} = W_2a$\n",
    "\n",
    "这一实现完全使用numpy来计算前向神经网络，loss，和反向传播。\n",
    "* forward pass\n",
    "* loss\n",
    "* backward pass\n",
    "\n",
    "numpy ndarray是一个普通的n维array。它不知道任何关于深度学习或者梯度(gradient)的知识，也不知道计算图(computation graph)，只是一种用来计算数学运算的数据结构。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T03:36:33.348399Z",
     "start_time": "2020-02-25T03:36:32.798236Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 29228837.983687624\n",
      "1 24198719.633842565\n",
      "2 23304362.816860273\n",
      "3 23138236.945732567\n",
      "4 21440415.137940705\n",
      "5 17677044.729448378\n",
      "6 12694815.893696152\n",
      "7 8181481.081893332\n",
      "8 4953331.466966225\n",
      "9 2977358.010056023\n",
      "10 1858002.7976885657\n",
      "11 1235154.7663061763\n",
      "12 880951.6961961221\n",
      "13 668692.0478266304\n",
      "14 532429.4887859911\n",
      "15 438439.6370128431\n",
      "16 369591.88163513603\n",
      "17 316454.4263452521\n",
      "18 273924.58833620267\n",
      "19 239011.14064763562\n",
      "20 209854.62084150483\n",
      "21 185155.22523234415\n",
      "22 164041.22216083\n",
      "23 145867.86745498492\n",
      "24 130127.7509176284\n",
      "25 116431.02644030296\n",
      "26 104461.6063021034\n",
      "27 93966.22293483978\n",
      "28 84729.62457731686\n",
      "29 76579.02304912546\n",
      "30 69363.8234690314\n",
      "31 62956.305641464955\n",
      "32 57250.98319896429\n",
      "33 52160.08516658566\n",
      "34 47605.818911365845\n",
      "35 43520.440433598415\n",
      "36 39849.35769058479\n",
      "37 36544.9635105283\n",
      "38 33563.42940815117\n",
      "39 30870.142830796656\n",
      "40 28431.835909946523\n",
      "41 26218.916704759213\n",
      "42 24211.412922572483\n",
      "43 22384.30790953869\n",
      "44 20719.529471764206\n",
      "45 19199.11507354322\n",
      "46 17808.62042018764\n",
      "47 16535.152789648677\n",
      "48 15367.67917622092\n",
      "49 14296.500523519728\n",
      "50 13311.90045810709\n",
      "51 12406.559737725667\n",
      "52 11572.963369040575\n",
      "53 10803.94183042939\n",
      "54 10093.88846230939\n",
      "55 9436.901528337392\n",
      "56 8828.92484259572\n",
      "57 8265.588707730072\n",
      "58 7743.693944112016\n",
      "59 7259.421076130839\n",
      "60 6809.372642321626\n",
      "61 6390.885197610991\n",
      "62 6001.885495637532\n",
      "63 5640.06634642354\n",
      "64 5302.772790456609\n",
      "65 4987.977311049895\n",
      "66 4694.135230644431\n",
      "67 4419.583943708625\n",
      "68 4162.8002573101585\n",
      "69 3922.5722064348756\n",
      "70 3697.709409639457\n",
      "71 3487.0465101562068\n",
      "72 3289.667227050904\n",
      "73 3104.48400263119\n",
      "74 2930.913149505996\n",
      "75 2767.960329162407\n",
      "76 2614.886034893273\n",
      "77 2471.0311555968738\n",
      "78 2335.800849440484\n",
      "79 2208.6301665640485\n",
      "80 2088.9722978980108\n",
      "81 1976.2909315575384\n",
      "82 1870.220830913917\n",
      "83 1770.3107261356513\n",
      "84 1676.0940771728783\n",
      "85 1587.2898981972583\n",
      "86 1503.5197230216727\n",
      "87 1424.4821339169712\n",
      "88 1349.9167674108453\n",
      "89 1279.5085486560909\n",
      "90 1213.0042000748103\n",
      "91 1150.2153459062438\n",
      "92 1090.8697995568577\n",
      "93 1034.7835627243594\n",
      "94 981.7611699376326\n",
      "95 931.6165432859981\n",
      "96 884.1765612545652\n",
      "97 839.3147684493331\n",
      "98 796.8508210034267\n",
      "99 756.6573519603218\n",
      "100 718.5965916225803\n",
      "101 682.6005187313381\n",
      "102 648.5507238251075\n",
      "103 616.2942089199928\n",
      "104 585.7230592125546\n",
      "105 556.7542598070033\n",
      "106 529.2980202518977\n",
      "107 503.26382014469755\n",
      "108 478.5871440874301\n",
      "109 455.1625296564215\n",
      "110 432.94404319963155\n",
      "111 411.8605446845939\n",
      "112 391.8525112610388\n",
      "113 372.8637073659223\n",
      "114 354.8338772922589\n",
      "115 337.7175276151768\n",
      "116 321.46876627441833\n",
      "117 306.0370454967246\n",
      "118 291.3821350950003\n",
      "119 277.45426827583657\n",
      "120 264.2191438546578\n",
      "121 251.64588986592338\n",
      "122 239.69442920570327\n",
      "123 228.33644201819\n",
      "124 217.53825947212096\n",
      "125 207.26999220640076\n",
      "126 197.50716970908206\n",
      "127 188.22197229469754\n",
      "128 179.38939260737217\n",
      "129 170.98962259457255\n",
      "130 162.99728539124933\n",
      "131 155.39570832276826\n",
      "132 148.15909897489246\n",
      "133 141.27398584518878\n",
      "134 134.71971716583732\n",
      "135 128.48332171535378\n",
      "136 122.54405698842963\n",
      "137 116.88914007120593\n",
      "138 111.50567082175053\n",
      "139 106.37772022594496\n",
      "140 101.49542940774481\n",
      "141 96.84513622287093\n",
      "142 92.41682806188936\n",
      "143 88.1976601622267\n",
      "144 84.17745960519234\n",
      "145 80.34536596630164\n",
      "146 76.69428059956371\n",
      "147 73.21501998866661\n",
      "148 69.8987181619905\n",
      "149 66.7374935749813\n",
      "150 63.72573171958543\n",
      "151 60.85301100853437\n",
      "152 58.11349930776469\n",
      "153 55.50214216387034\n",
      "154 53.011889671430644\n",
      "155 50.63702396141217\n",
      "156 48.372534514184515\n",
      "157 46.21266803356971\n",
      "158 44.15119643017449\n",
      "159 42.18565113171033\n",
      "160 40.309668922346674\n",
      "161 38.51926150676965\n",
      "162 36.811188061199005\n",
      "163 35.18150791742839\n",
      "164 33.62602532904799\n",
      "165 32.14119893140982\n",
      "166 30.723919769959345\n",
      "167 29.370974185191415\n",
      "168 28.079663175554273\n",
      "169 26.84722273484798\n",
      "170 25.669850203480127\n",
      "171 24.54568733996492\n",
      "172 23.472282765706105\n",
      "173 22.447415638737205\n",
      "174 21.46841807333182\n",
      "175 20.533544265468258\n",
      "176 19.640298302372415\n",
      "177 18.78684248495912\n",
      "178 17.971471121620823\n",
      "179 17.192417823075893\n",
      "180 16.44815794116181\n",
      "181 15.737295480886639\n",
      "182 15.05779068982723\n",
      "183 14.408466512565271\n",
      "184 13.78787282938552\n",
      "185 13.194745493120966\n",
      "186 12.627770618108572\n",
      "187 12.085852758031557\n",
      "188 11.567653559477746\n",
      "189 11.07223478878197\n",
      "190 10.598848174006575\n",
      "191 10.146031424578478\n",
      "192 9.713684194616974\n",
      "193 9.30034975595653\n",
      "194 8.904947773799012\n",
      "195 8.526857874811036\n",
      "196 8.165146365458654\n",
      "197 7.81936740851838\n",
      "198 7.488446149520663\n",
      "199 7.171865525370162\n",
      "200 6.869013743561523\n",
      "201 6.579209700218032\n",
      "202 6.302035701910685\n",
      "203 6.036758966674093\n",
      "204 5.782935780822216\n",
      "205 5.539976735164044\n",
      "206 5.307504745206783\n",
      "207 5.085212823213871\n",
      "208 4.8722371809206635\n",
      "209 4.6683763521471535\n",
      "210 4.473229013091717\n",
      "211 4.28641538640043\n",
      "212 4.1076716721649955\n",
      "213 3.9365006962983946\n",
      "214 3.772616848162915\n",
      "215 3.615658372499436\n",
      "216 3.4654027893367094\n",
      "217 3.3215415553343104\n",
      "218 3.1837275533487706\n",
      "219 3.051764883514319\n",
      "220 2.9254007656820438\n",
      "221 2.8044043739305033\n",
      "222 2.688498513703691\n",
      "223 2.577462875043513\n",
      "224 2.471153311185976\n",
      "225 2.3692748541095128\n",
      "226 2.271719277930077\n",
      "227 2.1782322352896077\n",
      "228 2.0886441929277746\n",
      "229 2.002811636854127\n",
      "230 1.92058906415935\n",
      "231 1.8418185751966247\n",
      "232 1.7663211132679044\n",
      "233 1.6939770099284546\n",
      "234 1.6246552003964498\n",
      "235 1.5582486142446157\n",
      "236 1.4945714714356502\n",
      "237 1.4335382730937667\n",
      "238 1.3750434344367513\n",
      "239 1.318996560524355\n",
      "240 1.2652775257377165\n",
      "241 1.2137922516989217\n",
      "242 1.1644544014400666\n",
      "243 1.117127154545747\n",
      "244 1.0717652013271581\n",
      "245 1.028260762495104\n",
      "246 0.9865529857038\n",
      "247 0.9465594141093203\n",
      "248 0.9082396690837113\n",
      "249 0.8714809287573896\n",
      "250 0.8362256296944787\n",
      "251 0.8024182953343519\n",
      "252 0.7700133397441715\n",
      "253 0.7389330935930064\n",
      "254 0.7091208107177358\n",
      "255 0.6805329227054122\n",
      "256 0.6531172390741075\n",
      "257 0.6268289837867211\n",
      "258 0.6016182328838274\n",
      "259 0.5774337553265553\n",
      "260 0.5542256308011391\n",
      "261 0.5319770268679427\n",
      "262 0.5106228808413625\n",
      "263 0.49013604093527696\n",
      "264 0.47048262195067636\n",
      "265 0.4516417592726487\n",
      "266 0.433555405067798\n",
      "267 0.4162011332090885\n",
      "268 0.3995527316236924\n",
      "269 0.3835862742887432\n",
      "270 0.368261223808672\n",
      "271 0.3535537780866264\n",
      "272 0.3394415003554224\n",
      "273 0.32590707022428034\n",
      "274 0.31291717006873676\n",
      "275 0.3004534773328712\n",
      "276 0.2884928365428638\n",
      "277 0.27701177541405314\n",
      "278 0.2659920271590485\n",
      "279 0.25541456725698797\n",
      "280 0.24526220669848728\n",
      "281 0.2355223019849755\n",
      "282 0.226174719025831\n",
      "283 0.2171975452936868\n",
      "284 0.2085813552082889\n",
      "285 0.20031300331848062\n",
      "286 0.19237720347207155\n",
      "287 0.18475626770815315\n",
      "288 0.1774411812763778\n",
      "289 0.17042141830729599\n",
      "290 0.1636831367913353\n",
      "291 0.1572105426615435\n",
      "292 0.15099984393228588\n",
      "293 0.14503852864701425\n",
      "294 0.13931410311415449\n",
      "295 0.1338154325967106\n",
      "296 0.12853553227648906\n",
      "297 0.12346753853151224\n",
      "298 0.11860233263119424\n",
      "299 0.11392888362936421\n",
      "300 0.10944141800466654\n",
      "301 0.10513303514149125\n",
      "302 0.1009970974941407\n",
      "303 0.09702351784996355\n",
      "304 0.09320820064269453\n",
      "305 0.08954538115350757\n",
      "306 0.08602858201095194\n",
      "307 0.08264933079991094\n",
      "308 0.07940383955516954\n",
      "309 0.07629034390887496\n",
      "310 0.07329838234319128\n",
      "311 0.07042326816178027\n",
      "312 0.06766207237003546\n",
      "313 0.06501106969479858\n",
      "314 0.06246499908787277\n",
      "315 0.06001837636836006\n",
      "316 0.05766830899954482\n",
      "317 0.055411499790931996\n",
      "318 0.05324422072157495\n",
      "319 0.05116173887507938\n",
      "320 0.049161343386174804\n",
      "321 0.04724019021536866\n",
      "322 0.04539518034999033\n",
      "323 0.04362185124324646\n",
      "324 0.04191826529633001\n",
      "325 0.04028317234385599\n",
      "326 0.038712458609228007\n",
      "327 0.037202025867950135\n",
      "328 0.03575112361734832\n",
      "329 0.034357513258034614\n",
      "330 0.0330186450805488\n",
      "331 0.031731901651168085\n",
      "332 0.030495796199642117\n",
      "333 0.029308510837738357\n",
      "334 0.028167629509111152\n",
      "335 0.027071308150126785\n",
      "336 0.026017808703211593\n",
      "337 0.025006045187905675\n",
      "338 0.024033907311961533\n",
      "339 0.02309943341640163\n",
      "340 0.022201441148808575\n",
      "341 0.02133926999538436\n",
      "342 0.020511070619775248\n",
      "343 0.019714392648459644\n",
      "344 0.018948879953508194\n",
      "345 0.018213564084917703\n",
      "346 0.01750676045120042\n",
      "347 0.016827394120734803\n",
      "348 0.016174694717802668\n",
      "349 0.015547835160555584\n",
      "350 0.014945092998040253\n",
      "351 0.014365765869244013\n",
      "352 0.013808965175472171\n",
      "353 0.013274208730878822\n",
      "354 0.012760005910628676\n",
      "355 0.012265820101735787\n",
      "356 0.011790855147843924\n",
      "357 0.011334790026836771\n",
      "358 0.010896506807990789\n",
      "359 0.010474823752800556\n",
      "360 0.010069573328496784\n",
      "361 0.009680334072316478\n",
      "362 0.009305988291246403\n",
      "363 0.008946144811845809\n",
      "364 0.008600327929784296\n",
      "365 0.008268148383702639\n",
      "366 0.007948660281392222\n",
      "367 0.007641543537008038\n",
      "368 0.007346436079517404\n",
      "369 0.007062885454348657\n",
      "370 0.006790167395993484\n",
      "371 0.006528062698005382\n",
      "372 0.006276144048259387\n",
      "373 0.006034141422393419\n",
      "374 0.005801533605199053\n",
      "375 0.00557772404577083\n",
      "376 0.005362621337024129\n",
      "377 0.005155923997080848\n",
      "378 0.004957120013731525\n",
      "379 0.004766028834693686\n",
      "380 0.004582413565040631\n",
      "381 0.004405896160107\n",
      "382 0.004236132695265543\n",
      "383 0.004072926248187944\n",
      "384 0.003916090873062921\n",
      "385 0.003765336905863995\n",
      "386 0.0036203555000503193\n",
      "387 0.003480982334712787\n",
      "388 0.0033470537988925983\n",
      "389 0.0032183378851866886\n",
      "390 0.0030945790997520704\n",
      "391 0.002975501799250488\n",
      "392 0.0028610863042587394\n",
      "393 0.002751068265988484\n",
      "394 0.0026452661573942034\n",
      "395 0.0025435490702114235\n",
      "396 0.0024457949801629055\n",
      "397 0.0023517977246436767\n",
      "398 0.0022614037907968064\n",
      "399 0.002174493804567857\n",
      "400 0.0020909721730804254\n",
      "401 0.002010658168902098\n",
      "402 0.0019334147761042612\n",
      "403 0.0018591479977022565\n",
      "404 0.0017877930159718078\n",
      "405 0.0017192181771143805\n",
      "406 0.0016532160785277797\n",
      "407 0.0015897432607341853\n",
      "408 0.0015287599457850546\n",
      "409 0.0014700964376827593\n",
      "410 0.001413676987855242\n",
      "411 0.0013594251665981403\n",
      "412 0.0013073004753470434\n",
      "413 0.0012571490767040623\n",
      "414 0.001208924485745368\n",
      "415 0.0011625549543140164\n",
      "416 0.0011180087804232531\n",
      "417 0.001075138480139085\n",
      "418 0.0010339164411490044\n",
      "419 0.0009942754687703814\n",
      "420 0.0009562147873687151\n",
      "421 0.0009195870876725376\n",
      "422 0.0008843421104481002\n",
      "423 0.0008504551675857385\n",
      "424 0.00081789296035416\n",
      "425 0.0007865550336320655\n",
      "426 0.0007564203066923625\n",
      "427 0.0007274489151340323\n",
      "428 0.0006996074153602707\n",
      "429 0.0006728130219164075\n",
      "430 0.0006470481266186848\n",
      "431 0.0006222728886029079\n",
      "432 0.0005984650913411329\n",
      "433 0.0005755542951759598\n",
      "434 0.0005535201739155982\n",
      "435 0.0005323410568647199\n",
      "436 0.0005120006786613982\n",
      "437 0.0004924106156141565\n",
      "438 0.00047356725512234224\n",
      "439 0.00045544846052883014\n",
      "440 0.0004380366736371318\n",
      "441 0.0004212782616251465\n",
      "442 0.00040516225940098293\n",
      "443 0.0003896642976823089\n",
      "444 0.00037477485051726266\n",
      "445 0.00036044264562108447\n",
      "446 0.00034665951865671635\n",
      "447 0.00033340316422492025\n",
      "448 0.0003206664984930334\n",
      "449 0.00030840622825569104\n",
      "450 0.0002966184352957926\n",
      "451 0.0002852926352067526\n",
      "452 0.00027439773583820385\n",
      "453 0.00026391068579990115\n",
      "454 0.00025382517928490507\n",
      "455 0.00024412555646294109\n",
      "456 0.00023480561498406713\n",
      "457 0.0002258336556387661\n",
      "458 0.00021720629265899394\n",
      "459 0.00020890799262713054\n",
      "460 0.00020093445000515822\n",
      "461 0.0001932588716623949\n",
      "462 0.0001858775147438177\n",
      "463 0.00017877812015157198\n",
      "464 0.0001719569240829486\n",
      "465 0.00016539162495917935\n",
      "466 0.00015908316633599562\n",
      "467 0.00015300880778951775\n",
      "468 0.00014717221124418503\n",
      "469 0.00014155391583181093\n",
      "470 0.00013615001575797373\n",
      "471 0.00013095274524139801\n",
      "472 0.00012595892683091414\n",
      "473 0.00012115146034651737\n",
      "474 0.00011652776462202472\n",
      "475 0.00011208073723119478\n",
      "476 0.00010780709862759815\n",
      "477 0.00010369337393188474\n",
      "478 9.973648725588673e-05\n",
      "479 9.59317066071863e-05\n",
      "480 9.227514354313764e-05\n",
      "481 8.875884067494012e-05\n",
      "482 8.537267860405553e-05\n",
      "483 8.211617018432998e-05\n",
      "484 7.89864844850078e-05\n",
      "485 7.597425050033223e-05\n",
      "486 7.307703462285488e-05\n",
      "487 7.029001093252338e-05\n",
      "488 6.761150123500686e-05\n",
      "489 6.503346028933437e-05\n",
      "490 6.255348085848237e-05\n",
      "491 6.01684341963066e-05\n",
      "492 5.787605430412499e-05\n",
      "493 5.567000585982516e-05\n",
      "494 5.354756498100253e-05\n",
      "495 5.150686385754285e-05\n",
      "496 4.95465002727429e-05\n",
      "497 4.7658195835770896e-05\n",
      "498 4.584158282928446e-05\n",
      "499 4.409426814024989e-05\n"
     ]
    }
   ],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10   # N表示训练数据的个数， D_in表示输入的特征数 H是中间层，\n",
    "\n",
    "# 随机创建一下训练数据\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for it in range(500):\n",
    "    # forward pass\n",
    "    h = x.dot(w1)   # N*H\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)   # N*D_out\n",
    "    \n",
    "    # compute loss\n",
    "    loss = np.square(y_pred-y).sum()\n",
    "    print(it, loss)\n",
    "    \n",
    "    # Backward pass\n",
    "    # compute the gradient\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    \n",
    "    # update weithts of w1 and w2\n",
    "    w1 -= learning_rate  * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: Tensors\n",
    "----------------\n",
    "\n",
    "这次我们使用PyTorch tensors来创建前向神经网络，计算损失，以及反向传播。\n",
    "\n",
    "一个PyTorch Tensor很像一个numpy的ndarray。但是它和numpy ndarray最大的区别是，PyTorch Tensor可以在CPU或者GPU上运算。如果想要在GPU上运算，就需要把Tensor换成cuda类型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T03:41:57.182994Z",
     "start_time": "2020-02-25T03:41:56.694285Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 30791678.0\n",
      "1 26445046.0\n",
      "2 26094528.0\n",
      "3 25603024.0\n",
      "4 22585934.0\n",
      "5 17110796.0\n",
      "6 11192685.0\n",
      "7 6643192.0\n",
      "8 3854957.25\n",
      "9 2337365.0\n",
      "10 1537026.75\n",
      "11 1102976.0\n",
      "12 849898.6875\n",
      "13 688406.5\n",
      "14 575851.625\n",
      "15 491679.0\n",
      "16 425364.125\n",
      "17 371280.78125\n",
      "18 326206.375\n",
      "19 288096.875\n",
      "20 255565.078125\n",
      "21 227582.28125\n",
      "22 203352.046875\n",
      "23 182242.671875\n",
      "24 163824.5625\n",
      "25 147687.671875\n",
      "26 133457.5\n",
      "27 120875.078125\n",
      "28 109716.1953125\n",
      "29 99784.171875\n",
      "30 90923.578125\n",
      "31 82982.46875\n",
      "32 75860.0390625\n",
      "33 69458.78125\n",
      "34 63693.89453125\n",
      "35 58488.7265625\n",
      "36 53781.82421875\n",
      "37 49522.078125\n",
      "38 45655.3125\n",
      "39 42136.515625\n",
      "40 38925.75390625\n",
      "41 35998.23046875\n",
      "42 33322.546875\n",
      "43 30874.09765625\n",
      "44 28629.70703125\n",
      "45 26570.5\n",
      "46 24679.21484375\n",
      "47 22940.9765625\n",
      "48 21341.61328125\n",
      "49 19867.580078125\n",
      "50 18507.841796875\n",
      "51 17256.236328125\n",
      "52 16099.46875\n",
      "53 15028.9267578125\n",
      "54 14037.8076171875\n",
      "55 13119.4931640625\n",
      "56 12267.853515625\n",
      "57 11477.5146484375\n",
      "58 10743.4365234375\n",
      "59 10061.3984375\n",
      "60 9426.9599609375\n",
      "61 8836.337890625\n",
      "62 8286.0322265625\n",
      "63 7773.32421875\n",
      "64 7295.10791015625\n",
      "65 6849.048828125\n",
      "66 6432.78125\n",
      "67 6043.79541015625\n",
      "68 5680.28466796875\n",
      "69 5340.6259765625\n",
      "70 5022.9990234375\n",
      "71 4725.845703125\n",
      "72 4447.67333984375\n",
      "73 4187.0146484375\n",
      "74 3942.729248046875\n",
      "75 3713.70263671875\n",
      "76 3498.9111328125\n",
      "77 3297.4765625\n",
      "78 3108.449951171875\n",
      "79 2930.93896484375\n",
      "80 2764.15576171875\n",
      "81 2607.45556640625\n",
      "82 2460.175048828125\n",
      "83 2321.7646484375\n",
      "84 2191.583740234375\n",
      "85 2069.184814453125\n",
      "86 1954.1343994140625\n",
      "87 1845.88720703125\n",
      "88 1743.9627685546875\n",
      "89 1647.9515380859375\n",
      "90 1557.55712890625\n",
      "91 1472.348876953125\n",
      "92 1392.154541015625\n",
      "93 1316.71142578125\n",
      "94 1245.5916748046875\n",
      "95 1178.6058349609375\n",
      "96 1115.4158935546875\n",
      "97 1055.7669677734375\n",
      "98 999.5046997070312\n",
      "99 946.4228515625\n",
      "100 896.33544921875\n",
      "101 849.0382690429688\n",
      "102 804.3472290039062\n",
      "103 762.103759765625\n",
      "104 722.18408203125\n",
      "105 684.43701171875\n",
      "106 648.7498168945312\n",
      "107 615.0191650390625\n",
      "108 583.0855712890625\n",
      "109 552.8839111328125\n",
      "110 524.3058471679688\n",
      "111 497.25872802734375\n",
      "112 471.6624450683594\n",
      "113 447.43292236328125\n",
      "114 424.4964294433594\n",
      "115 402.8086242675781\n",
      "116 382.26556396484375\n",
      "117 362.8289794921875\n",
      "118 344.4336242675781\n",
      "119 327.0076904296875\n",
      "120 310.4830017089844\n",
      "121 294.82568359375\n",
      "122 279.982421875\n",
      "123 265.9139404296875\n",
      "124 252.5754852294922\n",
      "125 239.92910766601562\n",
      "126 227.93527221679688\n",
      "127 216.55886840820312\n",
      "128 205.76889038085938\n",
      "129 195.53387451171875\n",
      "130 185.8177490234375\n",
      "131 176.60433959960938\n",
      "132 167.86172485351562\n",
      "133 159.56332397460938\n",
      "134 151.6847381591797\n",
      "135 144.204345703125\n",
      "136 137.1055908203125\n",
      "137 130.3649139404297\n",
      "138 123.96360778808594\n",
      "139 117.88265991210938\n",
      "140 112.10973358154297\n",
      "141 106.62715911865234\n",
      "142 101.41864013671875\n",
      "143 96.47183990478516\n",
      "144 91.77213287353516\n",
      "145 87.30618286132812\n",
      "146 83.06300354003906\n",
      "147 79.03138732910156\n",
      "148 75.19932556152344\n",
      "149 71.55679321289062\n",
      "150 68.09529113769531\n",
      "151 64.80371856689453\n",
      "152 61.67667770385742\n",
      "153 58.70307922363281\n",
      "154 55.8755989074707\n",
      "155 53.187034606933594\n",
      "156 50.63346862792969\n",
      "157 48.205238342285156\n",
      "158 45.896522521972656\n",
      "159 43.69819259643555\n",
      "160 41.609161376953125\n",
      "161 39.62217712402344\n",
      "162 37.73102569580078\n",
      "163 35.932220458984375\n",
      "164 34.22123336791992\n",
      "165 32.59349822998047\n",
      "166 31.043415069580078\n",
      "167 29.56901741027832\n",
      "168 28.1663818359375\n",
      "169 26.83112144470215\n",
      "170 25.560543060302734\n",
      "171 24.350547790527344\n",
      "172 23.199310302734375\n",
      "173 22.104156494140625\n",
      "174 21.060884475708008\n",
      "175 20.067543029785156\n",
      "176 19.121856689453125\n",
      "177 18.22167205810547\n",
      "178 17.364408493041992\n",
      "179 16.548795700073242\n",
      "180 15.771167755126953\n",
      "181 15.03109359741211\n",
      "182 14.326282501220703\n",
      "183 13.654984474182129\n",
      "184 13.016220092773438\n",
      "185 12.407217025756836\n",
      "186 11.827035903930664\n",
      "187 11.274632453918457\n",
      "188 10.748069763183594\n",
      "189 10.24715805053711\n",
      "190 9.76955795288086\n",
      "191 9.314933776855469\n",
      "192 8.881114959716797\n",
      "193 8.468491554260254\n",
      "194 8.075106620788574\n",
      "195 7.700073719024658\n",
      "196 7.342761516571045\n",
      "197 7.0024590492248535\n",
      "198 6.677896499633789\n",
      "199 6.368664741516113\n",
      "200 6.073827743530273\n",
      "201 5.793337821960449\n",
      "202 5.525592803955078\n",
      "203 5.270376682281494\n",
      "204 5.026975631713867\n",
      "205 4.795180797576904\n",
      "206 4.5743489265441895\n",
      "207 4.363325119018555\n",
      "208 4.162477970123291\n",
      "209 3.971059799194336\n",
      "210 3.7885594367980957\n",
      "211 3.6143088340759277\n",
      "212 3.448544979095459\n",
      "213 3.2901227474212646\n",
      "214 3.139432668685913\n",
      "215 2.9955172538757324\n",
      "216 2.8581342697143555\n",
      "217 2.7274279594421387\n",
      "218 2.6025497913360596\n",
      "219 2.4835503101348877\n",
      "220 2.370171546936035\n",
      "221 2.261782169342041\n",
      "222 2.158684730529785\n",
      "223 2.0600316524505615\n",
      "224 1.9661320447921753\n",
      "225 1.876508116722107\n",
      "226 1.791086196899414\n",
      "227 1.7095179557800293\n",
      "228 1.6316713094711304\n",
      "229 1.557474970817566\n",
      "230 1.4867637157440186\n",
      "231 1.419276237487793\n",
      "232 1.354769229888916\n",
      "233 1.293290138244629\n",
      "234 1.234635591506958\n",
      "235 1.178699016571045\n",
      "236 1.1251038312911987\n",
      "237 1.07422935962677\n",
      "238 1.0257041454315186\n",
      "239 0.9793090224266052\n",
      "240 0.9350168108940125\n",
      "241 0.8927714824676514\n",
      "242 0.8524354100227356\n",
      "243 0.8139870762825012\n",
      "244 0.7772958874702454\n",
      "245 0.7421576380729675\n",
      "246 0.7086973786354065\n",
      "247 0.6767895221710205\n",
      "248 0.6462739109992981\n",
      "249 0.6171650886535645\n",
      "250 0.5894423723220825\n",
      "251 0.5628644227981567\n",
      "252 0.5375347137451172\n",
      "253 0.5134381055831909\n",
      "254 0.49034202098846436\n",
      "255 0.46836960315704346\n",
      "256 0.44732335209846497\n",
      "257 0.427275687456131\n",
      "258 0.4081576466560364\n",
      "259 0.389889121055603\n",
      "260 0.37239283323287964\n",
      "261 0.3557654023170471\n",
      "262 0.3398284316062927\n",
      "263 0.32465022802352905\n",
      "264 0.3100437521934509\n",
      "265 0.29620975255966187\n",
      "266 0.28298869729042053\n",
      "267 0.2703897953033447\n",
      "268 0.25836271047592163\n",
      "269 0.24674314260482788\n",
      "270 0.23577880859375\n",
      "271 0.2253105193376541\n",
      "272 0.21522532403469086\n",
      "273 0.20564886927604675\n",
      "274 0.19652679562568665\n",
      "275 0.18774673342704773\n",
      "276 0.17944243550300598\n",
      "277 0.1714325100183487\n",
      "278 0.16380104422569275\n",
      "279 0.15654760599136353\n",
      "280 0.14960093796253204\n",
      "281 0.14294561743736267\n",
      "282 0.13659828901290894\n",
      "283 0.1305811107158661\n",
      "284 0.12478317320346832\n",
      "285 0.11925586313009262\n",
      "286 0.11392490565776825\n",
      "287 0.1088918074965477\n",
      "288 0.10409354418516159\n",
      "289 0.09949181973934174\n",
      "290 0.09507638961076736\n",
      "291 0.09086795151233673\n",
      "292 0.08687962591648102\n",
      "293 0.08304020762443542\n",
      "294 0.07936427742242813\n",
      "295 0.07586940377950668\n",
      "296 0.07253238558769226\n",
      "297 0.06933058053255081\n",
      "298 0.06628863513469696\n",
      "299 0.06337819248437881\n",
      "300 0.06058435142040253\n",
      "301 0.05792558193206787\n",
      "302 0.055363137274980545\n",
      "303 0.05293384939432144\n",
      "304 0.05058356747031212\n",
      "305 0.048379816114902496\n",
      "306 0.04625220596790314\n",
      "307 0.044224273413419724\n",
      "308 0.04228176921606064\n",
      "309 0.040416982024908066\n",
      "310 0.03863818570971489\n",
      "311 0.036946363747119904\n",
      "312 0.03535792604088783\n",
      "313 0.033802106976509094\n",
      "314 0.032332826405763626\n",
      "315 0.030916620045900345\n",
      "316 0.029571956023573875\n",
      "317 0.028279194608330727\n",
      "318 0.027050768956542015\n",
      "319 0.025882046669721603\n",
      "320 0.024759389460086823\n",
      "321 0.023671306669712067\n",
      "322 0.022639678791165352\n",
      "323 0.021663226187229156\n",
      "324 0.020730169489979744\n",
      "325 0.019818881526589394\n",
      "326 0.018977398052811623\n",
      "327 0.018147140741348267\n",
      "328 0.01737927831709385\n",
      "329 0.0166302602738142\n",
      "330 0.01591021567583084\n",
      "331 0.015225671231746674\n",
      "332 0.014572998508810997\n",
      "333 0.013955187052488327\n",
      "334 0.013352420181035995\n",
      "335 0.012788240797817707\n",
      "336 0.012243801727890968\n",
      "337 0.01173015683889389\n",
      "338 0.011222127825021744\n",
      "339 0.010741563513875008\n",
      "340 0.01028999499976635\n",
      "341 0.009849503636360168\n",
      "342 0.009434124454855919\n",
      "343 0.009038195013999939\n",
      "344 0.008661511354148388\n",
      "345 0.008294963277876377\n",
      "346 0.007953199557960033\n",
      "347 0.007617280352860689\n",
      "348 0.007300426252186298\n",
      "349 0.006999211385846138\n",
      "350 0.006714510731399059\n",
      "351 0.006434271577745676\n",
      "352 0.00616661598905921\n",
      "353 0.005916339810937643\n",
      "354 0.005675591062754393\n",
      "355 0.005444851238280535\n",
      "356 0.005222493316978216\n",
      "357 0.005007926840335131\n",
      "358 0.004806864075362682\n",
      "359 0.004615717101842165\n",
      "360 0.004430478438735008\n",
      "361 0.004250224679708481\n",
      "362 0.004085418302565813\n",
      "363 0.003923157230019569\n",
      "364 0.0037643006071448326\n",
      "365 0.003618178889155388\n",
      "366 0.0034728781320154667\n",
      "367 0.003338271053507924\n",
      "368 0.003208016511052847\n",
      "369 0.0030845256987959146\n",
      "370 0.0029626796022057533\n",
      "371 0.002851099707186222\n",
      "372 0.002739295829087496\n",
      "373 0.0026350277476012707\n",
      "374 0.0025392016395926476\n",
      "375 0.002443058416247368\n",
      "376 0.002352755982428789\n",
      "377 0.002261053305119276\n",
      "378 0.00217742333188653\n",
      "379 0.0020973074715584517\n",
      "380 0.002019488951191306\n",
      "381 0.0019466273952275515\n",
      "382 0.0018754793563857675\n",
      "383 0.0018099076114594936\n",
      "384 0.0017430869629606605\n",
      "385 0.001681534224189818\n",
      "386 0.0016217685770243406\n",
      "387 0.0015638958429917693\n",
      "388 0.0015099442098289728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "389 0.0014583675656467676\n",
      "390 0.0014074859209358692\n",
      "391 0.001359925139695406\n",
      "392 0.001312759704887867\n",
      "393 0.0012702061794698238\n",
      "394 0.0012273528845980763\n",
      "395 0.0011851646704599261\n",
      "396 0.0011452364269644022\n",
      "397 0.0011088816681876779\n",
      "398 0.0010719896527007222\n",
      "399 0.001038785558193922\n",
      "400 0.0010048933327198029\n",
      "401 0.0009719434310682118\n",
      "402 0.0009411567589268088\n",
      "403 0.0009119894239120185\n",
      "404 0.0008824139949865639\n",
      "405 0.0008561529102735221\n",
      "406 0.0008304755319841206\n",
      "407 0.0008055629441514611\n",
      "408 0.0007815311546437442\n",
      "409 0.000758456124458462\n",
      "410 0.0007338350987993181\n",
      "411 0.0007123203831724823\n",
      "412 0.000692638277541846\n",
      "413 0.0006712055183015764\n",
      "414 0.000650823290925473\n",
      "415 0.0006328407325781882\n",
      "416 0.000615122786257416\n",
      "417 0.0005976948887109756\n",
      "418 0.0005814839387312531\n",
      "419 0.0005641697207465768\n",
      "420 0.0005480671534314752\n",
      "421 0.0005319354822859168\n",
      "422 0.000517569191288203\n",
      "423 0.0005045298021286726\n",
      "424 0.000489511585328728\n",
      "425 0.00047623959835618734\n",
      "426 0.00046370894415304065\n",
      "427 0.00045139837311580777\n",
      "428 0.0004393839626573026\n",
      "429 0.00042779656359925866\n",
      "430 0.0004178558592684567\n",
      "431 0.0004054478195030242\n",
      "432 0.00039490777999162674\n",
      "433 0.00038458246854133904\n",
      "434 0.00037458661245182157\n",
      "435 0.00036531288060359657\n",
      "436 0.00035682128509506583\n",
      "437 0.0003472719108685851\n",
      "438 0.0003389122721273452\n",
      "439 0.00033116358099505305\n",
      "440 0.00032254180405288935\n",
      "441 0.0003152262943331152\n",
      "442 0.00030765627161599696\n",
      "443 0.00030041244463063776\n",
      "444 0.0002931822673417628\n",
      "445 0.00028615002520382404\n",
      "446 0.0002793382154777646\n",
      "447 0.000272955687250942\n",
      "448 0.000266479910351336\n",
      "449 0.0002601283777039498\n",
      "450 0.0002553742961026728\n",
      "451 0.000249309407081455\n",
      "452 0.00024360792303923517\n",
      "453 0.00023767907987348735\n",
      "454 0.0002328626433154568\n",
      "455 0.00022789675858803093\n",
      "456 0.00022380062728188932\n",
      "457 0.0002180237352149561\n",
      "458 0.00021331984316930175\n",
      "459 0.00020898933871649206\n",
      "460 0.00020440496155060828\n",
      "461 0.00020033544569741935\n",
      "462 0.00019594942568801343\n",
      "463 0.00019184505799785256\n",
      "464 0.0001879802148323506\n",
      "465 0.00018428807379677892\n",
      "466 0.0001805053761927411\n",
      "467 0.00017632116214372218\n",
      "468 0.00017288314120378345\n",
      "469 0.0001695669925538823\n",
      "470 0.00016587675781920552\n",
      "471 0.0001625127042643726\n",
      "472 0.00015916767006274313\n",
      "473 0.00015645846724510193\n",
      "474 0.00015347330190706998\n",
      "475 0.00015048470231704414\n",
      "476 0.00014762106002308428\n",
      "477 0.00014470327005255967\n",
      "478 0.00014213031681720167\n",
      "479 0.00013933358422946185\n",
      "480 0.00013715907698497176\n",
      "481 0.00013456281158141792\n",
      "482 0.00013194410712458193\n",
      "483 0.00012963893823325634\n",
      "484 0.0001277008414035663\n",
      "485 0.0001246759493369609\n",
      "486 0.00012282426177989691\n",
      "487 0.00012100648018531501\n",
      "488 0.00011858082143589854\n",
      "489 0.00011634680413408205\n",
      "490 0.00011483718117233366\n",
      "491 0.00011287290544714779\n",
      "492 0.00011100430856458843\n",
      "493 0.00010878535977099091\n",
      "494 0.00010688314068829641\n",
      "495 0.00010487181134521961\n",
      "496 0.00010315181134501472\n",
      "497 0.00010143280087504536\n",
      "498 0.00010001510236179456\n",
      "499 9.860527643468231e-05\n"
     ]
    }
   ],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10   # N表示训练数据的个数， D_in表示输入的特征数 H是中间层，\n",
    "\n",
    "# 随机创建一下训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "w1 = torch.randn(D_in, H)\n",
    "w2 = torch.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for it in range(500):\n",
    "    # forward pass\n",
    "    h = x.mm(w1)   # N*H\n",
    "    h_relu = h.clamp(min=0)   # 这类似一个夹子， 只需要最小值是0就可以了\n",
    "    y_pred = h_relu.mm(w2)   # N*D_out\n",
    "    \n",
    "    # compute loss\n",
    "    loss = (y_pred-y).pow(2).sum().item()\n",
    "    print(it, loss)\n",
    "    \n",
    "    # Backward pass\n",
    "    # compute the gradient\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.T)\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    # update weithts of w1 and w2\n",
    "    w1 -= learning_rate  * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单的autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T03:46:29.191655Z",
     "start_time": "2020-02-25T03:46:29.182728Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1., requires_grad=True)\n",
    "w = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(3., requires_grad=True)\n",
    "\n",
    "y = w * x + b   #  y = 2*1+3\n",
    "\n",
    "y.backward()\n",
    "# dy / dw = x\n",
    "print(w.grad)\n",
    "print(b.grad)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: Tensor和autograd\n",
    "-------------------------------\n",
    "\n",
    "PyTorch的一个重要功能就是autograd，也就是说只要定义了forward pass(前向神经网络)，计算了loss之后，PyTorch可以自动求导计算模型所有参数的梯度。\n",
    "\n",
    "一个PyTorch的Tensor表示计算图中的一个节点。如果``x``是一个Tensor并且``x.requires_grad=True``那么``x.grad``是另一个储存着``x``当前梯度(相对于一个scalar，常常是loss)的向量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T04:02:00.968928Z",
     "start_time": "2020-02-25T04:02:00.262666Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 31616036.0\n",
      "1 26910740.0\n",
      "2 23115910.0\n",
      "3 18284996.0\n",
      "4 13019692.0\n",
      "5 8476639.0\n",
      "6 5308443.0\n",
      "7 3353285.5\n",
      "8 2222494.75\n",
      "9 1566421.125\n",
      "10 1171352.875\n",
      "11 917815.75\n",
      "12 743820.8125\n",
      "13 616951.625\n",
      "14 519623.0625\n",
      "15 442455.40625\n",
      "16 379774.625\n",
      "17 328044.9375\n",
      "18 284879.25\n",
      "19 248502.0\n",
      "20 217591.625\n",
      "21 191160.09375\n",
      "22 168469.421875\n",
      "23 148881.375\n",
      "24 131900.125\n",
      "25 117140.6015625\n",
      "26 104250.53125\n",
      "27 92962.9375\n",
      "28 83061.46875\n",
      "29 74354.421875\n",
      "30 66671.0078125\n",
      "31 59879.7578125\n",
      "32 53865.0546875\n",
      "33 48525.4296875\n",
      "34 43776.35546875\n",
      "35 39549.33203125\n",
      "36 35775.55078125\n",
      "37 32400.7109375\n",
      "38 29378.3125\n",
      "39 26669.7421875\n",
      "40 24238.06640625\n",
      "41 22050.6796875\n",
      "42 20081.69921875\n",
      "43 18305.859375\n",
      "44 16704.373046875\n",
      "45 15258.2119140625\n",
      "46 13951.43359375\n",
      "47 12767.556640625\n",
      "48 11694.2470703125\n",
      "49 10720.9619140625\n",
      "50 9835.8359375\n",
      "51 9031.12890625\n",
      "52 8298.5986328125\n",
      "53 7631.13330078125\n",
      "54 7022.35205078125\n",
      "55 6466.5947265625\n",
      "56 5959.0322265625\n",
      "57 5495.00341796875\n",
      "58 5070.2724609375\n",
      "59 4681.19775390625\n",
      "60 4324.720703125\n",
      "61 3997.6611328125\n",
      "62 3697.555908203125\n",
      "63 3421.799072265625\n",
      "64 3168.32470703125\n",
      "65 2935.172119140625\n",
      "66 2720.57958984375\n",
      "67 2522.98876953125\n",
      "68 2340.937255859375\n",
      "69 2173.0595703125\n",
      "70 2018.208984375\n",
      "71 1875.2713623046875\n",
      "72 1743.20654296875\n",
      "73 1621.173828125\n",
      "74 1508.3189697265625\n",
      "75 1403.9267578125\n",
      "76 1307.3116455078125\n",
      "77 1217.846923828125\n",
      "78 1134.960693359375\n",
      "79 1058.13623046875\n",
      "80 986.904052734375\n",
      "81 920.802490234375\n",
      "82 859.4679565429688\n",
      "83 802.5194702148438\n",
      "84 749.6064453125\n",
      "85 700.4557495117188\n",
      "86 654.745849609375\n",
      "87 612.2315673828125\n",
      "88 572.660400390625\n",
      "89 535.8248291015625\n",
      "90 501.5833740234375\n",
      "91 469.75433349609375\n",
      "92 440.09600830078125\n",
      "93 412.4461364746094\n",
      "94 386.66064453125\n",
      "95 362.5968322753906\n",
      "96 340.13262939453125\n",
      "97 319.1585388183594\n",
      "98 299.5728759765625\n",
      "99 281.27001953125\n",
      "100 264.15875244140625\n",
      "101 248.16070556640625\n",
      "102 233.19827270507812\n",
      "103 219.20278930664062\n",
      "104 206.102783203125\n",
      "105 193.83509826660156\n",
      "106 182.34991455078125\n",
      "107 171.58541870117188\n",
      "108 161.49961853027344\n",
      "109 152.04574584960938\n",
      "110 143.18185424804688\n",
      "111 134.8680877685547\n",
      "112 127.06709289550781\n",
      "113 119.74840545654297\n",
      "114 112.87799072265625\n",
      "115 106.42625427246094\n",
      "116 100.36722564697266\n",
      "117 94.67412567138672\n",
      "118 89.3243408203125\n",
      "119 84.29747772216797\n",
      "120 79.57012939453125\n",
      "121 75.12322235107422\n",
      "122 70.93877410888672\n",
      "123 67.0028076171875\n",
      "124 63.300254821777344\n",
      "125 59.81364059448242\n",
      "126 56.52862548828125\n",
      "127 53.43528366088867\n",
      "128 50.5225830078125\n",
      "129 47.77666091918945\n",
      "130 45.18916320800781\n",
      "131 42.750736236572266\n",
      "132 40.450714111328125\n",
      "133 38.282684326171875\n",
      "134 36.23720932006836\n",
      "135 34.306304931640625\n",
      "136 32.48383331298828\n",
      "137 30.763124465942383\n",
      "138 29.13997459411621\n",
      "139 27.606853485107422\n",
      "140 26.159183502197266\n",
      "141 24.790740966796875\n",
      "142 23.49781036376953\n",
      "143 22.27651596069336\n",
      "144 21.121835708618164\n",
      "145 20.029409408569336\n",
      "146 18.998310089111328\n",
      "147 18.023651123046875\n",
      "148 17.10226058959961\n",
      "149 16.229694366455078\n",
      "150 15.4039945602417\n",
      "151 14.622472763061523\n",
      "152 13.882122993469238\n",
      "153 13.181145668029785\n",
      "154 12.517721176147461\n",
      "155 11.889191627502441\n",
      "156 11.293526649475098\n",
      "157 10.729458808898926\n",
      "158 10.194975852966309\n",
      "159 9.687942504882812\n",
      "160 9.207599639892578\n",
      "161 8.752317428588867\n",
      "162 8.320124626159668\n",
      "163 7.910455703735352\n",
      "164 7.521895408630371\n",
      "165 7.1530938148498535\n",
      "166 6.8030266761779785\n",
      "167 6.471005916595459\n",
      "168 6.155949592590332\n",
      "169 5.857063293457031\n",
      "170 5.573080062866211\n",
      "171 5.3032331466674805\n",
      "172 5.047306060791016\n",
      "173 4.804249286651611\n",
      "174 4.573103427886963\n",
      "175 4.353639602661133\n",
      "176 4.145384788513184\n",
      "177 3.946963310241699\n",
      "178 3.7587733268737793\n",
      "179 3.580097198486328\n",
      "180 3.4094252586364746\n",
      "181 3.2478859424591064\n",
      "182 3.0941154956817627\n",
      "183 2.9477570056915283\n",
      "184 2.808685541152954\n",
      "185 2.6763434410095215\n",
      "186 2.5505101680755615\n",
      "187 2.4307777881622314\n",
      "188 2.3169169425964355\n",
      "189 2.2085964679718018\n",
      "190 2.105273485183716\n",
      "191 2.00710391998291\n",
      "192 1.9136565923690796\n",
      "193 1.8248258829116821\n",
      "194 1.74021315574646\n",
      "195 1.6594963073730469\n",
      "196 1.5827428102493286\n",
      "197 1.5096982717514038\n",
      "198 1.4398789405822754\n",
      "199 1.3736505508422852\n",
      "200 1.310426950454712\n",
      "201 1.2501248121261597\n",
      "202 1.1928608417510986\n",
      "203 1.138222098350525\n",
      "204 1.0861796140670776\n",
      "205 1.0364373922348022\n",
      "206 0.9891724586486816\n",
      "207 0.9441483020782471\n",
      "208 0.9011187553405762\n",
      "209 0.860100269317627\n",
      "210 0.8210200667381287\n",
      "211 0.7838687300682068\n",
      "212 0.748332142829895\n",
      "213 0.7144594192504883\n",
      "214 0.6822406053543091\n",
      "215 0.6513510346412659\n",
      "216 0.6220540404319763\n",
      "217 0.5940162539482117\n",
      "218 0.5673888325691223\n",
      "219 0.5419501066207886\n",
      "220 0.5175301432609558\n",
      "221 0.4944048225879669\n",
      "222 0.4722183048725128\n",
      "223 0.45108604431152344\n",
      "224 0.4309169054031372\n",
      "225 0.4117244482040405\n",
      "226 0.3933183252811432\n",
      "227 0.3758612275123596\n",
      "228 0.3590948283672333\n",
      "229 0.3430953621864319\n",
      "230 0.32789286971092224\n",
      "231 0.3133128881454468\n",
      "232 0.29940977692604065\n",
      "233 0.2861787676811218\n",
      "234 0.2734585702419281\n",
      "235 0.2614217698574066\n",
      "236 0.24985922873020172\n",
      "237 0.23881874978542328\n",
      "238 0.22828929126262665\n",
      "239 0.21821066737174988\n",
      "240 0.2086075395345688\n",
      "241 0.199388325214386\n",
      "242 0.19064456224441528\n",
      "243 0.18222777545452118\n",
      "244 0.17425690591335297\n",
      "245 0.16662071645259857\n",
      "246 0.15930180251598358\n",
      "247 0.15235008299350739\n",
      "248 0.14570288360118866\n",
      "249 0.13927781581878662\n",
      "250 0.13321374356746674\n",
      "251 0.12740765511989594\n",
      "252 0.12185689806938171\n",
      "253 0.11655990779399872\n",
      "254 0.11146850883960724\n",
      "255 0.1065966933965683\n",
      "256 0.10196280479431152\n",
      "257 0.09753528237342834\n",
      "258 0.09329473972320557\n",
      "259 0.08924910426139832\n",
      "260 0.08537084609270096\n",
      "261 0.08165812492370605\n",
      "262 0.07812928408384323\n",
      "263 0.07475627213716507\n",
      "264 0.07151356339454651\n",
      "265 0.06842313706874847\n",
      "266 0.06547536700963974\n",
      "267 0.06265315413475037\n",
      "268 0.059953197836875916\n",
      "269 0.05734270438551903\n",
      "270 0.05487143620848656\n",
      "271 0.052529994398355484\n",
      "272 0.050277404487133026\n",
      "273 0.04808846488595009\n",
      "274 0.046016380190849304\n",
      "275 0.04403533786535263\n",
      "276 0.04215070977807045\n",
      "277 0.040343452244997025\n",
      "278 0.038626570254564285\n",
      "279 0.03696573153138161\n",
      "280 0.035401031374931335\n",
      "281 0.03388383984565735\n",
      "282 0.03242720663547516\n",
      "283 0.031038541346788406\n",
      "284 0.02973189763724804\n",
      "285 0.028464170172810555\n",
      "286 0.0272511038929224\n",
      "287 0.02609473653137684\n",
      "288 0.024983052164316177\n",
      "289 0.023919325321912766\n",
      "290 0.022908637300133705\n",
      "291 0.021945137530565262\n",
      "292 0.021015826612710953\n",
      "293 0.020129624754190445\n",
      "294 0.019276577979326248\n",
      "295 0.01847226917743683\n",
      "296 0.017690308392047882\n",
      "297 0.016947316005825996\n",
      "298 0.016238171607255936\n",
      "299 0.015553261153399944\n",
      "300 0.014912876300513744\n",
      "301 0.01428104005753994\n",
      "302 0.013682130724191666\n",
      "303 0.013098464347422123\n",
      "304 0.012561270035803318\n",
      "305 0.012041035108268261\n",
      "306 0.011541193351149559\n",
      "307 0.011062497273087502\n",
      "308 0.010606363415718079\n",
      "309 0.010171515867114067\n",
      "310 0.009747955948114395\n",
      "311 0.009351743385195732\n",
      "312 0.008960632607340813\n",
      "313 0.008594546467065811\n",
      "314 0.008243467658758163\n",
      "315 0.007910254411399364\n",
      "316 0.007590368390083313\n",
      "317 0.007280453108251095\n",
      "318 0.006985805928707123\n",
      "319 0.006704295985400677\n",
      "320 0.006427524611353874\n",
      "321 0.006169037893414497\n",
      "322 0.005918927025049925\n",
      "323 0.0056824469938874245\n",
      "324 0.0054641482420265675\n",
      "325 0.005242600105702877\n",
      "326 0.005037660710513592\n",
      "327 0.004842088557779789\n",
      "328 0.004653828218579292\n",
      "329 0.00446674507111311\n",
      "330 0.004291945602744818\n",
      "331 0.004124599043279886\n",
      "332 0.003965412266552448\n",
      "333 0.0038113819900900126\n",
      "334 0.0036654085852205753\n",
      "335 0.003523001680150628\n",
      "336 0.0033875913359224796\n",
      "337 0.0032619142439216375\n",
      "338 0.0031399414874613285\n",
      "339 0.003019302384927869\n",
      "340 0.0029020695947110653\n",
      "341 0.002796870656311512\n",
      "342 0.002691972069442272\n",
      "343 0.0025879594031721354\n",
      "344 0.0024935733526945114\n",
      "345 0.002402425743639469\n",
      "346 0.0023160469718277454\n",
      "347 0.002229169709607959\n",
      "348 0.0021482903975993395\n",
      "349 0.0020710874814540148\n",
      "350 0.0019982948433607817\n",
      "351 0.001927239471115172\n",
      "352 0.0018565085483714938\n",
      "353 0.0017905778950080276\n",
      "354 0.0017259679734706879\n",
      "355 0.001664851326495409\n",
      "356 0.001607681275345385\n",
      "357 0.0015505191404372454\n",
      "358 0.0014981827698647976\n",
      "359 0.0014478820376098156\n",
      "360 0.001397615997120738\n",
      "361 0.0013526156544685364\n",
      "362 0.0013035634765401483\n",
      "363 0.0012610109988600016\n",
      "364 0.0012166601372882724\n",
      "365 0.0011782677611336112\n",
      "366 0.0011388043640181422\n",
      "367 0.0011009504087269306\n",
      "368 0.0010672450298443437\n",
      "369 0.0010314143728464842\n",
      "370 0.0009991814149543643\n",
      "371 0.0009671428124420345\n",
      "372 0.0009367736056447029\n",
      "373 0.0009069238440133631\n",
      "374 0.0008791133295744658\n",
      "375 0.0008524365839548409\n",
      "376 0.0008247399819083512\n",
      "377 0.0008001372334547341\n",
      "378 0.000774940534029156\n",
      "379 0.0007518752827309072\n",
      "380 0.0007286891341209412\n",
      "381 0.0007068050326779485\n",
      "382 0.0006872552912682295\n",
      "383 0.000667815562337637\n",
      "384 0.0006479279254563153\n",
      "385 0.0006294759805314243\n",
      "386 0.0006117207231000066\n",
      "387 0.0005942563875578344\n",
      "388 0.0005772301810793579\n",
      "389 0.0005604014149866998\n",
      "390 0.0005446317372843623\n",
      "391 0.0005297302850522101\n",
      "392 0.0005157315172255039\n",
      "393 0.0005012978799641132\n",
      "394 0.0004889725241810083\n",
      "395 0.0004751912783831358\n",
      "396 0.0004625501751434058\n",
      "397 0.00045077523100189865\n",
      "398 0.0004382119223009795\n",
      "399 0.00042648438829928637\n",
      "400 0.00041433554724790156\n",
      "401 0.0004044648085255176\n",
      "402 0.00039380465750582516\n",
      "403 0.0003836782416328788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404 0.0003731283650267869\n",
      "405 0.0003634344320744276\n",
      "406 0.00035453628515824676\n",
      "407 0.0003459243453107774\n",
      "408 0.0003372969222255051\n",
      "409 0.00032950006425380707\n",
      "410 0.0003206252004019916\n",
      "411 0.00031325581949204206\n",
      "412 0.0003054137050639838\n",
      "413 0.00029831999563612044\n",
      "414 0.00029064068803563714\n",
      "415 0.00028391333762556314\n",
      "416 0.0002772982115857303\n",
      "417 0.0002702448982745409\n",
      "418 0.0002639677841216326\n",
      "419 0.0002585936163086444\n",
      "420 0.00025243283016607165\n",
      "421 0.00024709024000912905\n",
      "422 0.00024126422067638487\n",
      "423 0.00023520219838246703\n",
      "424 0.00023066128778737038\n",
      "425 0.00022541631187777966\n",
      "426 0.00022062752395868301\n",
      "427 0.00021593015117105097\n",
      "428 0.00021119722805451602\n",
      "429 0.0002068128960672766\n",
      "430 0.0002025668800342828\n",
      "431 0.00019842162146233022\n",
      "432 0.00019428404630161822\n",
      "433 0.00018990600074175745\n",
      "434 0.00018565051141195\n",
      "435 0.00018189370166510344\n",
      "436 0.0001781819883035496\n",
      "437 0.00017459750233683735\n",
      "438 0.00017080614634323865\n",
      "439 0.00016727806359995157\n",
      "440 0.00016375993436668068\n",
      "441 0.0001608405145816505\n",
      "442 0.00015785489813424647\n",
      "443 0.00015437927504535764\n",
      "444 0.00015119428280740976\n",
      "445 0.00014845024270471185\n",
      "446 0.00014521968842018396\n",
      "447 0.00014252662367653102\n",
      "448 0.0001401678309775889\n",
      "449 0.00013735215179622173\n",
      "450 0.0001350754318991676\n",
      "451 0.00013265811139717698\n",
      "452 0.00012990405957680196\n",
      "453 0.00012766289000865072\n",
      "454 0.00012568784586619586\n",
      "455 0.00012299294758122414\n",
      "456 0.0001210862465086393\n",
      "457 0.0001186744193546474\n",
      "458 0.00011640103912213817\n",
      "459 0.00011451315367594361\n",
      "460 0.00011218048894079402\n",
      "461 0.00011042522237403318\n",
      "462 0.0001083657843992114\n",
      "463 0.00010644131543813273\n",
      "464 0.00010499360359972343\n",
      "465 0.00010333737009204924\n",
      "466 0.00010100382496602833\n",
      "467 9.965640492737293e-05\n",
      "468 9.799616236705333e-05\n",
      "469 9.642994700698182e-05\n",
      "470 9.481612505624071e-05\n",
      "471 9.346255683340132e-05\n",
      "472 9.199312626151368e-05\n",
      "473 9.035713446792215e-05\n",
      "474 8.873414481058717e-05\n",
      "475 8.752857684157789e-05\n",
      "476 8.62411834532395e-05\n",
      "477 8.488442108500749e-05\n",
      "478 8.372005686396733e-05\n",
      "479 8.254999556811526e-05\n",
      "480 8.133625669870526e-05\n",
      "481 8.013725164346397e-05\n",
      "482 7.893650035839528e-05\n",
      "483 7.784385525155813e-05\n",
      "484 7.645085133844987e-05\n",
      "485 7.534398173447698e-05\n",
      "486 7.42996417102404e-05\n",
      "487 7.34580826247111e-05\n",
      "488 7.219264807645231e-05\n",
      "489 7.100157381501049e-05\n",
      "490 6.988077802816406e-05\n",
      "491 6.88566142343916e-05\n",
      "492 6.810362538089976e-05\n",
      "493 6.707452848786488e-05\n",
      "494 6.610209675272927e-05\n",
      "495 6.541100447066128e-05\n",
      "496 6.459067662945017e-05\n",
      "497 6.358227255987003e-05\n",
      "498 6.26549226581119e-05\n",
      "499 6.171147106215358e-05\n"
     ]
    }
   ],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10   # N表示训练数据的个数， D_in表示输入的特征数 H是中间层，\n",
    "\n",
    "# 随机创建一下训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "w1 = torch.randn(D_in, H, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for it in range(500):\n",
    "    # forward pass\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    # compute lossafd\n",
    "    loss = (y_pred-y).pow(2).sum()  #  computation graph\n",
    "    print(it, loss.item())\n",
    "    \n",
    "    # Backward pass\n",
    "    # compute the gradient\n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # update weithts of w1 and w2\n",
    "        w1 -= learning_rate  * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T03:59:54.835219Z",
     "start_time": "2020-02-25T03:59:54.821256Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -4759.0889,  -3616.0903,  -3843.6018,  ..., -11072.5781,\n",
       "          -2451.3350,  -1766.1671],\n",
       "        [ -7967.2842,    526.7576,   -284.1399,  ...,   4579.7217,\n",
       "          -3875.2419,  -6572.7490],\n",
       "        [  3489.5291,  -5378.6558,  31709.0645,  ...,   3620.1050,\n",
       "         -12476.5996,  10473.9258],\n",
       "        ...,\n",
       "        [  6210.0361,   3189.0911,  -2162.0293,  ..., -10237.3037,\n",
       "          -3920.1753,   1129.4575],\n",
       "        [  6645.1338,   -698.3760,   3044.2788,  ...,   1304.6511,\n",
       "           2051.7354,  -5745.3848],\n",
       "        [   941.8529,   2244.2927,  12504.7578,  ..., -11249.3877,\n",
       "           9321.3955,   9463.3652]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10   # N表示训练数据的个数， D_in表示输入的特征数 H是中间层，\n",
    "\n",
    "# 随机创建一下训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "\n",
    "w1 = torch.randn(D_in, H, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, requires_grad=True)\n",
    "\n",
    "y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "# compute loss\n",
    "loss = (y_pred-y).pow(2).sum()  #  computation graph\n",
    "loss.backward()\n",
    "\n",
    "w1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T04:00:47.118245Z",
     "start_time": "2020-02-25T04:00:47.108273Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -47590.8945,  -36160.9023,  -38436.0195,  ..., -110725.7812,\n",
       "          -24513.3516,  -17661.6719],\n",
       "        [ -79672.8438,    5267.5767,   -2841.3989,  ...,   45797.2227,\n",
       "          -38752.4180,  -65727.4922],\n",
       "        [  34895.2891,  -53786.5586,  317090.6562,  ...,   36201.0508,\n",
       "         -124766.0078,  104739.2500],\n",
       "        ...,\n",
       "        [  62100.3555,   31890.9141,  -21620.2930,  ..., -102373.0391,\n",
       "          -39201.7539,   11294.5742],\n",
       "        [  66451.3359,   -6983.7598,   30442.7910,  ...,   13046.5127,\n",
       "           20517.3516,  -57453.8438],\n",
       "        [   9418.5293,   22442.9277,  125047.5781,  ..., -112493.8906,\n",
       "           93213.9609,   94633.6641]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "# compute loss\n",
    "loss = (y_pred-y).pow(2).sum()  #  computation graph\n",
    "loss.backward()\n",
    "\n",
    "w1.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: nn\n",
    "-----------\n",
    "\n",
    "\n",
    "这次我们使用PyTorch中nn这个库来构建网络。\n",
    "用PyTorch autograd来构建计算图和计算gradients，\n",
    "然后PyTorch会帮我们自动计算gradient。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T04:18:13.734036Z",
     "start_time": "2020-02-25T04:18:12.812501Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 35238972.0\n",
      "1 33932324.0\n",
      "2 37197840.0\n",
      "3 38079456.0\n",
      "4 31585246.0\n",
      "5 20214708.0\n",
      "6 10358612.0\n",
      "7 4953301.5\n",
      "8 2588858.25\n",
      "9 1606277.25\n",
      "10 1155294.5\n",
      "11 908820.375\n",
      "12 748443.25\n",
      "13 631018.5\n",
      "14 538933.125\n",
      "15 464240.78125\n",
      "16 402280.40625\n",
      "17 350378.40625\n",
      "18 306484.4375\n",
      "19 269148.40625\n",
      "20 237199.09375\n",
      "21 209730.34375\n",
      "22 186008.375\n",
      "23 165437.0625\n",
      "24 147528.90625\n",
      "25 131894.3125\n",
      "26 118202.921875\n",
      "27 106162.625\n",
      "28 95544.0\n",
      "29 86154.015625\n",
      "30 77827.5\n",
      "31 70433.21875\n",
      "32 63848.4609375\n",
      "33 57966.515625\n",
      "34 52700.1484375\n",
      "35 47975.8984375\n",
      "36 43729.375\n",
      "37 39906.140625\n",
      "38 36459.12109375\n",
      "39 33346.390625\n",
      "40 30535.552734375\n",
      "41 27990.8125\n",
      "42 25682.162109375\n",
      "43 23584.416015625\n",
      "44 21676.525390625\n",
      "45 19939.251953125\n",
      "46 18356.3984375\n",
      "47 16912.81640625\n",
      "48 15593.1484375\n",
      "49 14386.158203125\n",
      "50 13282.5390625\n",
      "51 12270.5078125\n",
      "52 11342.6025390625\n",
      "53 10491.02734375\n",
      "54 9710.822265625\n",
      "55 8995.533203125\n",
      "56 8337.0966796875\n",
      "57 7730.38671875\n",
      "58 7171.224609375\n",
      "59 6655.3779296875\n",
      "60 6179.17138671875\n",
      "61 5739.3876953125\n",
      "62 5332.9736328125\n",
      "63 4957.04052734375\n",
      "64 4609.43896484375\n",
      "65 4287.57568359375\n",
      "66 3989.54541015625\n",
      "67 3713.4130859375\n",
      "68 3457.4560546875\n",
      "69 3220.14697265625\n",
      "70 3000.22802734375\n",
      "71 2796.163330078125\n",
      "72 2606.681640625\n",
      "73 2430.71142578125\n",
      "74 2267.310546875\n",
      "75 2115.437744140625\n",
      "76 1974.1881103515625\n",
      "77 1842.800048828125\n",
      "78 1720.59423828125\n",
      "79 1606.85986328125\n",
      "80 1500.9788818359375\n",
      "81 1402.422607421875\n",
      "82 1310.6402587890625\n",
      "83 1225.166259765625\n",
      "84 1145.5009765625\n",
      "85 1071.2364501953125\n",
      "86 1001.9830322265625\n",
      "87 937.38671875\n",
      "88 877.1141357421875\n",
      "89 820.8448486328125\n",
      "90 768.3279418945312\n",
      "91 719.298828125\n",
      "92 673.5034790039062\n",
      "93 630.7373657226562\n",
      "94 590.789794921875\n",
      "95 553.4502563476562\n",
      "96 518.55078125\n",
      "97 485.9375305175781\n",
      "98 455.43365478515625\n",
      "99 426.9509582519531\n",
      "100 400.3219299316406\n",
      "101 375.40472412109375\n",
      "102 352.0877685546875\n",
      "103 330.264892578125\n",
      "104 309.8313903808594\n",
      "105 290.7057800292969\n",
      "106 272.79559326171875\n",
      "107 256.0227355957031\n",
      "108 240.30722045898438\n",
      "109 225.58383178710938\n",
      "110 211.79129028320312\n",
      "111 198.86212158203125\n",
      "112 186.75473022460938\n",
      "113 175.3970489501953\n",
      "114 164.74697875976562\n",
      "115 154.75852966308594\n",
      "116 145.39373779296875\n",
      "117 136.60995483398438\n",
      "118 128.3716278076172\n",
      "119 120.640625\n",
      "120 113.38909149169922\n",
      "121 106.58271789550781\n",
      "122 100.19526672363281\n",
      "123 94.2007064819336\n",
      "124 88.57112121582031\n",
      "125 83.2889404296875\n",
      "126 78.32575988769531\n",
      "127 73.66685485839844\n",
      "128 69.29124450683594\n",
      "129 65.18130493164062\n",
      "130 61.32134246826172\n",
      "131 57.6947021484375\n",
      "132 54.28690719604492\n",
      "133 51.084529876708984\n",
      "134 48.07482147216797\n",
      "135 45.247928619384766\n",
      "136 42.58953857421875\n",
      "137 40.09054183959961\n",
      "138 37.74134826660156\n",
      "139 35.53261947631836\n",
      "140 33.45546340942383\n",
      "141 31.502567291259766\n",
      "142 29.665456771850586\n",
      "143 27.93816375732422\n",
      "144 26.312637329101562\n",
      "145 24.784698486328125\n",
      "146 23.346338272094727\n",
      "147 21.993453979492188\n",
      "148 20.7211856842041\n",
      "149 19.522899627685547\n",
      "150 18.395675659179688\n",
      "151 17.334369659423828\n",
      "152 16.33607292175293\n",
      "153 15.395599365234375\n",
      "154 14.510906219482422\n",
      "155 13.677153587341309\n",
      "156 12.892521858215332\n",
      "157 12.154118537902832\n",
      "158 11.458287239074707\n",
      "159 10.803470611572266\n",
      "160 10.18626594543457\n",
      "161 9.605713844299316\n",
      "162 9.05897331237793\n",
      "163 8.544002532958984\n",
      "164 8.058334350585938\n",
      "165 7.601620674133301\n",
      "166 7.171202659606934\n",
      "167 6.765076637268066\n",
      "168 6.382058143615723\n",
      "169 6.021426677703857\n",
      "170 5.681726455688477\n",
      "171 5.361151218414307\n",
      "172 5.059114456176758\n",
      "173 4.7743730545043945\n",
      "174 4.506143569946289\n",
      "175 4.252992630004883\n",
      "176 4.014243125915527\n",
      "177 3.789494276046753\n",
      "178 3.5771119594573975\n",
      "179 3.3768844604492188\n",
      "180 3.188349485397339\n",
      "181 3.0102169513702393\n",
      "182 2.84242844581604\n",
      "183 2.6838436126708984\n",
      "184 2.534670352935791\n",
      "185 2.39351749420166\n",
      "186 2.260603904724121\n",
      "187 2.135035514831543\n",
      "188 2.0167999267578125\n",
      "189 1.9047555923461914\n",
      "190 1.7993619441986084\n",
      "191 1.6998424530029297\n",
      "192 1.605841875076294\n",
      "193 1.5172585248947144\n",
      "194 1.4334838390350342\n",
      "195 1.3544560670852661\n",
      "196 1.2800065279006958\n",
      "197 1.2094146013259888\n",
      "198 1.1431162357330322\n",
      "199 1.0803377628326416\n",
      "200 1.021050214767456\n",
      "201 0.9651491045951843\n",
      "202 0.9122524261474609\n",
      "203 0.8622641563415527\n",
      "204 0.8150768280029297\n",
      "205 0.7706555128097534\n",
      "206 0.728668212890625\n",
      "207 0.6887648105621338\n",
      "208 0.6513341069221497\n",
      "209 0.6157598495483398\n",
      "210 0.5822498202323914\n",
      "211 0.5505524277687073\n",
      "212 0.5206518173217773\n",
      "213 0.492400199174881\n",
      "214 0.4657847285270691\n",
      "215 0.44050076603889465\n",
      "216 0.4167270064353943\n",
      "217 0.394193559885025\n",
      "218 0.3729519546031952\n",
      "219 0.35274845361709595\n",
      "220 0.333782821893692\n",
      "221 0.3158186078071594\n",
      "222 0.2988065779209137\n",
      "223 0.2827596962451935\n",
      "224 0.26752856373786926\n",
      "225 0.25319015979766846\n",
      "226 0.23959940671920776\n",
      "227 0.2267417162656784\n",
      "228 0.2145881950855255\n",
      "229 0.20305925607681274\n",
      "230 0.19226399064064026\n",
      "231 0.18197381496429443\n",
      "232 0.17226669192314148\n",
      "233 0.16306599974632263\n",
      "234 0.15436771512031555\n",
      "235 0.1461520940065384\n",
      "236 0.1383667290210724\n",
      "237 0.13103927671909332\n",
      "238 0.12409944832324982\n",
      "239 0.11746427416801453\n",
      "240 0.11123725771903992\n",
      "241 0.10533760488033295\n",
      "242 0.09975206851959229\n",
      "243 0.0944739282131195\n",
      "244 0.08947749435901642\n",
      "245 0.0847630649805069\n",
      "246 0.08028242737054825\n",
      "247 0.07605054974555969\n",
      "248 0.07203538715839386\n",
      "249 0.06824292242527008\n",
      "250 0.06468121707439423\n",
      "251 0.0612717866897583\n",
      "252 0.05807958170771599\n",
      "253 0.05502525717020035\n",
      "254 0.05214202404022217\n",
      "255 0.04941113665699959\n",
      "256 0.046803414821624756\n",
      "257 0.04437427967786789\n",
      "258 0.04203851521015167\n",
      "259 0.03985372930765152\n",
      "260 0.03776496276259422\n",
      "261 0.03581460565328598\n",
      "262 0.033953841775655746\n",
      "263 0.032186467200517654\n",
      "264 0.030507855117321014\n",
      "265 0.028942503035068512\n",
      "266 0.027444086968898773\n",
      "267 0.026026291772723198\n",
      "268 0.024668261408805847\n",
      "269 0.023411531001329422\n",
      "270 0.022198844701051712\n",
      "271 0.021057728677988052\n",
      "272 0.019968485459685326\n",
      "273 0.018942901864647865\n",
      "274 0.01798086054623127\n",
      "275 0.017059775069355965\n",
      "276 0.01618555560708046\n",
      "277 0.015357804484665394\n",
      "278 0.0145773496478796\n",
      "279 0.013844581320881844\n",
      "280 0.013132360763847828\n",
      "281 0.012472040019929409\n",
      "282 0.011837562546133995\n",
      "283 0.011239159852266312\n",
      "284 0.010686236433684826\n",
      "285 0.010145396925508976\n",
      "286 0.009636959992349148\n",
      "287 0.009152822196483612\n",
      "288 0.008699240162968636\n",
      "289 0.008272223174571991\n",
      "290 0.007862602360546589\n",
      "291 0.007472706958651543\n",
      "292 0.007108887657523155\n",
      "293 0.00675255386158824\n",
      "294 0.006425371393561363\n",
      "295 0.0061183227226138115\n",
      "296 0.005816602148115635\n",
      "297 0.005536630749702454\n",
      "298 0.005271312780678272\n",
      "299 0.005017532035708427\n",
      "300 0.004784321412444115\n",
      "301 0.004555912688374519\n",
      "302 0.004337874241173267\n",
      "303 0.004137929528951645\n",
      "304 0.003941274248063564\n",
      "305 0.0037583636585623026\n",
      "306 0.0035864003002643585\n",
      "307 0.003418891690671444\n",
      "308 0.003260888159275055\n",
      "309 0.0031096944585442543\n",
      "310 0.0029685632325708866\n",
      "311 0.002833437407389283\n",
      "312 0.002706636907532811\n",
      "313 0.002587046241387725\n",
      "314 0.0024675349704921246\n",
      "315 0.002360279206186533\n",
      "316 0.0022575631737709045\n",
      "317 0.0021586334332823753\n",
      "318 0.002065175911411643\n",
      "319 0.0019762718584388494\n",
      "320 0.0018941443413496017\n",
      "321 0.0018131828401237726\n",
      "322 0.0017359299818053842\n",
      "323 0.0016617639921605587\n",
      "324 0.001593095948919654\n",
      "325 0.0015257705235853791\n",
      "326 0.0014649343211203814\n",
      "327 0.0014059225795790553\n",
      "328 0.0013471010606735945\n",
      "329 0.001294072368182242\n",
      "330 0.0012420187704265118\n",
      "331 0.00119397125672549\n",
      "332 0.001146548194810748\n",
      "333 0.0011014525080099702\n",
      "334 0.0010591736063361168\n",
      "335 0.0010205659782513976\n",
      "336 0.000981608871370554\n",
      "337 0.0009449768695048988\n",
      "338 0.0009095159475691617\n",
      "339 0.0008755114395171404\n",
      "340 0.0008436517673544586\n",
      "341 0.0008143792510963976\n",
      "342 0.0007837272132746875\n",
      "343 0.0007559371879324317\n",
      "344 0.0007297982810996473\n",
      "345 0.0007028811378404498\n",
      "346 0.0006774042267352343\n",
      "347 0.0006539808236993849\n",
      "348 0.0006310032913461328\n",
      "349 0.0006110680988058448\n",
      "350 0.0005899687530472875\n",
      "351 0.0005701496847905219\n",
      "352 0.0005498193786479533\n",
      "353 0.0005326528334990144\n",
      "354 0.000514902058057487\n",
      "355 0.0004990975721739233\n",
      "356 0.0004808907979167998\n",
      "357 0.00046619941713288426\n",
      "358 0.00045110739301890135\n",
      "359 0.0004371769027784467\n",
      "360 0.00042231561383232474\n",
      "361 0.0004097341443412006\n",
      "362 0.0003974813735112548\n",
      "363 0.000385183171601966\n",
      "364 0.00037357723340392113\n",
      "365 0.0003632480220403522\n",
      "366 0.0003526930813677609\n",
      "367 0.0003422418376430869\n",
      "368 0.00033217930467799306\n",
      "369 0.00032172066858038306\n",
      "370 0.0003121122717857361\n",
      "371 0.000303642445942387\n",
      "372 0.0002945020387414843\n",
      "373 0.0002867385628633201\n",
      "374 0.0002786126278806478\n",
      "375 0.00027068943018093705\n",
      "376 0.00026475219056010246\n",
      "377 0.00025683589046821\n",
      "378 0.00025126879336312413\n",
      "379 0.00024423174909316003\n",
      "380 0.00023739799507893622\n",
      "381 0.00023094101925380528\n",
      "382 0.00022487735259346664\n",
      "383 0.00021936332632321864\n",
      "384 0.00021365811699070036\n",
      "385 0.00020818102348130196\n",
      "386 0.00020324338402133435\n",
      "387 0.0001980474917218089\n",
      "388 0.00019315039389766753\n",
      "389 0.00018827570602297783\n",
      "390 0.0001838901953306049\n",
      "391 0.00017980484699364752\n",
      "392 0.0001750284864101559\n",
      "393 0.00017045694403350353\n",
      "394 0.0001664930459810421\n",
      "395 0.00016274515655823052\n",
      "396 0.00015954283298924565\n",
      "397 0.00015549268573522568\n",
      "398 0.00015191841521300375\n",
      "399 0.00014831316366326064\n",
      "400 0.00014501958503387868\n",
      "401 0.00014167922199703753\n",
      "402 0.0001390809047734365\n",
      "403 0.0001358438457828015\n",
      "404 0.00013282554573379457\n",
      "405 0.0001298956194659695\n",
      "406 0.00012742432591039687\n",
      "407 0.0001244566374225542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408 0.00012189468543510884\n",
      "409 0.00011925929720746353\n",
      "410 0.00011706296936608851\n",
      "411 0.00011444018309703097\n",
      "412 0.0001122946705436334\n",
      "413 0.00010994556942023337\n",
      "414 0.0001074321917258203\n",
      "415 0.00010558062786003575\n",
      "416 0.00010360986198065802\n",
      "417 0.00010137570643564686\n",
      "418 9.942957694875076e-05\n",
      "419 9.783959831111133e-05\n",
      "420 9.589402179699391e-05\n",
      "421 9.460079309064895e-05\n",
      "422 9.269714064430445e-05\n",
      "423 9.082678298000246e-05\n",
      "424 8.929644536692649e-05\n",
      "425 8.765991515247151e-05\n",
      "426 8.612287638243288e-05\n",
      "427 8.462368714390323e-05\n",
      "428 8.30735734780319e-05\n",
      "429 8.1426820543129e-05\n",
      "430 8.028095180634409e-05\n",
      "431 7.868491957196966e-05\n",
      "432 7.71208869991824e-05\n",
      "433 7.561939128208905e-05\n",
      "434 7.443901267834008e-05\n",
      "435 7.359635492321104e-05\n",
      "436 7.223497959785163e-05\n",
      "437 7.088216807460412e-05\n",
      "438 6.959830352570862e-05\n",
      "439 6.855576793896034e-05\n",
      "440 6.739485979778692e-05\n",
      "441 6.618841143790632e-05\n",
      "442 6.495422712760046e-05\n",
      "443 6.412174843717366e-05\n",
      "444 6.30939903203398e-05\n",
      "445 6.219585338840261e-05\n",
      "446 6.119375757407397e-05\n",
      "447 6.0304148064460605e-05\n",
      "448 5.9378242440288886e-05\n",
      "449 5.846195199410431e-05\n",
      "450 5.769521521870047e-05\n",
      "451 5.67295282962732e-05\n",
      "452 5.590613727690652e-05\n",
      "453 5.512672214535996e-05\n",
      "454 5.443689587991685e-05\n",
      "455 5.371125371311791e-05\n",
      "456 5.288105603540316e-05\n",
      "457 5.212124960962683e-05\n",
      "458 5.1551483920775354e-05\n",
      "459 5.074737055110745e-05\n",
      "460 4.994303890271112e-05\n",
      "461 4.9424299504607916e-05\n",
      "462 4.8652356781531125e-05\n",
      "463 4.7993569751270115e-05\n",
      "464 4.747886487166397e-05\n",
      "465 4.6686742280144244e-05\n",
      "466 4.5843269617762417e-05\n",
      "467 4.520937363849953e-05\n",
      "468 4.4733598770108074e-05\n",
      "469 4.410494875628501e-05\n",
      "470 4.3409843783592805e-05\n",
      "471 4.281942528905347e-05\n",
      "472 4.2042498535010964e-05\n",
      "473 4.152533074375242e-05\n",
      "474 4.09023959946353e-05\n",
      "475 4.039503983221948e-05\n",
      "476 3.9981387089937925e-05\n",
      "477 3.957698936574161e-05\n",
      "478 3.907185237039812e-05\n",
      "479 3.8617017708020285e-05\n",
      "480 3.819973790086806e-05\n",
      "481 3.788274261751212e-05\n",
      "482 3.729472518898547e-05\n",
      "483 3.690538869705051e-05\n",
      "484 3.634446329670027e-05\n",
      "485 3.589283369365148e-05\n",
      "486 3.563077189028263e-05\n",
      "487 3.5321692848810926e-05\n",
      "488 3.4877011785283685e-05\n",
      "489 3.450305666774511e-05\n",
      "490 3.4073746064677835e-05\n",
      "491 3.3713473385432735e-05\n",
      "492 3.330515755806118e-05\n",
      "493 3.306289727333933e-05\n",
      "494 3.2472427847096696e-05\n",
      "495 3.216283585061319e-05\n",
      "496 3.180102430633269e-05\n",
      "497 3.154483420075849e-05\n",
      "498 3.10772520606406e-05\n",
      "499 3.081128670601174e-05\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10   # N表示训练数据的个数， D_in表示输入的特征数 H是中间层，\n",
    "\n",
    "# 随机创建一下训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "# this is key poing \n",
    "torch.nn.init.normal_(model[0].weight)\n",
    "torch.nn.init.normal_(model[2].weight)\n",
    "#model = model.cuda()\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for it in range(500):\n",
    "    # forward pass\n",
    "    y_pred = model(x)   # model.forward()\n",
    "    \n",
    "    # compute lossa\n",
    "    loss = (y_pred-y).pow(2).sum()  #  computation graph\n",
    "    print(it, loss.item())\n",
    "    \n",
    "    \n",
    "    model.zero_grad()\n",
    "    # Backward pass\n",
    "    # compute the gradient\n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():   # param (tensor, grad)\n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: optim\n",
    "--------------\n",
    "\n",
    "这一次我们不再手动更新模型的weights,而是使用optim这个包来帮助我们更新参数。\n",
    "optim这个package提供了各种不同的模型优化方法，包括SGD+momentum, RMSProp, Adam等等。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T04:24:54.224645Z",
     "start_time": "2020-02-25T04:24:53.335453Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 30046644.0\n",
      "1 26872956.0\n",
      "2 28097106.0\n",
      "3 28902388.0\n",
      "4 26177116.0\n",
      "5 19417960.0\n",
      "6 11955322.0\n",
      "7 6495286.0\n",
      "8 3477305.25\n",
      "9 1995551.25\n",
      "10 1280588.5\n",
      "11 912005.8125\n",
      "12 700790.125\n",
      "13 564917.3125\n",
      "14 468560.75\n",
      "15 395407.46875\n",
      "16 337437.5\n",
      "17 290232.53125\n",
      "18 251112.59375\n",
      "19 218351.125\n",
      "20 190684.0625\n",
      "21 167157.421875\n",
      "22 147045.3125\n",
      "23 129786.7109375\n",
      "24 114939.4140625\n",
      "25 102085.625\n",
      "26 90920.421875\n",
      "27 81176.609375\n",
      "28 72648.046875\n",
      "29 65172.3125\n",
      "30 58591.8828125\n",
      "31 52785.984375\n",
      "32 47647.140625\n",
      "33 43085.46875\n",
      "34 39026.28125\n",
      "35 35407.8359375\n",
      "36 32174.26171875\n",
      "37 29280.078125\n",
      "38 26685.390625\n",
      "39 24352.009765625\n",
      "40 22250.689453125\n",
      "41 20356.501953125\n",
      "42 18646.419921875\n",
      "43 17099.05859375\n",
      "44 15697.3388671875\n",
      "45 14425.6923828125\n",
      "46 13269.6142578125\n",
      "47 12217.634765625\n",
      "48 11258.74609375\n",
      "49 10383.6025390625\n",
      "50 9584.3984375\n",
      "51 8853.5498046875\n",
      "52 8184.451171875\n",
      "53 7571.14306640625\n",
      "54 7008.18603515625\n",
      "55 6491.107421875\n",
      "56 6015.79443359375\n",
      "57 5578.5224609375\n",
      "58 5176.0947265625\n",
      "59 4805.8056640625\n",
      "60 4464.119140625\n",
      "61 4148.63232421875\n",
      "62 3857.23388671875\n",
      "63 3587.845947265625\n",
      "64 3338.6396484375\n",
      "65 3108.10888671875\n",
      "66 2894.51171875\n",
      "67 2696.60302734375\n",
      "68 2513.3681640625\n",
      "69 2343.487548828125\n",
      "70 2185.807373046875\n",
      "71 2039.4046630859375\n",
      "72 1903.3702392578125\n",
      "73 1776.962158203125\n",
      "74 1659.6392822265625\n",
      "75 1550.536865234375\n",
      "76 1449.0169677734375\n",
      "77 1354.5240478515625\n",
      "78 1266.4814453125\n",
      "79 1184.4591064453125\n",
      "80 1108.02587890625\n",
      "81 1036.75146484375\n",
      "82 970.275146484375\n",
      "83 908.2714233398438\n",
      "84 850.40771484375\n",
      "85 796.3751831054688\n",
      "86 745.9326782226562\n",
      "87 698.8320922851562\n",
      "88 654.826416015625\n",
      "89 613.720703125\n",
      "90 575.3438720703125\n",
      "91 539.4964599609375\n",
      "92 505.984130859375\n",
      "93 474.6338195800781\n",
      "94 445.2895812988281\n",
      "95 417.8213806152344\n",
      "96 392.12359619140625\n",
      "97 368.0389099121094\n",
      "98 345.48828125\n",
      "99 324.3685607910156\n",
      "100 304.58001708984375\n",
      "101 286.0283203125\n",
      "102 268.6447448730469\n",
      "103 252.3364715576172\n",
      "104 237.04734802246094\n",
      "105 222.7105255126953\n",
      "106 209.26976013183594\n",
      "107 196.65699768066406\n",
      "108 184.825927734375\n",
      "109 173.72666931152344\n",
      "110 163.31256103515625\n",
      "111 153.53248596191406\n",
      "112 144.3558349609375\n",
      "113 135.73912048339844\n",
      "114 127.6526107788086\n",
      "115 120.05804443359375\n",
      "116 112.9256591796875\n",
      "117 106.22379302978516\n",
      "118 99.93009948730469\n",
      "119 94.02053833007812\n",
      "120 88.46744537353516\n",
      "121 83.24905395507812\n",
      "122 78.34231567382812\n",
      "123 73.73297882080078\n",
      "124 69.39818572998047\n",
      "125 65.32453918457031\n",
      "126 61.49406433105469\n",
      "127 57.89385223388672\n",
      "128 54.50832748413086\n",
      "129 51.323143005371094\n",
      "130 48.327720642089844\n",
      "131 45.5108642578125\n",
      "132 42.86054611206055\n",
      "133 40.36835861206055\n",
      "134 38.023223876953125\n",
      "135 35.81648254394531\n",
      "136 33.73970031738281\n",
      "137 31.78407096862793\n",
      "138 29.94594955444336\n",
      "139 28.214452743530273\n",
      "140 26.58514404296875\n",
      "141 25.051721572875977\n",
      "142 23.607128143310547\n",
      "143 22.24747085571289\n",
      "144 20.96718978881836\n",
      "145 19.76156234741211\n",
      "146 18.62775421142578\n",
      "147 17.559066772460938\n",
      "148 16.551822662353516\n",
      "149 15.603672981262207\n",
      "150 14.710707664489746\n",
      "151 13.86928653717041\n",
      "152 13.076556205749512\n",
      "153 12.329819679260254\n",
      "154 11.626262664794922\n",
      "155 10.963852882385254\n",
      "156 10.339582443237305\n",
      "157 9.750628471374512\n",
      "158 9.196072578430176\n",
      "159 8.673489570617676\n",
      "160 8.181325912475586\n",
      "161 7.717062473297119\n",
      "162 7.279660224914551\n",
      "163 6.866829872131348\n",
      "164 6.478203773498535\n",
      "165 6.111374855041504\n",
      "166 5.765719890594482\n",
      "167 5.440125465393066\n",
      "168 5.1324639320373535\n",
      "169 4.843170642852783\n",
      "170 4.5700154304504395\n",
      "171 4.312324523925781\n",
      "172 4.069518566131592\n",
      "173 3.8403122425079346\n",
      "174 3.62435245513916\n",
      "175 3.420656442642212\n",
      "176 3.228335380554199\n",
      "177 3.047079563140869\n",
      "178 2.876190662384033\n",
      "179 2.7147536277770996\n",
      "180 2.562523126602173\n",
      "181 2.4192121028900146\n",
      "182 2.283587694168091\n",
      "183 2.155782699584961\n",
      "184 2.035012722015381\n",
      "185 1.9214574098587036\n",
      "186 1.8141642808914185\n",
      "187 1.712921380996704\n",
      "188 1.6171685457229614\n",
      "189 1.5269275903701782\n",
      "190 1.4419831037521362\n",
      "191 1.3616915941238403\n",
      "192 1.285840630531311\n",
      "193 1.2142173051834106\n",
      "194 1.146651029586792\n",
      "195 1.0829638242721558\n",
      "196 1.0227742195129395\n",
      "197 0.9659437537193298\n",
      "198 0.9121629595756531\n",
      "199 0.8616489768028259\n",
      "200 0.813841700553894\n",
      "201 0.7686901092529297\n",
      "202 0.7260891795158386\n",
      "203 0.685843288898468\n",
      "204 0.6478821039199829\n",
      "205 0.6120722889900208\n",
      "206 0.5782085657119751\n",
      "207 0.5461812615394592\n",
      "208 0.5160679221153259\n",
      "209 0.48747962713241577\n",
      "210 0.460645854473114\n",
      "211 0.43524420261383057\n",
      "212 0.4112204909324646\n",
      "213 0.38847336173057556\n",
      "214 0.36714449524879456\n",
      "215 0.3469124734401703\n",
      "216 0.32778093218803406\n",
      "217 0.30967259407043457\n",
      "218 0.2926202118396759\n",
      "219 0.27653881907463074\n",
      "220 0.2613261640071869\n",
      "221 0.2469688504934311\n",
      "222 0.2333977222442627\n",
      "223 0.22060665488243103\n",
      "224 0.20847435295581818\n",
      "225 0.1970345824956894\n",
      "226 0.18621042370796204\n",
      "227 0.17600858211517334\n",
      "228 0.16635164618492126\n",
      "229 0.1572546809911728\n",
      "230 0.14867763221263885\n",
      "231 0.14044898748397827\n",
      "232 0.13281626999378204\n",
      "233 0.12558132410049438\n",
      "234 0.11867661774158478\n",
      "235 0.11217010766267776\n",
      "236 0.10603257268667221\n",
      "237 0.10023488104343414\n",
      "238 0.09475142508745193\n",
      "239 0.08956003189086914\n",
      "240 0.08469903469085693\n",
      "241 0.0801074281334877\n",
      "242 0.07571697235107422\n",
      "243 0.07159033417701721\n",
      "244 0.06769201904535294\n",
      "245 0.0639762431383133\n",
      "246 0.06049572676420212\n",
      "247 0.057211827486753464\n",
      "248 0.054109103977680206\n",
      "249 0.051166363060474396\n",
      "250 0.04837394505739212\n",
      "251 0.04575968533754349\n",
      "252 0.04326311871409416\n",
      "253 0.040922511368989944\n",
      "254 0.038710303604602814\n",
      "255 0.036614011973142624\n",
      "256 0.034623052924871445\n",
      "257 0.032740071415901184\n",
      "258 0.030965054407715797\n",
      "259 0.029299166053533554\n",
      "260 0.027721112594008446\n",
      "261 0.026230448856949806\n",
      "262 0.024814270436763763\n",
      "263 0.02348291501402855\n",
      "264 0.022198792546987534\n",
      "265 0.021009346470236778\n",
      "266 0.01987769454717636\n",
      "267 0.018810424953699112\n",
      "268 0.01780090294778347\n",
      "269 0.016847720369696617\n",
      "270 0.015933774411678314\n",
      "271 0.015082203783094883\n",
      "272 0.014278324320912361\n",
      "273 0.013519961386919022\n",
      "274 0.01278864685446024\n",
      "275 0.012110641226172447\n",
      "276 0.011464135721325874\n",
      "277 0.010860059410333633\n",
      "278 0.010281853377819061\n",
      "279 0.0097285695374012\n",
      "280 0.009228297509253025\n",
      "281 0.00873864721506834\n",
      "282 0.008281167596578598\n",
      "283 0.007845384068787098\n",
      "284 0.007435008883476257\n",
      "285 0.007047074381262064\n",
      "286 0.006682774052023888\n",
      "287 0.006329418160021305\n",
      "288 0.006012887693941593\n",
      "289 0.005704719107598066\n",
      "290 0.005406106822192669\n",
      "291 0.005128771997988224\n",
      "292 0.004863460082560778\n",
      "293 0.004617428872734308\n",
      "294 0.004379523452371359\n",
      "295 0.004165485966950655\n",
      "296 0.003957224544137716\n",
      "297 0.003760552266612649\n",
      "298 0.0035648446064442396\n",
      "299 0.0033859882969409227\n",
      "300 0.003221722785383463\n",
      "301 0.003058638423681259\n",
      "302 0.002910610754042864\n",
      "303 0.002769856248050928\n",
      "304 0.002638041041791439\n",
      "305 0.0025095948949456215\n",
      "306 0.002390301786363125\n",
      "307 0.002274882746860385\n",
      "308 0.0021658060140907764\n",
      "309 0.0020653665997087955\n",
      "310 0.001968580763787031\n",
      "311 0.0018755148630589247\n",
      "312 0.0017881265375763178\n",
      "313 0.0017055236967280507\n",
      "314 0.0016308401245623827\n",
      "315 0.001556514180265367\n",
      "316 0.0014858251670375466\n",
      "317 0.0014228842919692397\n",
      "318 0.001358124311082065\n",
      "319 0.0012988913804292679\n",
      "320 0.0012426832690835\n",
      "321 0.0011904190760105848\n",
      "322 0.0011393834138289094\n",
      "323 0.0010902727954089642\n",
      "324 0.0010437449673190713\n",
      "325 0.0010019480250775814\n",
      "326 0.0009607278043404222\n",
      "327 0.0009226898546330631\n",
      "328 0.0008845410193316638\n",
      "329 0.0008495816146023571\n",
      "330 0.0008160806610248983\n",
      "331 0.0007841162150725722\n",
      "332 0.0007535822223871946\n",
      "333 0.0007235021330416203\n",
      "334 0.0006968509987927973\n",
      "335 0.0006705066189169884\n",
      "336 0.0006450054934248328\n",
      "337 0.0006210535648278892\n",
      "338 0.0005977280088700354\n",
      "339 0.0005755434976890683\n",
      "340 0.0005544806481339037\n",
      "341 0.000533954007551074\n",
      "342 0.0005141294095665216\n",
      "343 0.0004963785759173334\n",
      "344 0.00047731661470606923\n",
      "345 0.0004601655527949333\n",
      "346 0.0004449633997865021\n",
      "347 0.0004302290908526629\n",
      "348 0.0004150844179093838\n",
      "349 0.00040097744204103947\n",
      "350 0.00038738560397177935\n",
      "351 0.00037443716428242624\n",
      "352 0.0003624561068136245\n",
      "353 0.00035059958463534713\n",
      "354 0.0003393279912415892\n",
      "355 0.00032887287670746446\n",
      "356 0.0003181295469403267\n",
      "357 0.0003087242948822677\n",
      "358 0.0002999647113028914\n",
      "359 0.0002902485430240631\n",
      "360 0.0002810351725202054\n",
      "361 0.0002734254230745137\n",
      "362 0.00026522050029598176\n",
      "363 0.0002577617415226996\n",
      "364 0.0002499608672223985\n",
      "365 0.00024326991115231067\n",
      "366 0.00023589603370055556\n",
      "367 0.00022910900588613003\n",
      "368 0.0002224887430202216\n",
      "369 0.00021610665135085583\n",
      "370 0.0002100756682921201\n",
      "371 0.00020507915178313851\n",
      "372 0.00019923267245758325\n",
      "373 0.0001932612940436229\n",
      "374 0.0001884697558125481\n",
      "375 0.0001831577392295003\n",
      "376 0.00017847810522653162\n",
      "377 0.00017384155944455415\n",
      "378 0.00016954497550614178\n",
      "379 0.00016465078806504607\n",
      "380 0.0001604368444532156\n",
      "381 0.00015670066932216287\n",
      "382 0.00015329569578170776\n",
      "383 0.00014967529568821192\n",
      "384 0.0001459435443393886\n",
      "385 0.0001420278858859092\n",
      "386 0.00013878976460546255\n",
      "387 0.0001356237626168877\n",
      "388 0.0001327011123066768\n",
      "389 0.0001300465955864638\n",
      "390 0.00012729006994049996\n",
      "391 0.00012417275866027921\n",
      "392 0.00012140016769990325\n",
      "393 0.00011852972966153175\n",
      "394 0.00011548637849045917\n",
      "395 0.00011311916750855744\n",
      "396 0.00011034806811949238\n",
      "397 0.00010858904715860263\n",
      "398 0.00010611283505568281\n",
      "399 0.00010446812666486949\n",
      "400 0.00010193357593379915\n",
      "401 9.972175030270591e-05\n",
      "402 9.770371252670884e-05\n",
      "403 9.557613520883024e-05\n",
      "404 9.338880045106634e-05\n",
      "405 9.196667815558612e-05\n",
      "406 8.989194611785933e-05\n",
      "407 8.816428453428671e-05\n",
      "408 8.632265235064551e-05\n",
      "409 8.458557567792013e-05\n",
      "410 8.291113044833764e-05\n",
      "411 8.137179247569293e-05\n",
      "412 7.964530232129619e-05\n",
      "413 7.814456330379471e-05\n",
      "414 7.682187424506992e-05\n",
      "415 7.547009590780362e-05\n",
      "416 7.391022518277168e-05\n",
      "417 7.272826042026281e-05\n",
      "418 7.163532427512109e-05\n",
      "419 7.026427920209244e-05\n",
      "420 6.90007145749405e-05\n",
      "421 6.820022099418566e-05\n",
      "422 6.686181586701423e-05\n",
      "423 6.554935680469498e-05\n",
      "424 6.421842408599332e-05\n",
      "425 6.29938585916534e-05\n",
      "426 6.193162698764354e-05\n",
      "427 6.095427670516074e-05\n",
      "428 5.989030614728108e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429 5.89289229537826e-05\n",
      "430 5.805636101285927e-05\n",
      "431 5.718878674088046e-05\n",
      "432 5.6409957323921844e-05\n",
      "433 5.519092519534752e-05\n",
      "434 5.436176434159279e-05\n",
      "435 5.337605398381129e-05\n",
      "436 5.264154606265947e-05\n",
      "437 5.1787013944704086e-05\n",
      "438 5.111621067044325e-05\n",
      "439 5.0486116379033774e-05\n",
      "440 4.982515747542493e-05\n",
      "441 4.89888661832083e-05\n",
      "442 4.852009078604169e-05\n",
      "443 4.779280425282195e-05\n",
      "444 4.688429908128455e-05\n",
      "445 4.6387413021875545e-05\n",
      "446 4.554789120447822e-05\n",
      "447 4.469692794373259e-05\n",
      "448 4.39111863670405e-05\n",
      "449 4.324377732700668e-05\n",
      "450 4.274714592611417e-05\n",
      "451 4.195357541902922e-05\n",
      "452 4.1634189983597025e-05\n",
      "453 4.100361547898501e-05\n",
      "454 4.048449409310706e-05\n",
      "455 4.005493246950209e-05\n",
      "456 3.954900967073627e-05\n",
      "457 3.889204890583642e-05\n",
      "458 3.8488626159960404e-05\n",
      "459 3.787735477089882e-05\n",
      "460 3.7536727177212015e-05\n",
      "461 3.7010198866482824e-05\n",
      "462 3.6553730751620606e-05\n",
      "463 3.6164652556180954e-05\n",
      "464 3.554927388904616e-05\n",
      "465 3.528537490637973e-05\n",
      "466 3.48001231031958e-05\n",
      "467 3.4266569855390117e-05\n",
      "468 3.380927591933869e-05\n",
      "469 3.362396819284186e-05\n",
      "470 3.3134845580207184e-05\n",
      "471 3.264787665102631e-05\n",
      "472 3.2326199288945645e-05\n",
      "473 3.1876101274974644e-05\n",
      "474 3.1600713555235416e-05\n",
      "475 3.119120447081514e-05\n",
      "476 3.076629582210444e-05\n",
      "477 3.0411260013352148e-05\n",
      "478 3.0110913940006867e-05\n",
      "479 2.966670763271395e-05\n",
      "480 2.946522545244079e-05\n",
      "481 2.9299737434484996e-05\n",
      "482 2.900166691688355e-05\n",
      "483 2.85999703919515e-05\n",
      "484 2.8232307158759795e-05\n",
      "485 2.802581548166927e-05\n",
      "486 2.7657428290694952e-05\n",
      "487 2.72974884865107e-05\n",
      "488 2.698437856452074e-05\n",
      "489 2.670802132342942e-05\n",
      "490 2.637481884448789e-05\n",
      "491 2.6206194888800383e-05\n",
      "492 2.5735256713232957e-05\n",
      "493 2.5560973881511018e-05\n",
      "494 2.5307504984084517e-05\n",
      "495 2.5012068363139406e-05\n",
      "496 2.4752322133281268e-05\n",
      "497 2.458520066284109e-05\n",
      "498 2.423373189230915e-05\n",
      "499 2.4041915821726434e-05\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10   # N表示训练数据的个数， D_in表示输入的特征数 H是中间层，\n",
    "\n",
    "# 随机创建一下训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "torch.nn.init.normal_(model[0].weight)\n",
    "torch.nn.init.normal_(model[2].weight)\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "# learning_rate = 1e-4\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for it in range(500):\n",
    "    # forward pass\n",
    "    y_pred = model(x)   # model.forward()\n",
    "    \n",
    "    # compute lossa\n",
    "    loss = (y_pred-y).pow(2).sum()  #  computation graph\n",
    "    print(it, loss.item())\n",
    "    \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    # Backward pass\n",
    "    # compute the gradient\n",
    "    loss.backward()\n",
    "    \n",
    "    # update model parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: 自定义 nn Modules\n",
    "--------------------------\n",
    "\n",
    "我们可以定义一个模型，这个模型继承自nn.Module类。如果需要定义一个比Sequential模型更加复杂的模型，就需要定义nn.Module模型。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T04:32:59.582771Z",
     "start_time": "2020-02-25T04:32:58.633217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 677.4461059570312\n",
      "1 660.1953125\n",
      "2 643.40283203125\n",
      "3 627.0911254882812\n",
      "4 611.3529052734375\n",
      "5 596.0787353515625\n",
      "6 581.1334838867188\n",
      "7 566.6232299804688\n",
      "8 552.5125122070312\n",
      "9 538.8779296875\n",
      "10 525.659912109375\n",
      "11 512.8281860351562\n",
      "12 500.3941650390625\n",
      "13 488.2879638671875\n",
      "14 476.56317138671875\n",
      "15 465.1693420410156\n",
      "16 454.06756591796875\n",
      "17 443.27435302734375\n",
      "18 432.8061218261719\n",
      "19 422.67767333984375\n",
      "20 412.8088684082031\n",
      "21 403.2757568359375\n",
      "22 394.0106201171875\n",
      "23 384.9906921386719\n",
      "24 376.22393798828125\n",
      "25 367.6571044921875\n",
      "26 359.3021240234375\n",
      "27 351.13427734375\n",
      "28 343.1561279296875\n",
      "29 335.37164306640625\n",
      "30 327.7716369628906\n",
      "31 320.35675048828125\n",
      "32 313.0692138671875\n",
      "33 305.9927978515625\n",
      "34 299.08563232421875\n",
      "35 292.3280029296875\n",
      "36 285.73370361328125\n",
      "37 279.2716064453125\n",
      "38 272.9289855957031\n",
      "39 266.7222595214844\n",
      "40 260.62432861328125\n",
      "41 254.62646484375\n",
      "42 248.77110290527344\n",
      "43 243.0587615966797\n",
      "44 237.45980834960938\n",
      "45 231.94979858398438\n",
      "46 226.5487518310547\n",
      "47 221.24937438964844\n",
      "48 216.05673217773438\n",
      "49 210.97134399414062\n",
      "50 205.98187255859375\n",
      "51 201.08865356445312\n",
      "52 196.29861450195312\n",
      "53 191.5998077392578\n",
      "54 187.00706481933594\n",
      "55 182.50460815429688\n",
      "56 178.09225463867188\n",
      "57 173.7587127685547\n",
      "58 169.5081787109375\n",
      "59 165.34609985351562\n",
      "60 161.27857971191406\n",
      "61 157.29000854492188\n",
      "62 153.38143920898438\n",
      "63 149.55149841308594\n",
      "64 145.80760192871094\n",
      "65 142.141357421875\n",
      "66 138.55389404296875\n",
      "67 135.03224182128906\n",
      "68 131.57530212402344\n",
      "69 128.19361877441406\n",
      "70 124.88248443603516\n",
      "71 121.650146484375\n",
      "72 118.47916412353516\n",
      "73 115.36931610107422\n",
      "74 112.32888793945312\n",
      "75 109.35055541992188\n",
      "76 106.43818664550781\n",
      "77 103.59671020507812\n",
      "78 100.81928253173828\n",
      "79 98.0985107421875\n",
      "80 95.436279296875\n",
      "81 92.82736206054688\n",
      "82 90.2698974609375\n",
      "83 87.77021789550781\n",
      "84 85.32119750976562\n",
      "85 82.92375183105469\n",
      "86 80.583984375\n",
      "87 78.29915618896484\n",
      "88 76.06367492675781\n",
      "89 73.87992858886719\n",
      "90 71.74774169921875\n",
      "91 69.66328430175781\n",
      "92 67.62574768066406\n",
      "93 65.63821411132812\n",
      "94 63.69828796386719\n",
      "95 61.80767059326172\n",
      "96 59.96125030517578\n",
      "97 58.158042907714844\n",
      "98 56.400421142578125\n",
      "99 54.68403625488281\n",
      "100 53.01411819458008\n",
      "101 51.3846321105957\n",
      "102 49.79822540283203\n",
      "103 48.252220153808594\n",
      "104 46.74318313598633\n",
      "105 45.27525329589844\n",
      "106 43.84551239013672\n",
      "107 42.45244598388672\n",
      "108 41.095985412597656\n",
      "109 39.777156829833984\n",
      "110 38.49283218383789\n",
      "111 37.240379333496094\n",
      "112 36.024051666259766\n",
      "113 34.84288787841797\n",
      "114 33.6933479309082\n",
      "115 32.57622146606445\n",
      "116 31.490978240966797\n",
      "117 30.435096740722656\n",
      "118 29.410226821899414\n",
      "119 28.41452407836914\n",
      "120 27.44782066345215\n",
      "121 26.509702682495117\n",
      "122 25.600770950317383\n",
      "123 24.71988868713379\n",
      "124 23.863985061645508\n",
      "125 23.03356170654297\n",
      "126 22.228981018066406\n",
      "127 21.449928283691406\n",
      "128 20.695158004760742\n",
      "129 19.96327018737793\n",
      "130 19.25396156311035\n",
      "131 18.568262100219727\n",
      "132 17.903074264526367\n",
      "133 17.259138107299805\n",
      "134 16.63593864440918\n",
      "135 16.033126831054688\n",
      "136 15.449836730957031\n",
      "137 14.885782241821289\n",
      "138 14.33948040008545\n",
      "139 13.811861991882324\n",
      "140 13.301773071289062\n",
      "141 12.809493064880371\n",
      "142 12.332691192626953\n",
      "143 11.87191390991211\n",
      "144 11.426897048950195\n",
      "145 10.996809005737305\n",
      "146 10.581441879272461\n",
      "147 10.180497169494629\n",
      "148 9.794183731079102\n",
      "149 9.420818328857422\n",
      "150 9.060794830322266\n",
      "151 8.71285343170166\n",
      "152 8.377193450927734\n",
      "153 8.052989959716797\n",
      "154 7.740582466125488\n",
      "155 7.439303398132324\n",
      "156 7.148759841918945\n",
      "157 6.86843204498291\n",
      "158 6.598633766174316\n",
      "159 6.338097095489502\n",
      "160 6.087294101715088\n",
      "161 5.845510482788086\n",
      "162 5.612809181213379\n",
      "163 5.388462066650391\n",
      "164 5.1725873947143555\n",
      "165 4.964388847351074\n",
      "166 4.764243125915527\n",
      "167 4.57166051864624\n",
      "168 4.386314392089844\n",
      "169 4.208132266998291\n",
      "170 4.036670684814453\n",
      "171 3.871772289276123\n",
      "172 3.7134296894073486\n",
      "173 3.5609793663024902\n",
      "174 3.414776563644409\n",
      "175 3.274092197418213\n",
      "176 3.1391801834106445\n",
      "177 3.0093653202056885\n",
      "178 2.884786605834961\n",
      "179 2.7651631832122803\n",
      "180 2.6501293182373047\n",
      "181 2.539942502975464\n",
      "182 2.434389591217041\n",
      "183 2.33288311958313\n",
      "184 2.235612392425537\n",
      "185 2.1421992778778076\n",
      "186 2.0526351928710938\n",
      "187 1.9666059017181396\n",
      "188 1.884158968925476\n",
      "189 1.804977536201477\n",
      "190 1.7291367053985596\n",
      "191 1.6562700271606445\n",
      "192 1.5864758491516113\n",
      "193 1.519529938697815\n",
      "194 1.4554760456085205\n",
      "195 1.3939342498779297\n",
      "196 1.3349521160125732\n",
      "197 1.2784576416015625\n",
      "198 1.2243379354476929\n",
      "199 1.172397494316101\n",
      "200 1.1226906776428223\n",
      "201 1.075054407119751\n",
      "202 1.0293934345245361\n",
      "203 0.9857016801834106\n",
      "204 0.9438115358352661\n",
      "205 0.9036819338798523\n",
      "206 0.8651933670043945\n",
      "207 0.8283231854438782\n",
      "208 0.7928970456123352\n",
      "209 0.7589446306228638\n",
      "210 0.7264392375946045\n",
      "211 0.695286214351654\n",
      "212 0.6653911471366882\n",
      "213 0.6367735266685486\n",
      "214 0.6093887686729431\n",
      "215 0.5831723213195801\n",
      "216 0.5580803155899048\n",
      "217 0.5340376496315002\n",
      "218 0.5110644102096558\n",
      "219 0.4891044497489929\n",
      "220 0.4681091010570526\n",
      "221 0.4480343461036682\n",
      "222 0.4288426637649536\n",
      "223 0.4104922413825989\n",
      "224 0.39293554425239563\n",
      "225 0.3761309087276459\n",
      "226 0.3600613474845886\n",
      "227 0.3446914851665497\n",
      "228 0.32999086380004883\n",
      "229 0.3159233629703522\n",
      "230 0.3024680018424988\n",
      "231 0.2896043360233307\n",
      "232 0.2772981524467468\n",
      "233 0.2655291259288788\n",
      "234 0.25426965951919556\n",
      "235 0.24350139498710632\n",
      "236 0.23320351541042328\n",
      "237 0.22338607907295227\n",
      "238 0.2139902412891388\n",
      "239 0.2050083577632904\n",
      "240 0.19642020761966705\n",
      "241 0.18820028007030487\n",
      "242 0.18034237623214722\n",
      "243 0.17282746732234955\n",
      "244 0.16563156247138977\n",
      "245 0.15874803066253662\n",
      "246 0.15216302871704102\n",
      "247 0.14586572349071503\n",
      "248 0.13983625173568726\n",
      "249 0.1340729296207428\n",
      "250 0.12854745984077454\n",
      "251 0.12326550483703613\n",
      "252 0.11821606010198593\n",
      "253 0.11336963623762131\n",
      "254 0.10873723030090332\n",
      "255 0.1043025404214859\n",
      "256 0.10005715489387512\n",
      "257 0.09599658101797104\n",
      "258 0.09210234135389328\n",
      "259 0.0883760005235672\n",
      "260 0.08484506607055664\n",
      "261 0.08149395883083344\n",
      "262 0.07829020172357559\n",
      "263 0.07522204518318176\n",
      "264 0.0722862109541893\n",
      "265 0.06947754323482513\n",
      "266 0.0667857900261879\n",
      "267 0.06420865654945374\n",
      "268 0.061741262674331665\n",
      "269 0.05937844514846802\n",
      "270 0.057120148092508316\n",
      "271 0.05496089905500412\n",
      "272 0.052892349660396576\n",
      "273 0.05090906098484993\n",
      "274 0.0490078404545784\n",
      "275 0.04718524217605591\n",
      "276 0.04543742537498474\n",
      "277 0.04376104101538658\n",
      "278 0.042153168469667435\n",
      "279 0.04060981050133705\n",
      "280 0.039130568504333496\n",
      "281 0.03770961984992027\n",
      "282 0.03634567931294441\n",
      "283 0.035034261643886566\n",
      "284 0.033775683492422104\n",
      "285 0.03256705403327942\n",
      "286 0.03140565752983093\n",
      "287 0.030289677903056145\n",
      "288 0.029216932132840157\n",
      "289 0.02818579040467739\n",
      "290 0.02719387784600258\n",
      "291 0.026239944621920586\n",
      "292 0.02532261423766613\n",
      "293 0.024439692497253418\n",
      "294 0.023590149357914925\n",
      "295 0.022772276774048805\n",
      "296 0.02198583446443081\n",
      "297 0.021227698773145676\n",
      "298 0.020498091354966164\n",
      "299 0.019795289263129234\n",
      "300 0.019118471071124077\n",
      "301 0.018466545268893242\n",
      "302 0.017837759107351303\n",
      "303 0.01723233051598072\n",
      "304 0.01664840802550316\n",
      "305 0.016085876151919365\n",
      "306 0.015543381683528423\n",
      "307 0.015020030550658703\n",
      "308 0.014515279792249203\n",
      "309 0.014028588309884071\n",
      "310 0.013559005223214626\n",
      "311 0.013105979189276695\n",
      "312 0.012668645940721035\n",
      "313 0.012246677652001381\n",
      "314 0.011839435435831547\n",
      "315 0.01144607923924923\n",
      "316 0.011066445149481297\n",
      "317 0.010699771344661713\n",
      "318 0.010345729999244213\n",
      "319 0.010003809817135334\n",
      "320 0.00967345293611288\n",
      "321 0.00935436226427555\n",
      "322 0.009046085178852081\n",
      "323 0.008748292922973633\n",
      "324 0.008460520766675472\n",
      "325 0.008182412013411522\n",
      "326 0.007913647219538689\n",
      "327 0.007653900887817144\n",
      "328 0.00740278884768486\n",
      "329 0.00716005964204669\n",
      "330 0.006925757508724928\n",
      "331 0.006698481272906065\n",
      "332 0.006479126401245594\n",
      "333 0.006267017684876919\n",
      "334 0.006061960943043232\n",
      "335 0.005863526836037636\n",
      "336 0.0056716990657150745\n",
      "337 0.005486157722771168\n",
      "338 0.005306660197675228\n",
      "339 0.005133055150508881\n",
      "340 0.004965138155966997\n",
      "341 0.00480267871171236\n",
      "342 0.0046455347910523415\n",
      "343 0.004493500106036663\n",
      "344 0.004346397239714861\n",
      "345 0.004204090218991041\n",
      "346 0.00406637554988265\n",
      "347 0.0039331684820353985\n",
      "348 0.0038042564410716295\n",
      "349 0.0036795290652662516\n",
      "350 0.0035588496830314398\n",
      "351 0.0034420627634972334\n",
      "352 0.003329057712107897\n",
      "353 0.0032196796964854\n",
      "354 0.0031138714402914047\n",
      "355 0.0030114820692688227\n",
      "356 0.0029123839922249317\n",
      "357 0.0028164777904748917\n",
      "358 0.002723684534430504\n",
      "359 0.0026338710449635983\n",
      "360 0.002546977484598756\n",
      "361 0.0024628841783851385\n",
      "362 0.0023814875166863203\n",
      "363 0.002302731852978468\n",
      "364 0.0022265256848186255\n",
      "365 0.0021527684293687344\n",
      "366 0.002081406069919467\n",
      "367 0.002012349897995591\n",
      "368 0.0019455240108072758\n",
      "369 0.001880852272734046\n",
      "370 0.001818268676288426\n",
      "371 0.0017577221151441336\n",
      "372 0.0016991293523460627\n",
      "373 0.0016424295026808977\n",
      "374 0.0015875808894634247\n",
      "375 0.001534495735540986\n",
      "376 0.00148312549572438\n",
      "377 0.0014334330335259438\n",
      "378 0.0013853608397766948\n",
      "379 0.0013388452352955937\n",
      "380 0.0012938351137563586\n",
      "381 0.001250297063961625\n",
      "382 0.0012081784661859274\n",
      "383 0.00116743054240942\n",
      "384 0.0011280117323622108\n",
      "385 0.0010898812906816602\n",
      "386 0.0010530055733397603\n",
      "387 0.0010173195041716099\n",
      "388 0.0009828089969232678\n",
      "389 0.000949433131609112\n",
      "390 0.0009171484271064401\n",
      "391 0.0008859175140969455\n",
      "392 0.0008557217661291361\n",
      "393 0.000826511939521879\n",
      "394 0.0007982654497027397\n",
      "395 0.0007709558703936636\n",
      "396 0.0007445430965162814\n",
      "397 0.0007189988973550498\n",
      "398 0.000694301794283092\n",
      "399 0.0006704312982037663\n",
      "400 0.0006473385728895664\n",
      "401 0.000625012384261936\n",
      "402 0.0006034329999238253\n",
      "403 0.0005825753905810416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404 0.0005624009063467383\n",
      "405 0.0005429052980616689\n",
      "406 0.0005240599857643247\n",
      "407 0.0005058430833742023\n",
      "408 0.00048823212273418903\n",
      "409 0.0004712125810328871\n",
      "410 0.00045476065133698285\n",
      "411 0.00043887001811526716\n",
      "412 0.000423512130510062\n",
      "413 0.00040866376366466284\n",
      "414 0.00039432247285731137\n",
      "415 0.00038046445115469396\n",
      "416 0.0003670684527605772\n",
      "417 0.00035413316800259054\n",
      "418 0.00034162792144343257\n",
      "419 0.0003295606002211571\n",
      "420 0.00031788949854671955\n",
      "421 0.00030662002973258495\n",
      "422 0.00029573836945928633\n",
      "423 0.00028522347565740347\n",
      "424 0.0002750659768935293\n",
      "425 0.0002652580151334405\n",
      "426 0.0002557891421020031\n",
      "427 0.0002466423320583999\n",
      "428 0.00023780978517606854\n",
      "429 0.00022928013640921563\n",
      "430 0.0002210510428994894\n",
      "431 0.0002130962529918179\n",
      "432 0.0002054244832834229\n",
      "433 0.00019800462177954614\n",
      "434 0.00019085522217210382\n",
      "435 0.00018394859216641635\n",
      "436 0.00017728273815009743\n",
      "437 0.00017084863793570548\n",
      "438 0.00016464114014524966\n",
      "439 0.0001586447178851813\n",
      "440 0.00015286104462575167\n",
      "441 0.00014727868256159127\n",
      "442 0.00014189643843565136\n",
      "443 0.0001366985379718244\n",
      "444 0.00013168678560759872\n",
      "445 0.00012684859393630177\n",
      "446 0.00012218633492011577\n",
      "447 0.00011768432159442455\n",
      "448 0.00011334500595694408\n",
      "449 0.000109154760139063\n",
      "450 0.00010511749133002013\n",
      "451 0.00010122036474058405\n",
      "452 9.746721480041742e-05\n",
      "453 9.384236909681931e-05\n",
      "454 9.034771210281178e-05\n",
      "455 8.697843441041186e-05\n",
      "456 8.373262244276702e-05\n",
      "457 8.060133404796943e-05\n",
      "458 7.758320134598762e-05\n",
      "459 7.467209798051044e-05\n",
      "460 7.186634320532903e-05\n",
      "461 6.91615350660868e-05\n",
      "462 6.655537436017767e-05\n",
      "463 6.404432497220114e-05\n",
      "464 6.162438512546942e-05\n",
      "465 5.9289493947289884e-05\n",
      "466 5.704166687792167e-05\n",
      "467 5.487493035616353e-05\n",
      "468 5.2786985179409385e-05\n",
      "469 5.077560126665048e-05\n",
      "470 4.883570727542974e-05\n",
      "471 4.697252734331414e-05\n",
      "472 4.5172062527853996e-05\n",
      "473 4.344067565398291e-05\n",
      "474 4.17733499489259e-05\n",
      "475 4.016653838334605e-05\n",
      "476 3.861838922603056e-05\n",
      "477 3.712786201504059e-05\n",
      "478 3.569381806300953e-05\n",
      "479 3.4313241485506296e-05\n",
      "480 3.298105366411619e-05\n",
      "481 3.170116906403564e-05\n",
      "482 3.0466744647128507e-05\n",
      "483 2.9279903174028732e-05\n",
      "484 2.8138205379946157e-05\n",
      "485 2.7037660402129404e-05\n",
      "486 2.5980511054513045e-05\n",
      "487 2.4960067094070837e-05\n",
      "488 2.3981358026503585e-05\n",
      "489 2.3037380742607638e-05\n",
      "490 2.2127971533336677e-05\n",
      "491 2.1255360479699448e-05\n",
      "492 2.041547304543201e-05\n",
      "493 1.960653935384471e-05\n",
      "494 1.8829250620910898e-05\n",
      "495 1.8080909285345115e-05\n",
      "496 1.736181729938835e-05\n",
      "497 1.666930074861739e-05\n",
      "498 1.6003865312086418e-05\n",
      "499 1.5363708371296525e-05\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10   # N表示训练数据的个数， D_in表示输入的特征数 H是中间层，\n",
    "\n",
    "# 随机创建一下训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        \n",
    "        # define the model architecture\n",
    "        self.linear1 = torch.nn.Linear(D_in, H, bias=False)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear2(self.linear1(x).clamp(min=0))\n",
    "        return y_pred\n",
    "\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for it in range(500):\n",
    "    # forward pass\n",
    "    y_pred = model(x)   # model.forward()\n",
    "    \n",
    "    # compute lossa\n",
    "    loss = (y_pred-y).pow(2).sum()  #  computation graph\n",
    "    print(it, loss.item())\n",
    "    \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    # Backward pass\n",
    "    # compute the gradient\n",
    "    loss.backward()\n",
    "    \n",
    "    # update model parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FizzBuzz\n",
    "\n",
    "FizzBuzz是一个简单的小游戏。游戏规则如下：从1开始往上数数，当遇到3的倍数的时候，说fizz，当遇到5的倍数，说buzz，当遇到15的倍数，就说fizzbuzz，其他情况下则正常数数。\n",
    "\n",
    "我们可以写一个简单的小程序来决定要返回正常数值还是fizz, buzz 或者 fizzbuzz。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T04:15:01.892305Z",
     "start_time": "2020-02-26T04:15:01.881335Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "buzz\n",
      "fizz\n",
      "fizzbuzz\n"
     ]
    }
   ],
   "source": [
    "# one-hot encode the desired outputs: [number, \"fizz\", \"buzz\", \"fizzbuzz\"]\n",
    "def fizz_buzz_encode(i):\n",
    "    if i % 15 == 0: return 3\n",
    "    elif i % 5 == 0: return 2\n",
    "    elif i % 3 == 0: return 1\n",
    "    else:            return 0\n",
    "\n",
    "def fizz_buzz_decode(i, prediction):\n",
    "    return [str(i), \"fizz\", \"buzz\", \"fizzbuzz\"][prediction]\n",
    "\n",
    "print(fizz_buzz_decode(1, fizz_buzz_encode(1)))\n",
    "print(fizz_buzz_decode(2, fizz_buzz_encode(2)))\n",
    "print(fizz_buzz_decode(5, fizz_buzz_encode(5)))\n",
    "print(fizz_buzz_decode(12, fizz_buzz_encode(12)))\n",
    "print(fizz_buzz_decode(15, fizz_buzz_encode(15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T04:15:03.566845Z",
     "start_time": "2020-02-26T04:15:03.559864Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', 'fizz', 'buzz', 'fizzbuzz']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[str(1), \"fizz\", \"buzz\", \"fizzbuzz\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们首先定义模型的输入与输出(训练数据)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T04:15:05.159405Z",
     "start_time": "2020-02-26T04:15:04.728558Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32667, 15])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "NUM_DIGITS = 15\n",
    "\n",
    "def binary_encode(i, num_digits):\n",
    "    return np.array([i >> d & 1 for d in range(num_digits)])\n",
    "\n",
    "trX = torch.Tensor([binary_encode(i, NUM_DIGITS) for i in range(101, 2 ** NUM_DIGITS)])\n",
    "trY = torch.LongTensor([fizz_buzz_encode(i) for i in range(101, 2 ** NUM_DIGITS)])\n",
    "\n",
    "trX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们用PyTorch定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T04:16:55.908803Z",
     "start_time": "2020-02-26T04:16:55.903774Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "NUM_HIDDEN1 = 100\n",
    "NUM_HIDDEN2 = 60\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(NUM_DIGITS, NUM_HIDDEN1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(NUM_HIDDEN1, NUM_HIDDEN2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(NUM_HIDDEN2, 4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 为了让我们的模型学会FizzBuzz这个游戏，我们需要定义一个损失函数，和一个优化算法。\n",
    "- 这个优化算法会不断优化（降低）损失函数，使得模型的在该任务上取得尽可能低的损失值。\n",
    "- 损失值低往往表示我们的模型表现好，损失值高表示我们的模型表现差。\n",
    "- 由于FizzBuzz游戏本质上是一个分类问题，我们选用Cross Entropyy Loss函数。\n",
    "- 优化函数我们选用Stochastic Gradient Descent。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T04:17:10.300228Z",
     "start_time": "2020-02-26T04:17:10.295234Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是模型的训练代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T04:23:30.625425Z",
     "start_time": "2020-02-26T04:17:20.869030Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Loss: 1.138267993927002\n",
      "Epoch:  1 Loss: 1.1378307342529297\n",
      "Epoch:  2 Loss: 1.1375750303268433\n",
      "Epoch:  3 Loss: 1.1374017000198364\n",
      "Epoch:  4 Loss: 1.137266993522644\n",
      "Epoch:  5 Loss: 1.13716721534729\n",
      "Epoch:  6 Loss: 1.1370795965194702\n",
      "Epoch:  7 Loss: 1.1370128393173218\n",
      "Epoch:  8 Loss: 1.136949896812439\n",
      "Epoch:  9 Loss: 1.1368873119354248\n",
      "Epoch:  10 Loss: 1.1368368864059448\n",
      "Epoch:  11 Loss: 1.1367844343185425\n",
      "Epoch:  12 Loss: 1.136731505393982\n",
      "Epoch:  13 Loss: 1.136678695678711\n",
      "Epoch:  14 Loss: 1.1366205215454102\n",
      "Epoch:  15 Loss: 1.136557936668396\n",
      "Epoch:  16 Loss: 1.1364970207214355\n",
      "Epoch:  17 Loss: 1.136421799659729\n",
      "Epoch:  18 Loss: 1.136338233947754\n",
      "Epoch:  19 Loss: 1.1362360715866089\n",
      "Epoch:  20 Loss: 1.1361300945281982\n",
      "Epoch:  21 Loss: 1.1359829902648926\n",
      "Epoch:  22 Loss: 1.135834813117981\n",
      "Epoch:  23 Loss: 1.135672688484192\n",
      "Epoch:  24 Loss: 1.1354960203170776\n",
      "Epoch:  25 Loss: 1.1353040933609009\n",
      "Epoch:  26 Loss: 1.1351165771484375\n",
      "Epoch:  27 Loss: 1.1348793506622314\n",
      "Epoch:  28 Loss: 1.134468674659729\n",
      "Epoch:  29 Loss: 1.1339375972747803\n",
      "Epoch:  30 Loss: 1.1333446502685547\n",
      "Epoch:  31 Loss: 1.1326913833618164\n",
      "Epoch:  32 Loss: 1.1319806575775146\n",
      "Epoch:  33 Loss: 1.1312543153762817\n",
      "Epoch:  34 Loss: 1.1303848028182983\n",
      "Epoch:  35 Loss: 1.1294366121292114\n",
      "Epoch:  36 Loss: 1.1284137964248657\n",
      "Epoch:  37 Loss: 1.1273107528686523\n",
      "Epoch:  38 Loss: 1.1261470317840576\n",
      "Epoch:  39 Loss: 1.124830722808838\n",
      "Epoch:  40 Loss: 1.1235758066177368\n",
      "Epoch:  41 Loss: 1.1221014261245728\n",
      "Epoch:  42 Loss: 1.1203560829162598\n",
      "Epoch:  43 Loss: 1.1183322668075562\n",
      "Epoch:  44 Loss: 1.1159005165100098\n",
      "Epoch:  45 Loss: 1.1131256818771362\n",
      "Epoch:  46 Loss: 1.1096020936965942\n",
      "Epoch:  47 Loss: 1.1058576107025146\n",
      "Epoch:  48 Loss: 1.1003645658493042\n",
      "Epoch:  49 Loss: 1.0946921110153198\n",
      "Epoch:  50 Loss: 1.088028907775879\n",
      "Epoch:  51 Loss: 1.0796403884887695\n",
      "Epoch:  52 Loss: 1.0702435970306396\n",
      "Epoch:  53 Loss: 1.0567806959152222\n",
      "Epoch:  54 Loss: 1.0458751916885376\n",
      "Epoch:  55 Loss: 1.0347799062728882\n",
      "Epoch:  56 Loss: 1.0328978300094604\n",
      "Epoch:  57 Loss: 1.021458387374878\n",
      "Epoch:  58 Loss: 1.0017142295837402\n",
      "Epoch:  59 Loss: 0.955139696598053\n",
      "Epoch:  60 Loss: 0.9860209226608276\n",
      "Epoch:  61 Loss: 0.8945536017417908\n",
      "Epoch:  62 Loss: 0.8781858682632446\n",
      "Epoch:  63 Loss: 0.8757557272911072\n",
      "Epoch:  64 Loss: 0.819403886795044\n",
      "Epoch:  65 Loss: 0.8091420531272888\n",
      "Epoch:  66 Loss: 0.7882546782493591\n",
      "Epoch:  67 Loss: 0.7805701494216919\n",
      "Epoch:  68 Loss: 0.757465660572052\n",
      "Epoch:  69 Loss: 0.7613643407821655\n",
      "Epoch:  70 Loss: 0.7220513820648193\n",
      "Epoch:  71 Loss: 0.8071288466453552\n",
      "Epoch:  72 Loss: 0.766469419002533\n",
      "Epoch:  73 Loss: 0.954687774181366\n",
      "Epoch:  74 Loss: 1.049963355064392\n",
      "Epoch:  75 Loss: 0.9723141193389893\n",
      "Epoch:  76 Loss: 0.9971205592155457\n",
      "Epoch:  77 Loss: 0.7070375084877014\n",
      "Epoch:  78 Loss: 0.6764103770256042\n",
      "Epoch:  79 Loss: 1.2364815473556519\n",
      "Epoch:  80 Loss: 1.0064325332641602\n",
      "Epoch:  81 Loss: 0.7182565927505493\n",
      "Epoch:  82 Loss: 1.7155653238296509\n",
      "Epoch:  83 Loss: 1.1741594076156616\n",
      "Epoch:  84 Loss: 0.9449066519737244\n",
      "Epoch:  85 Loss: 0.5876306295394897\n",
      "Epoch:  86 Loss: 0.5776106715202332\n",
      "Epoch:  87 Loss: 1.0061423778533936\n",
      "Epoch:  88 Loss: 0.600734293460846\n",
      "Epoch:  89 Loss: 0.8083540797233582\n",
      "Epoch:  90 Loss: 0.7411348223686218\n",
      "Epoch:  91 Loss: 0.6482946276664734\n",
      "Epoch:  92 Loss: 0.5938130021095276\n",
      "Epoch:  93 Loss: 3.4226229190826416\n",
      "Epoch:  94 Loss: 0.6914408206939697\n",
      "Epoch:  95 Loss: 0.7802174091339111\n",
      "Epoch:  96 Loss: 0.5674181580543518\n",
      "Epoch:  97 Loss: 0.5563710927963257\n",
      "Epoch:  98 Loss: 2.1602022647857666\n",
      "Epoch:  99 Loss: 0.5742716789245605\n",
      "Epoch:  100 Loss: 0.7258968949317932\n",
      "Epoch:  101 Loss: 0.995216429233551\n",
      "Epoch:  102 Loss: 0.674640417098999\n",
      "Epoch:  103 Loss: 1.047355055809021\n",
      "Epoch:  104 Loss: 0.6522697806358337\n",
      "Epoch:  105 Loss: 0.5090527534484863\n",
      "Epoch:  106 Loss: 3.811781883239746\n",
      "Epoch:  107 Loss: 1.1252295970916748\n",
      "Epoch:  108 Loss: 1.0596814155578613\n",
      "Epoch:  109 Loss: 0.5716990828514099\n",
      "Epoch:  110 Loss: 0.4202207028865814\n",
      "Epoch:  111 Loss: 0.6042816638946533\n",
      "Epoch:  112 Loss: 0.5536560416221619\n",
      "Epoch:  113 Loss: 2.6409199237823486\n",
      "Epoch:  114 Loss: 0.7552719712257385\n",
      "Epoch:  115 Loss: 0.7994218468666077\n",
      "Epoch:  116 Loss: 0.4820426404476166\n",
      "Epoch:  117 Loss: 1.0570929050445557\n",
      "Epoch:  118 Loss: 0.5897659063339233\n",
      "Epoch:  119 Loss: 0.6641358733177185\n",
      "Epoch:  120 Loss: 0.638684868812561\n",
      "Epoch:  121 Loss: 2.8577702045440674\n",
      "Epoch:  122 Loss: 0.48230960965156555\n",
      "Epoch:  123 Loss: 2.7780537605285645\n",
      "Epoch:  124 Loss: 1.057592749595642\n",
      "Epoch:  125 Loss: 0.5105803608894348\n",
      "Epoch:  126 Loss: 0.4328862130641937\n",
      "Epoch:  127 Loss: 0.4707309305667877\n",
      "Epoch:  128 Loss: 1.2597119808197021\n",
      "Epoch:  129 Loss: 0.4698384404182434\n",
      "Epoch:  130 Loss: 0.43180760741233826\n",
      "Epoch:  131 Loss: 2.0343034267425537\n",
      "Epoch:  132 Loss: 0.4774821400642395\n",
      "Epoch:  133 Loss: 0.9505318403244019\n",
      "Epoch:  134 Loss: 0.9967420101165771\n",
      "Epoch:  135 Loss: 0.4889860451221466\n",
      "Epoch:  136 Loss: 2.0482773780822754\n",
      "Epoch:  137 Loss: 0.834396243095398\n",
      "Epoch:  138 Loss: 0.5742977261543274\n",
      "Epoch:  139 Loss: 0.5334257483482361\n",
      "Epoch:  140 Loss: 0.4969511926174164\n",
      "Epoch:  141 Loss: 0.879576563835144\n",
      "Epoch:  142 Loss: 0.5302950143814087\n",
      "Epoch:  143 Loss: 0.4920174777507782\n",
      "Epoch:  144 Loss: 2.796041965484619\n",
      "Epoch:  145 Loss: 0.8649559617042542\n",
      "Epoch:  146 Loss: 0.7342355251312256\n",
      "Epoch:  147 Loss: 2.0879063606262207\n",
      "Epoch:  148 Loss: 1.8822294473648071\n",
      "Epoch:  149 Loss: 2.096529483795166\n",
      "Epoch:  150 Loss: 0.5741382837295532\n",
      "Epoch:  151 Loss: 1.4019849300384521\n",
      "Epoch:  152 Loss: 1.7513102293014526\n",
      "Epoch:  153 Loss: 0.5097798705101013\n",
      "Epoch:  154 Loss: 1.2933034896850586\n",
      "Epoch:  155 Loss: 0.562789261341095\n",
      "Epoch:  156 Loss: 1.1877164840698242\n",
      "Epoch:  157 Loss: 0.46082010865211487\n",
      "Epoch:  158 Loss: 0.424196720123291\n",
      "Epoch:  159 Loss: 0.7094292044639587\n",
      "Epoch:  160 Loss: 0.4468773901462555\n",
      "Epoch:  161 Loss: 2.6331191062927246\n",
      "Epoch:  162 Loss: 0.9195080995559692\n",
      "Epoch:  163 Loss: 1.994458794593811\n",
      "Epoch:  164 Loss: 2.0594797134399414\n",
      "Epoch:  165 Loss: 2.9286415576934814\n",
      "Epoch:  166 Loss: 0.7262868881225586\n",
      "Epoch:  167 Loss: 2.101949453353882\n",
      "Epoch:  168 Loss: 1.277668833732605\n",
      "Epoch:  169 Loss: 0.9337753653526306\n",
      "Epoch:  170 Loss: 0.5641258955001831\n",
      "Epoch:  171 Loss: 0.5902211666107178\n",
      "Epoch:  172 Loss: 0.14884518086910248\n",
      "Epoch:  173 Loss: 1.2249658107757568\n",
      "Epoch:  174 Loss: 2.8769636154174805\n",
      "Epoch:  175 Loss: 0.8536844253540039\n",
      "Epoch:  176 Loss: 0.7948039174079895\n",
      "Epoch:  177 Loss: 0.22286885976791382\n",
      "Epoch:  178 Loss: 2.9949910640716553\n",
      "Epoch:  179 Loss: 0.9182772040367126\n",
      "Epoch:  180 Loss: 0.2145659476518631\n",
      "Epoch:  181 Loss: 0.672451913356781\n",
      "Epoch:  182 Loss: 4.456449031829834\n",
      "Epoch:  183 Loss: 2.545854091644287\n",
      "Epoch:  184 Loss: 1.3396213054656982\n",
      "Epoch:  185 Loss: 2.4013864994049072\n",
      "Epoch:  186 Loss: 0.48450878262519836\n",
      "Epoch:  187 Loss: 1.1459671258926392\n",
      "Epoch:  188 Loss: 0.902125895023346\n",
      "Epoch:  189 Loss: 1.3707324266433716\n",
      "Epoch:  190 Loss: 1.4362539052963257\n",
      "Epoch:  191 Loss: 1.1642181873321533\n",
      "Epoch:  192 Loss: 0.6548720598220825\n",
      "Epoch:  193 Loss: 0.5851709842681885\n",
      "Epoch:  194 Loss: 1.133972406387329\n",
      "Epoch:  195 Loss: 0.48261287808418274\n",
      "Epoch:  196 Loss: 0.8519306182861328\n",
      "Epoch:  197 Loss: 1.227347493171692\n",
      "Epoch:  198 Loss: 0.23853887617588043\n",
      "Epoch:  199 Loss: 0.4430582523345947\n",
      "Epoch:  200 Loss: 0.4871103763580322\n",
      "Epoch:  201 Loss: 1.3764927387237549\n",
      "Epoch:  202 Loss: 2.1186933517456055\n",
      "Epoch:  203 Loss: 0.6646025776863098\n",
      "Epoch:  204 Loss: 0.47149941325187683\n",
      "Epoch:  205 Loss: 0.8213838338851929\n",
      "Epoch:  206 Loss: 0.39749962091445923\n",
      "Epoch:  207 Loss: 0.6184542775154114\n",
      "Epoch:  208 Loss: 1.0784183740615845\n",
      "Epoch:  209 Loss: 0.4277211129665375\n",
      "Epoch:  210 Loss: 0.3948444128036499\n",
      "Epoch:  211 Loss: 0.3680996894836426\n",
      "Epoch:  212 Loss: 0.3128184676170349\n",
      "Epoch:  213 Loss: 0.32302916049957275\n",
      "Epoch:  214 Loss: 0.3025199770927429\n",
      "Epoch:  215 Loss: 0.3900338411331177\n",
      "Epoch:  216 Loss: 0.2741682231426239\n",
      "Epoch:  217 Loss: 0.3014954924583435\n",
      "Epoch:  218 Loss: 0.24189448356628418\n",
      "Epoch:  219 Loss: 0.2693369686603546\n",
      "Epoch:  220 Loss: 0.2361496239900589\n",
      "Epoch:  221 Loss: 0.23821043968200684\n",
      "Epoch:  222 Loss: 0.21583117544651031\n",
      "Epoch:  223 Loss: 0.1930581033229828\n",
      "Epoch:  224 Loss: 0.1864168792963028\n",
      "Epoch:  225 Loss: 0.14629560708999634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  226 Loss: 0.14506632089614868\n",
      "Epoch:  227 Loss: 0.14081089198589325\n",
      "Epoch:  228 Loss: 0.5455402135848999\n",
      "Epoch:  229 Loss: 0.10637131333351135\n",
      "Epoch:  230 Loss: 0.28111568093299866\n",
      "Epoch:  231 Loss: 0.245904803276062\n",
      "Epoch:  232 Loss: 0.22363275289535522\n",
      "Epoch:  233 Loss: 0.2082826793193817\n",
      "Epoch:  234 Loss: 0.20129109919071198\n",
      "Epoch:  235 Loss: 0.15661315619945526\n",
      "Epoch:  236 Loss: 0.13792641460895538\n",
      "Epoch:  237 Loss: 0.09344135224819183\n",
      "Epoch:  238 Loss: 0.07729433476924896\n",
      "Epoch:  239 Loss: 0.05960829555988312\n",
      "Epoch:  240 Loss: 0.09379560500383377\n",
      "Epoch:  241 Loss: 0.09196894615888596\n",
      "Epoch:  242 Loss: 0.059797193855047226\n",
      "Epoch:  243 Loss: 0.09074312448501587\n",
      "Epoch:  244 Loss: 0.08205784857273102\n",
      "Epoch:  245 Loss: 0.09499765932559967\n",
      "Epoch:  246 Loss: 0.05609648674726486\n",
      "Epoch:  247 Loss: 0.09355001896619797\n",
      "Epoch:  248 Loss: 0.08232072740793228\n",
      "Epoch:  249 Loss: 0.06323523074388504\n",
      "Epoch:  250 Loss: 0.05026042088866234\n",
      "Epoch:  251 Loss: 0.14104458689689636\n",
      "Epoch:  252 Loss: 0.08353596925735474\n",
      "Epoch:  253 Loss: 0.42161470651626587\n",
      "Epoch:  254 Loss: 0.46344277262687683\n",
      "Epoch:  255 Loss: 0.06153620034456253\n",
      "Epoch:  256 Loss: 0.14433448016643524\n",
      "Epoch:  257 Loss: 0.051949642598629\n",
      "Epoch:  258 Loss: 0.15663595497608185\n",
      "Epoch:  259 Loss: 0.0984906256198883\n",
      "Epoch:  260 Loss: 0.09262000024318695\n",
      "Epoch:  261 Loss: 0.07782833278179169\n",
      "Epoch:  262 Loss: 0.07143798470497131\n",
      "Epoch:  263 Loss: 0.0547381155192852\n",
      "Epoch:  264 Loss: 0.0487927682697773\n",
      "Epoch:  265 Loss: 0.052605971693992615\n",
      "Epoch:  266 Loss: 0.03740566223859787\n",
      "Epoch:  267 Loss: 0.03802897781133652\n",
      "Epoch:  268 Loss: 0.035103145986795425\n",
      "Epoch:  269 Loss: 0.03476904705166817\n",
      "Epoch:  270 Loss: 0.03449564054608345\n",
      "Epoch:  271 Loss: 0.03308795019984245\n",
      "Epoch:  272 Loss: 0.17452441155910492\n",
      "Epoch:  273 Loss: 0.033189382404088974\n",
      "Epoch:  274 Loss: 0.03624039143323898\n",
      "Epoch:  275 Loss: 0.03100494295358658\n",
      "Epoch:  276 Loss: 0.03296254947781563\n",
      "Epoch:  277 Loss: 0.029269615188241005\n",
      "Epoch:  278 Loss: 0.02708398550748825\n",
      "Epoch:  279 Loss: 0.028900036588311195\n",
      "Epoch:  280 Loss: 0.030268115922808647\n",
      "Epoch:  281 Loss: 0.026302922517061234\n",
      "Epoch:  282 Loss: 0.025050241500139236\n",
      "Epoch:  283 Loss: 0.026400867849588394\n",
      "Epoch:  284 Loss: 0.023942096158862114\n",
      "Epoch:  285 Loss: 0.022797279059886932\n",
      "Epoch:  286 Loss: 0.02325311489403248\n",
      "Epoch:  287 Loss: 0.022574516013264656\n",
      "Epoch:  288 Loss: 0.0220376867800951\n",
      "Epoch:  289 Loss: 0.023618673905730247\n",
      "Epoch:  290 Loss: 0.021143611520528793\n",
      "Epoch:  291 Loss: 0.02142464555799961\n",
      "Epoch:  292 Loss: 0.02866547554731369\n",
      "Epoch:  293 Loss: 0.0279703252017498\n",
      "Epoch:  294 Loss: 0.020157646387815475\n",
      "Epoch:  295 Loss: 0.026685992255806923\n",
      "Epoch:  296 Loss: 0.027128195390105247\n",
      "Epoch:  297 Loss: 0.021465202793478966\n",
      "Epoch:  298 Loss: 0.021798690780997276\n",
      "Epoch:  299 Loss: 0.020149409770965576\n",
      "Epoch:  300 Loss: 0.02526569366455078\n",
      "Epoch:  301 Loss: 0.02658265084028244\n",
      "Epoch:  302 Loss: 0.02202211320400238\n",
      "Epoch:  303 Loss: 0.022743863984942436\n",
      "Epoch:  304 Loss: 0.01704809069633484\n",
      "Epoch:  305 Loss: 0.02206408604979515\n",
      "Epoch:  306 Loss: 0.021197304129600525\n",
      "Epoch:  307 Loss: 0.0195102971047163\n",
      "Epoch:  308 Loss: 0.018781283870339394\n",
      "Epoch:  309 Loss: 0.018055690452456474\n",
      "Epoch:  310 Loss: 0.017503200098872185\n",
      "Epoch:  311 Loss: 0.01777763105928898\n",
      "Epoch:  312 Loss: 0.01669979840517044\n",
      "Epoch:  313 Loss: 0.017307695001363754\n",
      "Epoch:  314 Loss: 0.01596451736986637\n",
      "Epoch:  315 Loss: 0.015757862478494644\n",
      "Epoch:  316 Loss: 0.015177303925156593\n",
      "Epoch:  317 Loss: 0.015156230889260769\n",
      "Epoch:  318 Loss: 0.014738879166543484\n",
      "Epoch:  319 Loss: 0.014505305327475071\n",
      "Epoch:  320 Loss: 0.014146290719509125\n",
      "Epoch:  321 Loss: 0.0137526486068964\n",
      "Epoch:  322 Loss: 0.013395408168435097\n",
      "Epoch:  323 Loss: 0.013030316680669785\n",
      "Epoch:  324 Loss: 0.012687237933278084\n",
      "Epoch:  325 Loss: 0.012545009143650532\n",
      "Epoch:  326 Loss: 0.012378045357763767\n",
      "Epoch:  327 Loss: 0.012409927323460579\n",
      "Epoch:  328 Loss: 0.012023327872157097\n",
      "Epoch:  329 Loss: 0.011818681843578815\n",
      "Epoch:  330 Loss: 0.01173620019108057\n",
      "Epoch:  331 Loss: 0.012082019820809364\n",
      "Epoch:  332 Loss: 0.011686328798532486\n",
      "Epoch:  333 Loss: 0.011551792733371258\n",
      "Epoch:  334 Loss: 0.011317385360598564\n",
      "Epoch:  335 Loss: 0.011308570392429829\n",
      "Epoch:  336 Loss: 0.011333523318171501\n",
      "Epoch:  337 Loss: 0.011225312948226929\n",
      "Epoch:  338 Loss: 0.011025922372937202\n",
      "Epoch:  339 Loss: 0.010880258865654469\n",
      "Epoch:  340 Loss: 0.01074188482016325\n",
      "Epoch:  341 Loss: 0.010720454156398773\n",
      "Epoch:  342 Loss: 0.010575282387435436\n",
      "Epoch:  343 Loss: 0.010547062382102013\n",
      "Epoch:  344 Loss: 0.010420748963952065\n",
      "Epoch:  345 Loss: 0.010567841120064259\n",
      "Epoch:  346 Loss: 0.010337505489587784\n",
      "Epoch:  347 Loss: 0.01018175296485424\n",
      "Epoch:  348 Loss: 0.010137894190847874\n",
      "Epoch:  349 Loss: 0.010066994465887547\n",
      "Epoch:  350 Loss: 0.009961893782019615\n",
      "Epoch:  351 Loss: 0.009902453050017357\n",
      "Epoch:  352 Loss: 0.00981810037046671\n",
      "Epoch:  353 Loss: 0.009808133356273174\n",
      "Epoch:  354 Loss: 0.009597060270607471\n",
      "Epoch:  355 Loss: 0.009554551914334297\n",
      "Epoch:  356 Loss: 0.00945775955915451\n",
      "Epoch:  357 Loss: 0.009396938607096672\n",
      "Epoch:  358 Loss: 0.009366168640553951\n",
      "Epoch:  359 Loss: 0.009225779213011265\n",
      "Epoch:  360 Loss: 0.009302912279963493\n",
      "Epoch:  361 Loss: 0.009121912531554699\n",
      "Epoch:  362 Loss: 0.009004456922411919\n",
      "Epoch:  363 Loss: 0.008998148143291473\n",
      "Epoch:  364 Loss: 0.008846288546919823\n",
      "Epoch:  365 Loss: 0.00890700425952673\n",
      "Epoch:  366 Loss: 0.008705975487828255\n",
      "Epoch:  367 Loss: 0.008698356337845325\n",
      "Epoch:  368 Loss: 0.008639147505164146\n",
      "Epoch:  369 Loss: 0.008465096354484558\n",
      "Epoch:  370 Loss: 0.008517706766724586\n",
      "Epoch:  371 Loss: 0.008356977254152298\n",
      "Epoch:  372 Loss: 0.008289703167974949\n",
      "Epoch:  373 Loss: 0.00817069225013256\n",
      "Epoch:  374 Loss: 0.008075390011072159\n",
      "Epoch:  375 Loss: 0.008037874475121498\n",
      "Epoch:  376 Loss: 0.007911581546068192\n",
      "Epoch:  377 Loss: 0.007880561985075474\n",
      "Epoch:  378 Loss: 0.0076539525762200356\n",
      "Epoch:  379 Loss: 0.0077199507504701614\n",
      "Epoch:  380 Loss: 0.00794973410665989\n",
      "Epoch:  381 Loss: 0.007852592505514622\n",
      "Epoch:  382 Loss: 0.007657099049538374\n",
      "Epoch:  383 Loss: 0.007599893491715193\n",
      "Epoch:  384 Loss: 0.007542388513684273\n",
      "Epoch:  385 Loss: 0.007385441567748785\n",
      "Epoch:  386 Loss: 0.007325660903006792\n",
      "Epoch:  387 Loss: 0.00729359732940793\n",
      "Epoch:  388 Loss: 0.007317437790334225\n",
      "Epoch:  389 Loss: 0.007240505889058113\n",
      "Epoch:  390 Loss: 0.007132553029805422\n",
      "Epoch:  391 Loss: 0.0069801704958081245\n",
      "Epoch:  392 Loss: 0.006966973189264536\n",
      "Epoch:  393 Loss: 0.00690936716273427\n",
      "Epoch:  394 Loss: 0.006886197719722986\n",
      "Epoch:  395 Loss: 0.00671351607888937\n",
      "Epoch:  396 Loss: 0.006712642498314381\n",
      "Epoch:  397 Loss: 0.0065521239303052425\n",
      "Epoch:  398 Loss: 0.006634135730564594\n",
      "Epoch:  399 Loss: 0.006519470829516649\n",
      "Epoch:  400 Loss: 0.006682110484689474\n",
      "Epoch:  401 Loss: 0.006401489954441786\n",
      "Epoch:  402 Loss: 0.006486973259598017\n",
      "Epoch:  403 Loss: 0.0063795046880841255\n",
      "Epoch:  404 Loss: 0.006371529307216406\n",
      "Epoch:  405 Loss: 0.006181512027978897\n",
      "Epoch:  406 Loss: 0.006275859661400318\n",
      "Epoch:  407 Loss: 0.006158086471259594\n",
      "Epoch:  408 Loss: 0.006413322873413563\n",
      "Epoch:  409 Loss: 0.006152146961539984\n",
      "Epoch:  410 Loss: 0.006016951519995928\n",
      "Epoch:  411 Loss: 0.006184058263897896\n",
      "Epoch:  412 Loss: 0.006145081017166376\n",
      "Epoch:  413 Loss: 0.005973492283374071\n",
      "Epoch:  414 Loss: 0.00608617952093482\n",
      "Epoch:  415 Loss: 0.006037477403879166\n",
      "Epoch:  416 Loss: 0.005832446739077568\n",
      "Epoch:  417 Loss: 0.005805924069136381\n",
      "Epoch:  418 Loss: 0.005943968892097473\n",
      "Epoch:  419 Loss: 0.005821813363581896\n",
      "Epoch:  420 Loss: 0.006098528392612934\n",
      "Epoch:  421 Loss: 0.005771202966570854\n",
      "Epoch:  422 Loss: 0.005621691234409809\n",
      "Epoch:  423 Loss: 0.005714263767004013\n",
      "Epoch:  424 Loss: 0.005595813039690256\n",
      "Epoch:  425 Loss: 0.005802887957543135\n",
      "Epoch:  426 Loss: 0.005687394179403782\n",
      "Epoch:  427 Loss: 0.005635058972984552\n",
      "Epoch:  428 Loss: 0.005737594794481993\n",
      "Epoch:  429 Loss: 0.005584460683166981\n",
      "Epoch:  430 Loss: 0.0054590655490756035\n",
      "Epoch:  431 Loss: 0.00560614513233304\n",
      "Epoch:  432 Loss: 0.005500045605003834\n",
      "Epoch:  433 Loss: 0.005673097912222147\n",
      "Epoch:  434 Loss: 0.0053625768050551414\n",
      "Epoch:  435 Loss: 0.0055276332423090935\n",
      "Epoch:  436 Loss: 0.005429739598184824\n",
      "Epoch:  437 Loss: 0.0054610176011919975\n",
      "Epoch:  438 Loss: 0.005255596712231636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  439 Loss: 0.0056419288739562035\n",
      "Epoch:  440 Loss: 0.005249716341495514\n",
      "Epoch:  441 Loss: 0.005272258538752794\n",
      "Epoch:  442 Loss: 0.005351168569177389\n",
      "Epoch:  443 Loss: 0.005225026980042458\n",
      "Epoch:  444 Loss: 0.005404407158493996\n",
      "Epoch:  445 Loss: 0.005112088285386562\n",
      "Epoch:  446 Loss: 0.005360560491681099\n",
      "Epoch:  447 Loss: 0.005011062603443861\n",
      "Epoch:  448 Loss: 0.0051924255676567554\n",
      "Epoch:  449 Loss: 0.004995359107851982\n",
      "Epoch:  450 Loss: 0.005179122556000948\n",
      "Epoch:  451 Loss: 0.004964151885360479\n",
      "Epoch:  452 Loss: 0.0049764420837163925\n",
      "Epoch:  453 Loss: 0.004921347834169865\n",
      "Epoch:  454 Loss: 0.005083159543573856\n",
      "Epoch:  455 Loss: 0.004995716270059347\n",
      "Epoch:  456 Loss: 0.004817563574761152\n",
      "Epoch:  457 Loss: 0.0049244193360209465\n",
      "Epoch:  458 Loss: 0.004777616821229458\n",
      "Epoch:  459 Loss: 0.004909788258373737\n",
      "Epoch:  460 Loss: 0.004752723034471273\n",
      "Epoch:  461 Loss: 0.00477380957454443\n",
      "Epoch:  462 Loss: 0.004658168647438288\n",
      "Epoch:  463 Loss: 0.0049100336618721485\n",
      "Epoch:  464 Loss: 0.004757572431117296\n",
      "Epoch:  465 Loss: 0.004594599828124046\n",
      "Epoch:  466 Loss: 0.004729055799543858\n",
      "Epoch:  467 Loss: 0.0045638307929039\n",
      "Epoch:  468 Loss: 0.004661760292947292\n",
      "Epoch:  469 Loss: 0.004744702950119972\n",
      "Epoch:  470 Loss: 0.004531119950115681\n",
      "Epoch:  471 Loss: 0.004588198848068714\n",
      "Epoch:  472 Loss: 0.004435697570443153\n",
      "Epoch:  473 Loss: 0.0045867739245295525\n",
      "Epoch:  474 Loss: 0.004366269335150719\n",
      "Epoch:  475 Loss: 0.0044971914030611515\n",
      "Epoch:  476 Loss: 0.00454324297606945\n",
      "Epoch:  477 Loss: 0.004351484589278698\n",
      "Epoch:  478 Loss: 0.00452326750382781\n",
      "Epoch:  479 Loss: 0.004316363483667374\n",
      "Epoch:  480 Loss: 0.004431265406310558\n",
      "Epoch:  481 Loss: 0.004305536393076181\n",
      "Epoch:  482 Loss: 0.004348417744040489\n",
      "Epoch:  483 Loss: 0.004369801376014948\n",
      "Epoch:  484 Loss: 0.0042138658463954926\n",
      "Epoch:  485 Loss: 0.0044124191626906395\n",
      "Epoch:  486 Loss: 0.004295624792575836\n",
      "Epoch:  487 Loss: 0.004155346192419529\n",
      "Epoch:  488 Loss: 0.004218380432575941\n",
      "Epoch:  489 Loss: 0.004282601643353701\n",
      "Epoch:  490 Loss: 0.004038799554109573\n",
      "Epoch:  491 Loss: 0.004184252582490444\n",
      "Epoch:  492 Loss: 0.004060830920934677\n",
      "Epoch:  493 Loss: 0.0042180404998362064\n",
      "Epoch:  494 Loss: 0.004160481505095959\n",
      "Epoch:  495 Loss: 0.003971366677433252\n",
      "Epoch:  496 Loss: 0.004117672331631184\n",
      "Epoch:  497 Loss: 0.004031606484204531\n",
      "Epoch:  498 Loss: 0.003890640800818801\n",
      "Epoch:  499 Loss: 0.004072953015565872\n",
      "Epoch:  500 Loss: 0.004098626784980297\n",
      "Epoch:  501 Loss: 0.0038406073581427336\n",
      "Epoch:  502 Loss: 0.003963732160627842\n",
      "Epoch:  503 Loss: 0.003943708725273609\n",
      "Epoch:  504 Loss: 0.003878316842019558\n",
      "Epoch:  505 Loss: 0.0039542182348668575\n",
      "Epoch:  506 Loss: 0.003762225853279233\n",
      "Epoch:  507 Loss: 0.0038260817527770996\n",
      "Epoch:  508 Loss: 0.0039001537952572107\n",
      "Epoch:  509 Loss: 0.003773302771151066\n",
      "Epoch:  510 Loss: 0.0039046425372362137\n",
      "Epoch:  511 Loss: 0.0037837596610188484\n",
      "Epoch:  512 Loss: 0.00371443759649992\n",
      "Epoch:  513 Loss: 0.003625588258728385\n",
      "Epoch:  514 Loss: 0.0037211517337709665\n",
      "Epoch:  515 Loss: 0.003736070590093732\n",
      "Epoch:  516 Loss: 0.0036483220756053925\n",
      "Epoch:  517 Loss: 0.003688751719892025\n",
      "Epoch:  518 Loss: 0.0036426708102226257\n",
      "Epoch:  519 Loss: 0.003509205998852849\n",
      "Epoch:  520 Loss: 0.0037022291216999292\n",
      "Epoch:  521 Loss: 0.003614499233663082\n",
      "Epoch:  522 Loss: 0.003475469769909978\n",
      "Epoch:  523 Loss: 0.003658183151856065\n",
      "Epoch:  524 Loss: 0.003620853880420327\n",
      "Epoch:  525 Loss: 0.0034190595615655184\n",
      "Epoch:  526 Loss: 0.0036450724583119154\n",
      "Epoch:  527 Loss: 0.0034923304338008165\n",
      "Epoch:  528 Loss: 0.003387457923963666\n",
      "Epoch:  529 Loss: 0.003437307197600603\n",
      "Epoch:  530 Loss: 0.0034943928476423025\n",
      "Epoch:  531 Loss: 0.0034660089295357466\n",
      "Epoch:  532 Loss: 0.003310868050903082\n",
      "Epoch:  533 Loss: 0.0034948126412928104\n",
      "Epoch:  534 Loss: 0.0034674222115427256\n",
      "Epoch:  535 Loss: 0.003279002383351326\n",
      "Epoch:  536 Loss: 0.0035015607718378305\n",
      "Epoch:  537 Loss: 0.0034365439787507057\n",
      "Epoch:  538 Loss: 0.0032702749595046043\n",
      "Epoch:  539 Loss: 0.003444118658080697\n",
      "Epoch:  540 Loss: 0.0033549757208675146\n",
      "Epoch:  541 Loss: 0.003207865636795759\n",
      "Epoch:  542 Loss: 0.0034477775916457176\n",
      "Epoch:  543 Loss: 0.003192203352227807\n",
      "Epoch:  544 Loss: 0.0033237335737794638\n",
      "Epoch:  545 Loss: 0.0032997638918459415\n",
      "Epoch:  546 Loss: 0.003410295583307743\n",
      "Epoch:  547 Loss: 0.003174604382365942\n",
      "Epoch:  548 Loss: 0.003312621498480439\n",
      "Epoch:  549 Loss: 0.0033609920646995306\n",
      "Epoch:  550 Loss: 0.0031165003310889006\n",
      "Epoch:  551 Loss: 0.0031127112451940775\n",
      "Epoch:  552 Loss: 0.003255444345995784\n",
      "Epoch:  553 Loss: 0.0032036523334681988\n",
      "Epoch:  554 Loss: 0.003214360913261771\n",
      "Epoch:  555 Loss: 0.0031263178680092096\n",
      "Epoch:  556 Loss: 0.003173978067934513\n",
      "Epoch:  557 Loss: 0.0031012578401714563\n",
      "Epoch:  558 Loss: 0.003105440177023411\n",
      "Epoch:  559 Loss: 0.0031196512281894684\n",
      "Epoch:  560 Loss: 0.0031372075900435448\n",
      "Epoch:  561 Loss: 0.0030352724716067314\n",
      "Epoch:  562 Loss: 0.003024694276973605\n",
      "Epoch:  563 Loss: 0.003080121474340558\n",
      "Epoch:  564 Loss: 0.002974357921630144\n",
      "Epoch:  565 Loss: 0.0029512145556509495\n",
      "Epoch:  566 Loss: 0.0030944885220378637\n",
      "Epoch:  567 Loss: 0.002885425230488181\n",
      "Epoch:  568 Loss: 0.002999002579599619\n",
      "Epoch:  569 Loss: 0.002985420636832714\n",
      "Epoch:  570 Loss: 0.0030174278654158115\n",
      "Epoch:  571 Loss: 0.0029564520809799433\n",
      "Epoch:  572 Loss: 0.0028185038827359676\n",
      "Epoch:  573 Loss: 0.002943899482488632\n",
      "Epoch:  574 Loss: 0.00291683548130095\n",
      "Epoch:  575 Loss: 0.002758984686806798\n",
      "Epoch:  576 Loss: 0.0028799811843782663\n",
      "Epoch:  577 Loss: 0.0028624373953789473\n",
      "Epoch:  578 Loss: 0.0027514880057424307\n",
      "Epoch:  579 Loss: 0.0028479178436100483\n",
      "Epoch:  580 Loss: 0.0028236464131623507\n",
      "Epoch:  581 Loss: 0.0028320741839706898\n",
      "Epoch:  582 Loss: 0.0027865187730640173\n",
      "Epoch:  583 Loss: 0.002695859642699361\n",
      "Epoch:  584 Loss: 0.002715928014367819\n",
      "Epoch:  585 Loss: 0.0028051172848790884\n",
      "Epoch:  586 Loss: 0.0026958214584738016\n",
      "Epoch:  587 Loss: 0.002749636536464095\n",
      "Epoch:  588 Loss: 0.002554611535742879\n",
      "Epoch:  589 Loss: 0.002723602345213294\n",
      "Epoch:  590 Loss: 0.002612132579088211\n",
      "Epoch:  591 Loss: 0.002704806160181761\n",
      "Epoch:  592 Loss: 0.002500198082998395\n",
      "Epoch:  593 Loss: 0.0026497584767639637\n",
      "Epoch:  594 Loss: 0.0025610770098865032\n",
      "Epoch:  595 Loss: 0.002608679933473468\n",
      "Epoch:  596 Loss: 0.0025292476639151573\n",
      "Epoch:  597 Loss: 0.002488661790266633\n",
      "Epoch:  598 Loss: 0.002482658252120018\n",
      "Epoch:  599 Loss: 0.0025299531407654285\n",
      "Epoch:  600 Loss: 0.0024852289352566004\n",
      "Epoch:  601 Loss: 0.0024794198106974363\n",
      "Epoch:  602 Loss: 0.0023703081533312798\n",
      "Epoch:  603 Loss: 0.002431670669466257\n",
      "Epoch:  604 Loss: 0.0024381373077630997\n",
      "Epoch:  605 Loss: 0.0024058851413428783\n",
      "Epoch:  606 Loss: 0.0024026362225413322\n",
      "Epoch:  607 Loss: 0.002401593141257763\n",
      "Epoch:  608 Loss: 0.002339585917070508\n",
      "Epoch:  609 Loss: 0.0022828285582363605\n",
      "Epoch:  610 Loss: 0.002324605593457818\n",
      "Epoch:  611 Loss: 0.0023449358996003866\n",
      "Epoch:  612 Loss: 0.0022960896603763103\n",
      "Epoch:  613 Loss: 0.002294385340064764\n",
      "Epoch:  614 Loss: 0.002187818754464388\n",
      "Epoch:  615 Loss: 0.002276116516441107\n",
      "Epoch:  616 Loss: 0.0022400279995054007\n",
      "Epoch:  617 Loss: 0.0022494138684123755\n",
      "Epoch:  618 Loss: 0.0022028200328350067\n",
      "Epoch:  619 Loss: 0.00220553413964808\n",
      "Epoch:  620 Loss: 0.0021740570664405823\n",
      "Epoch:  621 Loss: 0.002184458076953888\n",
      "Epoch:  622 Loss: 0.0021420223638415337\n",
      "Epoch:  623 Loss: 0.0020819928031414747\n",
      "Epoch:  624 Loss: 0.002105012536048889\n",
      "Epoch:  625 Loss: 0.00212996662594378\n",
      "Epoch:  626 Loss: 0.0020899672526866198\n",
      "Epoch:  627 Loss: 0.002086922060698271\n",
      "Epoch:  628 Loss: 0.0020544975996017456\n",
      "Epoch:  629 Loss: 0.002061517909169197\n",
      "Epoch:  630 Loss: 0.0020319095347076654\n",
      "Epoch:  631 Loss: 0.0020380693022161722\n",
      "Epoch:  632 Loss: 0.0019987314008176327\n",
      "Epoch:  633 Loss: 0.0019424086203798652\n",
      "Epoch:  634 Loss: 0.0019696655217558146\n",
      "Epoch:  635 Loss: 0.0019719526171684265\n",
      "Epoch:  636 Loss: 0.0019503276562318206\n",
      "Epoch:  637 Loss: 0.0019390750676393509\n",
      "Epoch:  638 Loss: 0.0019052332499995828\n",
      "Epoch:  639 Loss: 0.0019224249990656972\n",
      "Epoch:  640 Loss: 0.0018932351376861334\n",
      "Epoch:  641 Loss: 0.0018760536331683397\n",
      "Epoch:  642 Loss: 0.0018768111476674676\n",
      "Epoch:  643 Loss: 0.0018501272425055504\n",
      "Epoch:  644 Loss: 0.0018461330328136683\n",
      "Epoch:  645 Loss: 0.0018229010747745633\n",
      "Epoch:  646 Loss: 0.0018166410736739635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  647 Loss: 0.0017645381158217788\n",
      "Epoch:  648 Loss: 0.0017914645140990615\n",
      "Epoch:  649 Loss: 0.0017902274848893285\n",
      "Epoch:  650 Loss: 0.0017523820279166102\n",
      "Epoch:  651 Loss: 0.0017184412572532892\n",
      "Epoch:  652 Loss: 0.0017474258784204721\n",
      "Epoch:  653 Loss: 0.00173443544190377\n",
      "Epoch:  654 Loss: 0.0017155759269371629\n",
      "Epoch:  655 Loss: 0.0017276947619393468\n",
      "Epoch:  656 Loss: 0.001683209789916873\n",
      "Epoch:  657 Loss: 0.0016832036199048162\n",
      "Epoch:  658 Loss: 0.0016827438957989216\n",
      "Epoch:  659 Loss: 0.0016603353433310986\n",
      "Epoch:  660 Loss: 0.0016437646700069308\n",
      "Epoch:  661 Loss: 0.0016427726950496435\n",
      "Epoch:  662 Loss: 0.0016352564562112093\n",
      "Epoch:  663 Loss: 0.0016116272890940309\n",
      "Epoch:  664 Loss: 0.0016137947095558047\n",
      "Epoch:  665 Loss: 0.0015883657615631819\n",
      "Epoch:  666 Loss: 0.001580648124217987\n",
      "Epoch:  667 Loss: 0.0015681576915085316\n",
      "Epoch:  668 Loss: 0.0015622233040630817\n",
      "Epoch:  669 Loss: 0.0015531626995652914\n",
      "Epoch:  670 Loss: 0.0015053011011332273\n",
      "Epoch:  671 Loss: 0.0015365466242656112\n",
      "Epoch:  672 Loss: 0.0015205666422843933\n",
      "Epoch:  673 Loss: 0.0014991684583947062\n",
      "Epoch:  674 Loss: 0.0015152475098147988\n",
      "Epoch:  675 Loss: 0.0014834508765488863\n",
      "Epoch:  676 Loss: 0.0014779296470806003\n",
      "Epoch:  677 Loss: 0.0014839257346466184\n",
      "Epoch:  678 Loss: 0.0014517639065161347\n",
      "Epoch:  679 Loss: 0.0014703472843393683\n",
      "Epoch:  680 Loss: 0.0014407968847081065\n",
      "Epoch:  681 Loss: 0.0014363984810188413\n",
      "Epoch:  682 Loss: 0.0014286770019680262\n",
      "Epoch:  683 Loss: 0.0014142876025289297\n",
      "Epoch:  684 Loss: 0.0014137570979073644\n",
      "Epoch:  685 Loss: 0.0014078960521146655\n",
      "Epoch:  686 Loss: 0.001361483708024025\n",
      "Epoch:  687 Loss: 0.00139812717679888\n",
      "Epoch:  688 Loss: 0.0013700082199648023\n",
      "Epoch:  689 Loss: 0.0013625100255012512\n",
      "Epoch:  690 Loss: 0.0013642312260344625\n",
      "Epoch:  691 Loss: 0.0013475175946950912\n",
      "Epoch:  692 Loss: 0.0013483191141858697\n",
      "Epoch:  693 Loss: 0.0013362898025661707\n",
      "Epoch:  694 Loss: 0.0013214867794886231\n",
      "Epoch:  695 Loss: 0.0013255159137770534\n",
      "Epoch:  696 Loss: 0.0013113017193973064\n",
      "Epoch:  697 Loss: 0.001311239437200129\n",
      "Epoch:  698 Loss: 0.0012926816707476974\n",
      "Epoch:  699 Loss: 0.0012886110926046968\n",
      "Epoch:  700 Loss: 0.0012813241919502616\n",
      "Epoch:  701 Loss: 0.001284158555790782\n",
      "Epoch:  702 Loss: 0.001259337761439383\n",
      "Epoch:  703 Loss: 0.0012704109540209174\n",
      "Epoch:  704 Loss: 0.0012521571479737759\n",
      "Epoch:  705 Loss: 0.0012487644562497735\n",
      "Epoch:  706 Loss: 0.0012431315844878554\n",
      "Epoch:  707 Loss: 0.0012298288056626916\n",
      "Epoch:  708 Loss: 0.0012375094229355454\n",
      "Epoch:  709 Loss: 0.001222225371748209\n",
      "Epoch:  710 Loss: 0.0012160718906670809\n",
      "Epoch:  711 Loss: 0.0012138328747823834\n",
      "Epoch:  712 Loss: 0.0012021490838378668\n",
      "Epoch:  713 Loss: 0.001191002782434225\n",
      "Epoch:  714 Loss: 0.0011909952154383063\n",
      "Epoch:  715 Loss: 0.0011857901699841022\n",
      "Epoch:  716 Loss: 0.0011797696352005005\n",
      "Epoch:  717 Loss: 0.0011729717953130603\n",
      "Epoch:  718 Loss: 0.0011628096690401435\n",
      "Epoch:  719 Loss: 0.0011702870251610875\n",
      "Epoch:  720 Loss: 0.0011578156845644116\n",
      "Epoch:  721 Loss: 0.0011434229090809822\n",
      "Epoch:  722 Loss: 0.0011537055252119899\n",
      "Epoch:  723 Loss: 0.001140817068517208\n",
      "Epoch:  724 Loss: 0.0011374535970389843\n",
      "Epoch:  725 Loss: 0.0011336327297613025\n",
      "Epoch:  726 Loss: 0.0011202848982065916\n",
      "Epoch:  727 Loss: 0.001117659849114716\n",
      "Epoch:  728 Loss: 0.0011175662511959672\n",
      "Epoch:  729 Loss: 0.0011044623097404838\n",
      "Epoch:  730 Loss: 0.0011029901215806603\n",
      "Epoch:  731 Loss: 0.0010985470144078135\n",
      "Epoch:  732 Loss: 0.0010970109142363071\n",
      "Epoch:  733 Loss: 0.0010810585226863623\n",
      "Epoch:  734 Loss: 0.0010800526943057775\n",
      "Epoch:  735 Loss: 0.0010783866746351123\n",
      "Epoch:  736 Loss: 0.001076828921213746\n",
      "Epoch:  737 Loss: 0.0010675990488380194\n",
      "Epoch:  738 Loss: 0.0010687517933547497\n",
      "Epoch:  739 Loss: 0.0010518538765609264\n",
      "Epoch:  740 Loss: 0.0010619075037539005\n",
      "Epoch:  741 Loss: 0.0010468277614563704\n",
      "Epoch:  742 Loss: 0.0010430747643113136\n",
      "Epoch:  743 Loss: 0.0010421810438856483\n",
      "Epoch:  744 Loss: 0.0010402710177004337\n",
      "Epoch:  745 Loss: 0.0010306043550372124\n",
      "Epoch:  746 Loss: 0.0010240634437650442\n",
      "Epoch:  747 Loss: 0.0010210141772404313\n",
      "Epoch:  748 Loss: 0.0010132179595530033\n",
      "Epoch:  749 Loss: 0.0010188461747020483\n",
      "Epoch:  750 Loss: 0.001005083555355668\n",
      "Epoch:  751 Loss: 0.0010027209063991904\n",
      "Epoch:  752 Loss: 0.0010003088973462582\n",
      "Epoch:  753 Loss: 0.0009998506866395473\n",
      "Epoch:  754 Loss: 0.0009864032035693526\n",
      "Epoch:  755 Loss: 0.0009920685552060604\n",
      "Epoch:  756 Loss: 0.0009822105057537556\n",
      "Epoch:  757 Loss: 0.0009791285265237093\n",
      "Epoch:  758 Loss: 0.0009773483034223318\n",
      "Epoch:  759 Loss: 0.0009757220977917314\n",
      "Epoch:  760 Loss: 0.0009654733003117144\n",
      "Epoch:  761 Loss: 0.0009628684492781758\n",
      "Epoch:  762 Loss: 0.0009565019281581044\n",
      "Epoch:  763 Loss: 0.0009606501553207636\n",
      "Epoch:  764 Loss: 0.0009510316303931177\n",
      "Epoch:  765 Loss: 0.0009465377661399543\n",
      "Epoch:  766 Loss: 0.000947884633205831\n",
      "Epoch:  767 Loss: 0.0009343144483864307\n",
      "Epoch:  768 Loss: 0.0009419167181476951\n",
      "Epoch:  769 Loss: 0.0009297567303292453\n",
      "Epoch:  770 Loss: 0.0009294800693169236\n",
      "Epoch:  771 Loss: 0.0009222183143720031\n",
      "Epoch:  772 Loss: 0.0009287185966968536\n",
      "Epoch:  773 Loss: 0.0009206071263179183\n",
      "Epoch:  774 Loss: 0.0009158552857115865\n",
      "Epoch:  775 Loss: 0.0009094382985495031\n",
      "Epoch:  776 Loss: 0.000909901806153357\n",
      "Epoch:  777 Loss: 0.0009051162051036954\n",
      "Epoch:  778 Loss: 0.0009040232980623841\n",
      "Epoch:  779 Loss: 0.0008948193280957639\n",
      "Epoch:  780 Loss: 0.0008952772477641702\n",
      "Epoch:  781 Loss: 0.0008882133988663554\n",
      "Epoch:  782 Loss: 0.0008871167083270848\n",
      "Epoch:  783 Loss: 0.000887556467205286\n",
      "Epoch:  784 Loss: 0.0008791641448624432\n",
      "Epoch:  785 Loss: 0.0008794058812782168\n",
      "Epoch:  786 Loss: 0.0008747715037316084\n",
      "Epoch:  787 Loss: 0.0008709285757504404\n",
      "Epoch:  788 Loss: 0.0008698957972228527\n",
      "Epoch:  789 Loss: 0.0008621277520433068\n",
      "Epoch:  790 Loss: 0.000863530847709626\n",
      "Epoch:  791 Loss: 0.0008599040447734296\n",
      "Epoch:  792 Loss: 0.000858686922583729\n",
      "Epoch:  793 Loss: 0.0008491553599014878\n",
      "Epoch:  794 Loss: 0.0008507732418365777\n",
      "Epoch:  795 Loss: 0.0008472763001918793\n",
      "Epoch:  796 Loss: 0.0008427009452134371\n",
      "Epoch:  797 Loss: 0.0008429787121713161\n",
      "Epoch:  798 Loss: 0.0008372605079784989\n",
      "Epoch:  799 Loss: 0.0008369283168576658\n",
      "Epoch:  800 Loss: 0.0008305939263664186\n",
      "Epoch:  801 Loss: 0.0008325810777023435\n",
      "Epoch:  802 Loss: 0.0008249772945418954\n",
      "Epoch:  803 Loss: 0.0008227090584114194\n",
      "Epoch:  804 Loss: 0.0008223009645007551\n",
      "Epoch:  805 Loss: 0.0008208767976611853\n",
      "Epoch:  806 Loss: 0.000813495775219053\n",
      "Epoch:  807 Loss: 0.0008139943238347769\n",
      "Epoch:  808 Loss: 0.0008123441366478801\n",
      "Epoch:  809 Loss: 0.000809113378636539\n",
      "Epoch:  810 Loss: 0.0008026605355553329\n",
      "Epoch:  811 Loss: 0.0008035654900595546\n",
      "Epoch:  812 Loss: 0.0008015385828912258\n",
      "Epoch:  813 Loss: 0.0007965628756210208\n",
      "Epoch:  814 Loss: 0.0007935561588965356\n",
      "Epoch:  815 Loss: 0.0007924570236355066\n",
      "Epoch:  816 Loss: 0.0007889931439422071\n",
      "Epoch:  817 Loss: 0.0007860689074732363\n",
      "Epoch:  818 Loss: 0.0007860615151003003\n",
      "Epoch:  819 Loss: 0.0007830546819604933\n",
      "Epoch:  820 Loss: 0.0007804306806065142\n",
      "Epoch:  821 Loss: 0.0007742244051769376\n",
      "Epoch:  822 Loss: 0.0007767039351165295\n",
      "Epoch:  823 Loss: 0.0007754316902719438\n",
      "Epoch:  824 Loss: 0.0007672188221476972\n",
      "Epoch:  825 Loss: 0.0007695644162595272\n",
      "Epoch:  826 Loss: 0.000765863573178649\n",
      "Epoch:  827 Loss: 0.0007623936980962753\n",
      "Epoch:  828 Loss: 0.0007597391959279776\n",
      "Epoch:  829 Loss: 0.0007595090428367257\n",
      "Epoch:  830 Loss: 0.0007565729902125895\n",
      "Epoch:  831 Loss: 0.0007528264541178942\n",
      "Epoch:  832 Loss: 0.0007497061160393059\n",
      "Epoch:  833 Loss: 0.0007495848694816232\n",
      "Epoch:  834 Loss: 0.0007448800024576485\n",
      "Epoch:  835 Loss: 0.0007464030059054494\n",
      "Epoch:  836 Loss: 0.000740927061997354\n",
      "Epoch:  837 Loss: 0.0007414153078570962\n",
      "Epoch:  838 Loss: 0.0007360480958595872\n",
      "Epoch:  839 Loss: 0.0007365306955762208\n",
      "Epoch:  840 Loss: 0.0007327611092478037\n",
      "Epoch:  841 Loss: 0.0007286086911335588\n",
      "Epoch:  842 Loss: 0.0007303237216547132\n",
      "Epoch:  843 Loss: 0.0007271741633303463\n",
      "Epoch:  844 Loss: 0.0007227185415104032\n",
      "Epoch:  845 Loss: 0.0007216171361505985\n",
      "Epoch:  846 Loss: 0.0007206223672255874\n",
      "Epoch:  847 Loss: 0.0007149411831051111\n",
      "Epoch:  848 Loss: 0.0007176080835051835\n",
      "Epoch:  849 Loss: 0.0007153504993766546\n",
      "Epoch:  850 Loss: 0.0007110292208380997\n",
      "Epoch:  851 Loss: 0.0007104685646481812\n",
      "Epoch:  852 Loss: 0.0007061843643896282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  853 Loss: 0.0007040233467705548\n",
      "Epoch:  854 Loss: 0.0007047598483040929\n",
      "Epoch:  855 Loss: 0.0007018456817604601\n",
      "Epoch:  856 Loss: 0.0006984574138186872\n",
      "Epoch:  857 Loss: 0.0006973659037612379\n",
      "Epoch:  858 Loss: 0.0006946188514120877\n",
      "Epoch:  859 Loss: 0.000691103283315897\n",
      "Epoch:  860 Loss: 0.0006922857137396932\n",
      "Epoch:  861 Loss: 0.0006902097375132143\n",
      "Epoch:  862 Loss: 0.0006850683130323887\n",
      "Epoch:  863 Loss: 0.0006865118630230427\n",
      "Epoch:  864 Loss: 0.0006811614730395377\n",
      "Epoch:  865 Loss: 0.0006824195152148604\n",
      "Epoch:  866 Loss: 0.0006787504535168409\n",
      "Epoch:  867 Loss: 0.0006756570073775947\n",
      "Epoch:  868 Loss: 0.0006763159180991352\n",
      "Epoch:  869 Loss: 0.0006739312084391713\n",
      "Epoch:  870 Loss: 0.0006705901469103992\n",
      "Epoch:  871 Loss: 0.0006703922990709543\n",
      "Epoch:  872 Loss: 0.0006672871531918645\n",
      "Epoch:  873 Loss: 0.0006639096536673605\n",
      "Epoch:  874 Loss: 0.0006655961042270064\n",
      "Epoch:  875 Loss: 0.0006629239651374519\n",
      "Epoch:  876 Loss: 0.000659655430354178\n",
      "Epoch:  877 Loss: 0.0006608147523365915\n",
      "Epoch:  878 Loss: 0.0006584747461602092\n",
      "Epoch:  879 Loss: 0.0006534909480251372\n",
      "Epoch:  880 Loss: 0.0006539413006976247\n",
      "Epoch:  881 Loss: 0.0006532595725730062\n",
      "Epoch:  882 Loss: 0.0006469966028816998\n",
      "Epoch:  883 Loss: 0.0006486097117885947\n",
      "Epoch:  884 Loss: 0.0006460373406298459\n",
      "Epoch:  885 Loss: 0.000647163949906826\n",
      "Epoch:  886 Loss: 0.0006413436494767666\n",
      "Epoch:  887 Loss: 0.0006423885934054852\n",
      "Epoch:  888 Loss: 0.0006401168066076934\n",
      "Epoch:  889 Loss: 0.0006365754525177181\n",
      "Epoch:  890 Loss: 0.0006379519472829998\n",
      "Epoch:  891 Loss: 0.00063546426827088\n",
      "Epoch:  892 Loss: 0.0006325075519271195\n",
      "Epoch:  893 Loss: 0.0006308690644800663\n",
      "Epoch:  894 Loss: 0.0006305482238531113\n",
      "Epoch:  895 Loss: 0.0006265228730626404\n",
      "Epoch:  896 Loss: 0.0006276926142163575\n",
      "Epoch:  897 Loss: 0.0006258747889660299\n",
      "Epoch:  898 Loss: 0.0006237737252376974\n",
      "Epoch:  899 Loss: 0.0006214538007043302\n",
      "Epoch:  900 Loss: 0.0006199859781190753\n",
      "Epoch:  901 Loss: 0.0006186846294440329\n",
      "Epoch:  902 Loss: 0.0006164386868476868\n",
      "Epoch:  903 Loss: 0.000616061850450933\n",
      "Epoch:  904 Loss: 0.0006141583435237408\n",
      "Epoch:  905 Loss: 0.0006101547623984516\n",
      "Epoch:  906 Loss: 0.0006110015674494207\n",
      "Epoch:  907 Loss: 0.0006078930455259979\n",
      "Epoch:  908 Loss: 0.0006084188935346901\n",
      "Epoch:  909 Loss: 0.0006070000235922635\n",
      "Epoch:  910 Loss: 0.000604805420152843\n",
      "Epoch:  911 Loss: 0.0006033507524989545\n",
      "Epoch:  912 Loss: 0.0006006818730384111\n",
      "Epoch:  913 Loss: 0.0006005535251460969\n",
      "Epoch:  914 Loss: 0.0005975962267257273\n",
      "Epoch:  915 Loss: 0.0005964046576991677\n",
      "Epoch:  916 Loss: 0.0005943833966739476\n",
      "Epoch:  917 Loss: 0.0005944218137301505\n",
      "Epoch:  918 Loss: 0.0005923283169977367\n",
      "Epoch:  919 Loss: 0.0005897764349356294\n",
      "Epoch:  920 Loss: 0.0005894297501072288\n",
      "Epoch:  921 Loss: 0.0005877969088032842\n",
      "Epoch:  922 Loss: 0.0005857524229213595\n",
      "Epoch:  923 Loss: 0.0005854199989698827\n",
      "Epoch:  924 Loss: 0.0005839911755174398\n",
      "Epoch:  925 Loss: 0.0005806179251521826\n",
      "Epoch:  926 Loss: 0.0005802907980978489\n",
      "Epoch:  927 Loss: 0.0005789524875581264\n",
      "Epoch:  928 Loss: 0.0005770666757598519\n",
      "Epoch:  929 Loss: 0.0005767362308688462\n",
      "Epoch:  930 Loss: 0.0005758539191447198\n",
      "Epoch:  931 Loss: 0.0005724201910197735\n",
      "Epoch:  932 Loss: 0.0005724232760258019\n",
      "Epoch:  933 Loss: 0.0005701451445929706\n",
      "Epoch:  934 Loss: 0.0005693026469089091\n",
      "Epoch:  935 Loss: 0.0005668472149409354\n",
      "Epoch:  936 Loss: 0.0005666825454682112\n",
      "Epoch:  937 Loss: 0.0005647826474159956\n",
      "Epoch:  938 Loss: 0.0005636051646433771\n",
      "Epoch:  939 Loss: 0.0005630185478366911\n",
      "Epoch:  940 Loss: 0.0005617846036329865\n",
      "Epoch:  941 Loss: 0.0005587476771324873\n",
      "Epoch:  942 Loss: 0.000558959087356925\n",
      "Epoch:  943 Loss: 0.0005559661076404154\n",
      "Epoch:  944 Loss: 0.0005564585444517434\n",
      "Epoch:  945 Loss: 0.0005538396071642637\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-d847b213200f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatchX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatchY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    914\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m--> 916\u001b[1;33m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[0;32m    917\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   1993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1994\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1995\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1996\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1997\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   1822\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[0;32m   1823\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1824\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1825\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1826\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# start training it\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "for epoch in range(5000):\n",
    "    for start in range (0, len(trX), BATCH_SIZE):\n",
    "        end = start + BATCH_SIZE\n",
    "        batchX = trX[start:end]\n",
    "        batchY = trY[start:end]\n",
    "        \n",
    "        y_pred = model(batchX)\n",
    "        loss = loss_fn(y_pred, batchY)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # find loss on training data\n",
    "    loss = loss_fn(model(trX), trY).item()\n",
    "    print('Epoch: ', epoch, 'Loss:', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后我们用训练好的模型尝试在1到100这些数字上玩FizzBuzz游戏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T04:23:34.856372Z",
     "start_time": "2020-02-26T04:23:34.848394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', 'fizz', '4', 'buzz', 'fizz', '7', '8', 'fizz', 'buzz', '11', 'fizz', '13', '14', 'fizzbuzz', '16', '17', 'fizz', '19', 'buzz', 'fizz', '22', '23', 'fizz', 'buzz', '26', 'fizz', '28', '29', 'fizzbuzz', '31', '32', 'fizz', '34', 'buzz', 'fizz', '37', '38', 'fizz', 'buzz', '41', 'fizz', '43', '44', 'fizzbuzz', '46', '47', 'fizz', '49', 'buzz', 'fizz', '52', '53', 'fizz', 'buzz', '56', 'fizz', '58', '59', 'fizzbuzz', '61', '62', 'fizz', '64', 'buzz', 'fizz', '67', '68', 'fizz', 'buzz', '71', 'fizz', '73', '74', 'fizzbuzz', '76', '77', 'fizz', '79', 'buzz', 'fizz', '82', '83', 'fizz', 'buzz', '86', 'fizz', '88', '89', 'fizzbuzz', '91', '92', 'fizz', '94', 'buzz', 'fizz', '97', '98', 'fizz', 'buzz']\n"
     ]
    }
   ],
   "source": [
    "# Output now\n",
    "testX = torch.Tensor([binary_encode(i, NUM_DIGITS) for i in range(1, 101)])\n",
    "with torch.no_grad():\n",
    "    testY = model(testX)\n",
    "\n",
    "predictions = zip(range(1, 101), list(testY.max(1)[1].data.tolist()))\n",
    "\n",
    "print([fizz_buzz_decode(i, x) for (i, x) in predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T04:23:38.072279Z",
     "start_time": "2020-02-26T04:23:38.064300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([17.7681, 17.1494, 13.3867, 17.8413, 12.3776, 19.1203, 25.4462, 14.3620,\n",
       "        16.1881, 20.8343, 12.9536, 15.7778, 15.9730, 14.4690, 18.2388, 12.8996,\n",
       "        16.2242, 12.7267, 12.2100, 16.9453, 14.9159, 21.9203, 19.6804, 16.6676,\n",
       "        14.7927, 12.9554, 18.4135, 16.3170, 15.4389, 20.3333, 12.5983, 18.0805,\n",
       "        16.2848, 20.6795, 14.3171, 10.0244, 19.8468, 17.7737, 15.7562, 10.8759,\n",
       "        12.6595, 17.4195, 16.9138,  6.4123, 18.7537, 10.8812, 15.3287, 17.7650,\n",
       "        16.1690, 16.9489, 14.3415, 17.7596, 19.7313, 14.6282, 17.9475, 13.4497,\n",
       "        13.6473, 17.7920, 16.5408, 22.9646, 11.9299, 18.0382, 20.1401, 18.4846,\n",
       "        12.7421, 20.2961, 25.7020, 15.0136, 15.8155, 17.5675, 17.4281, 14.8811,\n",
       "        15.9034, 14.4747, 16.9561,  8.4638, 14.9793, 19.2727, 15.5045, 16.3638,\n",
       "        15.3432, 22.4011, 20.7857, 16.8445, 16.9367, 15.6769, 19.8419, 16.8357,\n",
       "        15.6094, 19.8485, 14.4447, 16.5610, 14.1885, 12.5160, 12.3527, 10.3508,\n",
       "        19.7394, 17.9377, 16.3983, 14.8112]),\n",
       "indices=tensor([0, 0, 1, 0, 2, 1, 0, 0, 1, 2, 0, 1, 0, 0, 3, 0, 0, 1, 0, 2, 1, 0, 0, 1,\n",
       "        2, 0, 1, 0, 0, 3, 0, 0, 1, 0, 2, 1, 0, 0, 1, 2, 0, 1, 0, 0, 3, 0, 0, 1,\n",
       "        0, 2, 1, 0, 0, 1, 2, 0, 1, 0, 0, 3, 0, 0, 1, 0, 2, 1, 0, 0, 1, 2, 0, 1,\n",
       "        0, 0, 3, 0, 0, 1, 0, 2, 1, 0, 0, 1, 2, 0, 1, 0, 0, 3, 0, 0, 1, 0, 2, 1,\n",
       "        0, 0, 1, 2]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY.max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T04:23:40.878283Z",
     "start_time": "2020-02-26T04:23:40.871262Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.sum(testY.max(1)[1].numpy() == np.array([fizz_buzz_encode(i) for i in range(1,101)])))\n",
    "testY.max(1)[1].numpy() == np.array([fizz_buzz_encode(i) for i in range(1,101)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
